{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ey--O2jcRnW"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9rH14h0pxL5",
        "outputId": "1412408f-4b12-4989-9446-a2d1728a03a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: spacy[cuda-autodetect] in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.3.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (0.15.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (2.10.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (3.5.0)\n",
            "Requirement already satisfied: cupy-wheel<13.0.0,>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy[cuda-autodetect]) (12.3.0)\n",
            "Requirement already satisfied: cupy-cuda12x==12.3.0 in /usr/local/lib/python3.11/dist-packages (from cupy-wheel<13.0.0,>=11.0.0->spacy[cuda-autodetect]) (12.3.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x==12.3.0->cupy-wheel<13.0.0,>=11.0.0->spacy[cuda-autodetect]) (0.8.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy[cuda-autodetect]) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[cuda-autodetect]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[cuda-autodetect]) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy[cuda-autodetect]) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy[cuda-autodetect]) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy[cuda-autodetect]) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy[cuda-autodetect]) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy[cuda-autodetect]) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy[cuda-autodetect]) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy[cuda-autodetect]) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy[cuda-autodetect]) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy[cuda-autodetect]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy[cuda-autodetect]) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy[cuda-autodetect]) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy[cuda-autodetect]) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1 torchtext==0.15.2 spacy[cuda-autodetect]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "kvymCIx0u2a_",
        "outputId": "9f8152c8-f483-4102-cd7b-614d9b9b5b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: spacy 3.8.4\n",
            "Uninstalling spacy-3.8.4:\n",
            "  Successfully uninstalled spacy-3.8.4\n",
            "Found existing installation: Cython 3.0.12\n",
            "Uninstalling Cython-3.0.12:\n",
            "  Successfully uninstalled Cython-3.0.12\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cython\n",
            "  Downloading Cython-3.0.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m314.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Cython-3.0.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m292.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, cython\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires spacy<4, which is not installed.\n",
            "cupy-cuda12x 12.3.0 requires numpy<1.29,>=1.20, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cython-3.0.12 numpy-2.2.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "308fea031f1c4ac985a68ef78176e8e4",
              "pip_warning": {
                "packages": [
                  "Cython",
                  "cython"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy==3.7.4\n",
            "  Downloading spacy-3.7.4.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# 1. Clean existing installs\n",
        "!pip uninstall -y spacy cython numpy\n",
        "\n",
        "# 2. Install latest compatible versions\n",
        "!pip install --upgrade --no-cache-dir numpy cython\n",
        "!pip install --no-binary spacy spacy==3.7.4  # Latest stable as of 2024\n",
        "\n",
        "# 3. Verify\n",
        "!python -c \"import spacy; print(spacy.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PgR32j24cTR9"
      },
      "outputs": [],
      "source": [
        "!pip install -U 'spacy[cuda-autodetect]' -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWJoajxGHupW",
        "outputId": "6a3020d9-1553-4849-f076-b6fe037a3110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM9qxt7Sq_2-",
        "outputId": "e1f66627-8191-4e81-ee0f-ff5d88cf035b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.0.1+cu117\n",
            "TorchText version: 0.15.2+cpu\n"
          ]
        }
      ],
      "source": [
        "# Verify installation\n",
        "import torch\n",
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"TorchText version:\", torchtext.__version__)\n",
        "\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_de = spacy.load(\"de_core_news_sm\")  # For German"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96u-2MGiXs-4"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "giaE9jhGXmY-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ydNngtERXu4H",
        "outputId": "4723ffaa-e345-42b6-bec5-8ca2aeed91cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SieEu-aCLUUn"
      },
      "outputs": [],
      "source": [
        "random_seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJawqQ9GX8rF"
      },
      "source": [
        "# MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CnGEAqsBXzkU"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure d_model is divisible by num_heads for splitting into multiple heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model  # Dimension of the model\n",
        "        self.num_heads = num_heads  # Number of attention heads\n",
        "        self.d_k = d_model // num_heads  # Dimension of each head\n",
        "\n",
        "        # Linear transformations for query, key, value, and output\n",
        "        self.W_q = nn.Linear(d_model, d_model)  # Query weights\n",
        "        self.W_k = nn.Linear(d_model, d_model)  # Key weights\n",
        "        self.W_v = nn.Linear(d_model, d_model)  # Value weights\n",
        "        self.W_o = nn.Linear(d_model, d_model)  # Output weights\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Compute attention scores (dot product of Q and K, scaled by sqrt(d_k))\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # Apply mask (if provided) to prevent attention to certain positions\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        # Compute attention probabilities using softmax\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        # Compute the weighted sum of values (V) using attention probabilities\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape input tensor to split into multiple heads\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Reshape tensor to combine multiple heads back into a single tensor\n",
        "        batch_size, _, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split into multiple heads\n",
        "        Q = self.split_heads(self.W_q(Q))  # Query\n",
        "        K = self.split_heads(self.W_k(K))  # Key\n",
        "        V = self.split_heads(self.W_v(V))  # Value\n",
        "\n",
        "        # Compute attention output using scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        # Combine heads and apply output linear transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7yfVhkMYEhx"
      },
      "source": [
        "# Position wise Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nhLEdRqBYAyN"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        # First fully connected layer: expands dimension from d_model to d_ff\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        # Second fully connected layer: reduces dimension back to d_model\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        # ReLU activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the first fully connected layer followed by ReLU activation\n",
        "        # Then apply the second fully connected layer\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv9loWVOYKQt"
      },
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_SqsjrSaYIlt"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Initialize a zero matrix to store positional encodings\n",
        "        pe = torch.zeros(max_seq_length, d_model, device=device)\n",
        "\n",
        "        # Create a tensor for positions (0 to max_seq_length - 1)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float, device=device).unsqueeze(1)\n",
        "\n",
        "        # Compute the divisor term for sinusoidal functions\n",
        "        div_term = torch.pow(10_000, (-torch.arange(0, d_model, 2, device=device).float() / d_model))\n",
        "\n",
        "        # Apply sine to even indices in the positional encoding matrix\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd indices in the positional encoding matrix\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register the positional encoding matrix as a buffer (not a trainable parameter)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encodings to the input tensor x\n",
        "        # The encodings are truncated to match the sequence length of x\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKUrDvlYYRWk"
      },
      "source": [
        "# Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i-Suvxv0YOqh"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention mechanism\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Position-wise feed-forward network\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        # Layer normalization for the attention output\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        # Layer normalization for the feed-forward output\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Compute self-attention output (Q, K, V are all x)\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        # Apply residual connection, dropout, and layer normalization\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        # Compute feed-forward network output\n",
        "        ff_output = self.feed_forward(x)\n",
        "        # Apply residual connection, dropout, and layer normalization\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8ySRXYzYYH9"
      },
      "source": [
        "# Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HRTalpKJYU6P"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention mechanism\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Multi-head cross-attention mechanism\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Position-wise feed-forward network\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        # Layer normalization for self-attention output\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        # Layer normalization for cross-attention output\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        # Layer normalization for feed-forward output\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        # Compute self-attention output (Q, K, V are all x, with target mask)\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        # Apply residual connection, dropout, and layer normalization\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        # Compute cross-attention output (Q is x, K and V are encoder output, with source mask)\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        # Apply residual connection, dropout, and layer normalization\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        # Compute feed-forward network output\n",
        "        ff_output = self.feed_forward(x)\n",
        "        # Apply residual connection, dropout, and layer normalization\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YubbLshNYekk"
      },
      "source": [
        "# Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "x6RA6zpgYbHF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Embedding layers for source and target sequences\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        # Positional encoding layer\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # Stack of encoder and decoder layers\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        # Final linear layer to project decoder output to target vocabulary size\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        # Create source mask to ignore padding tokens (where src == 0)\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        # Create target mask to ignore padding tokens (where tgt == 0)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        # Create a \"no-peak\" mask to prevent attending to future tokens in the target sequence\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
        "        # Combine padding mask and no-peak mask for the target sequence\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Generate source and target masks\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "\n",
        "        # Embed source and target sequences and add positional encoding\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        # Pass the embedded source sequence through the encoder layers\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        # Pass the embedded target sequence and encoder output through the decoder layers\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Project the decoder output to the target vocabulary size\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fu_f8PgYnXw"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi6nauuSYhQR",
        "outputId": "5e3cef2e-ede3-4ccc-eb8e-97da34beb8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-21 08:08:32--  https://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10364105 (9.9M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "deu-eng.zip         100%[===================>]   9.88M  27.2MB/s    in 0.4s    \n",
            "\n",
            "2025-03-21 08:08:33 (27.2 MB/s) - ‘deu-eng.zip’ saved [10364105/10364105]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download German-English dataset\n",
        "!wget https://www.manythings.org/anki/deu-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h14iqrxHZOXI",
        "outputId": "4bd4d183-7b7f-489d-b530-ac0d11d8e674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ]
        }
      ],
      "source": [
        "\n",
        "!unzip deu-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uptfO7IsZ2TO"
      },
      "outputs": [],
      "source": [
        "# file reading\n",
        "with open('deu.txt', 'r') as f:\n",
        "    lines = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HECjSS1lZ-x3",
        "outputId": "dc27c4c0-b0b1-4137-e651-76b17d7e8808"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "277891"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bsswMXSfaAy6",
        "outputId": "0c53b2f8-b119-46e7-a2cf-9557c005b773"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm doing fine.\\tMir geht es gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #239432 (CK) & #659407 (Esperantostern)\\n\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OUCSX2QzaEJM"
      },
      "outputs": [],
      "source": [
        "# Remove everything after the 2nd tab character.\n",
        "# As we can see above, we only need the first two columns of the data\n",
        "lines = [line.split('\\t') for line in lines]\n",
        "lines = ['\\t'.join(line[:2]) for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mYLNfLd9aMYw",
        "outputId": "fed37fd0-c23a-46be-9e4c-05749308e50a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm doing fine.\\tMir geht es gut.\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0jrrLdoMKRs-"
      },
      "outputs": [],
      "source": [
        "# Create train, val, test split\n",
        "train_lines, val_test_lines = train_test_split(lines, test_size=0.2, random_state=random_seed, shuffle=True)\n",
        "val_lines, test_lines = train_test_split(val_test_lines, test_size=0.5, random_state=random_seed, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXX2WcrTLlPO",
        "outputId": "bc626a2e-db16-4779-8a47-7c5aa5d68623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "222312\n",
            "27789\n",
            "27790\n"
          ]
        }
      ],
      "source": [
        "print(len(train_lines))\n",
        "print(len(val_lines))\n",
        "print(len(test_lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NJwO1OCELxwM",
        "outputId": "2097eb4e-8615-45f6-e989-f0a6253fa234"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I've never seen Tom eat meat.\\tIch habe Tom noch nie Fleisch essen sehen.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_lines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wuOmjimML1GB",
        "outputId": "31936fc8-fdd3-4ec2-df5f-fd8db155b0d9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"There's probably a better solution.\\tEs gibt vielleicht eine noch bessere Lösung.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_lines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5CIt4obBL3fK",
        "outputId": "dc70ecdb-33ca-4d78-f154-954cdca0d5ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"People who have children are happier than people who don't have children.\\tLeute, die Kinder haben, sind glücklicher als solche ohne.\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_lines[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA5cl9qOIukd"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XWO6oUP9aQrE"
      },
      "outputs": [],
      "source": [
        "SRC_LANGUAGE = \"en\"\n",
        "TGT_LANGUAGE = \"de\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "K57FvxR9HjHS"
      },
      "outputs": [],
      "source": [
        "tokenizer = {}\n",
        "tokenizer[SRC_LANGUAGE] = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
        "tokenizer[TGT_LANGUAGE] = get_tokenizer(\"spacy\", \"de_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av5y5eatMm3P"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9pQb9TXQIQHU"
      },
      "outputs": [],
      "source": [
        "class SentencePairDataset(Dataset):\n",
        "    def __init__(self, lines, src_tokenizer, tgt_tokenizer):\n",
        "        super(SentencePairDataset, self).__init__()\n",
        "\n",
        "        # Store the list of sentence pairs\n",
        "        self.lines = lines\n",
        "        # Tokenizers for source and target languages\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of sentence pairs\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve a sentence pair at the given index\n",
        "        line = self.lines[idx]\n",
        "\n",
        "        # Split the line into source and target sentences\n",
        "        src, tgt = line.split('\\t')\n",
        "        # Tokenize the source and target sentences\n",
        "        src_tokens = self.src_tokenizer(src)\n",
        "        tgt_tokens = self.tgt_tokenizer(tgt)\n",
        "\n",
        "        # Return the tokenized source and target sequences\n",
        "        return src_tokens, tgt_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_VhZbqdSKIn5"
      },
      "outputs": [],
      "source": [
        "train_ds = SentencePairDataset(train_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\n",
        "val_ds = SentencePairDataset(val_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\n",
        "test_ds = SentencePairDataset(test_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpBOyY4vqwfi",
        "outputId": "81a0d78a-c660-406d-9b63-2f017f5b3525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "111\n",
            "53\n",
            "35\n"
          ]
        }
      ],
      "source": [
        "# Length of longest src sequence\n",
        "print(max(len(x[0]) for x in train_ds))\n",
        "print(max(len(x[0]) for x in val_ds))\n",
        "print(max(len(x[0]) for x in test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFxXfbeSrB4h",
        "outputId": "2b51bed3-d95b-45e6-ab5c-4859851c4b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "88\n",
            "63\n",
            "33\n"
          ]
        }
      ],
      "source": [
        "# Length of longest tgt sequence\n",
        "print(max(len(x[1]) for x in train_ds))\n",
        "print(max(len(x[1]) for x in val_ds))\n",
        "print(max(len(x[1]) for x in test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsIgzRtAMNQ-",
        "outputId": "54ae92a4-4dd1-4aaf-abea-ad0035c4d2b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['I', \"'ve\", 'never', 'seen', 'Tom', 'eat', 'meat', '.'],\n",
              " ['Ich', 'habe', 'Tom', 'noch', 'nie', 'Fleisch', 'essen', 'sehen', '.'])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqo3q9RkMkJC"
      },
      "source": [
        "## Create Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rvyraKQ4r9gG"
      },
      "outputs": [],
      "source": [
        "vocab = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "f4FMeg4yMP8x"
      },
      "outputs": [],
      "source": [
        "src_vocab_size = 10_000\n",
        "tgt_vocab_size = 10_000\n",
        "max_seq_len = 100\n",
        "\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "BOS_IDX = 2\n",
        "EOS_IDX = 3\n",
        "\n",
        "special_symbols = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ujR7SLRvM1h7"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(dataset, lang_idx=0):\n",
        "    # Get the total number of items in the dataset\n",
        "    n = len(dataset)\n",
        "    # Initialize the index counter\n",
        "    i = 0\n",
        "\n",
        "    # Iterate through the dataset\n",
        "    while i < n:\n",
        "        # Yield the token sequence at the specified language index (0 for source, 1 for target)\n",
        "        yield dataset[i][lang_idx]\n",
        "        # Move to the next item in the dataset\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "knnnCTpMNGMA"
      },
      "outputs": [],
      "source": [
        "src_iterator = yield_tokens(train_ds, lang_idx=0)\n",
        "tgt_iterator = yield_tokens(train_ds, lang_idx=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "igmo37DaNTox"
      },
      "outputs": [],
      "source": [
        "vocab[SRC_LANGUAGE] = build_vocab_from_iterator(\n",
        "    src_iterator,\n",
        "    min_freq=1,\n",
        "    specials=special_symbols,\n",
        "    special_first=True,\n",
        "    max_tokens=src_vocab_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YZ0aiHA-OAEf"
      },
      "outputs": [],
      "source": [
        "vocab[TGT_LANGUAGE] = build_vocab_from_iterator(\n",
        "    tgt_iterator,\n",
        "    min_freq=1,\n",
        "    specials=special_symbols,\n",
        "    special_first=True,\n",
        "    max_tokens=tgt_vocab_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "J5I7rHfFOMTP"
      },
      "outputs": [],
      "source": [
        "vocab[SRC_LANGUAGE].set_default_index(UNK_IDX)\n",
        "vocab[TGT_LANGUAGE].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6dKNNCzOfoy",
        "outputId": "6a4be639-4836-4dc0-d528-897e17da33e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2203"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab[SRC_LANGUAGE]['hello']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJCNg2MfORYQ",
        "outputId": "d70c3588-081f-44bf-b2b9-a82f617afe68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2345"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab[TGT_LANGUAGE]['Hallo']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hfK2UQZFmxQD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def collate_fn(batch, vocab):\n",
        "    # Get the batch size\n",
        "    batch_size = len(batch)\n",
        "    # Unzip the batch into source and target sequences\n",
        "    srcs, tgts = zip(*batch)\n",
        "    # Initialize tensors for source and target sequences with padding\n",
        "    src_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
        "    tgt_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
        "\n",
        "    # Process each sequence in the batch\n",
        "    for i in range(batch_size):\n",
        "        # Convert source sequence to tensor: add BOS, EOS, and pad to max_seq_len\n",
        "        src_vectors[i] = torch.tensor(\n",
        "            ([BOS_IDX] + vocab[SRC_LANGUAGE](srcs[i]) + [EOS_IDX] + [0] * (max_seq_len - len(srcs[i])))[:max_seq_len],\n",
        "            dtype=torch.long, device=device\n",
        "        )\n",
        "        # Convert target sequence to tensor: add BOS, EOS, and pad to max_seq_len\n",
        "        tgt_vectors[i] = torch.tensor(\n",
        "            ([BOS_IDX] + vocab[TGT_LANGUAGE](tgts[i]) + [EOS_IDX] + [0] * (max_seq_len - len(tgts[i])))[:max_seq_len],\n",
        "            dtype=torch.long, device=device\n",
        "        )\n",
        "\n",
        "    # Return the processed source and target tensors\n",
        "    return src_vectors, tgt_vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Uqxlzw-Hu_YJ"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\n",
        "val_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\n",
        "test_dataloader = DataLoader(test_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxUsDY5JzNj7",
        "outputId": "ba6438bc-563b-4ee4-9479-9d326f232fb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "------------------------------\n",
            "Epoch: 1, Training Loss: 9.277532577514648\n",
            "Epoch: 1, Training Loss: 8.17051887512207\n",
            "Epoch: 1, Training Loss: 7.805712699890137\n",
            "Epoch: 1, Training Loss: 7.453547954559326\n",
            "Epoch: 1, Training Loss: 7.307225704193115\n",
            "Epoch: 1, Training Loss: 7.313108444213867\n",
            "Epoch: 1, Training Loss: 7.0290117263793945\n",
            "Epoch: 1, Training Loss: 7.357789516448975\n",
            "Epoch: 1, Training Loss: 6.998960971832275\n",
            "Epoch: 1, Training Loss: 6.995582580566406\n",
            "Epoch: 1, Training Loss: 6.985655784606934\n",
            "Epoch: 1, Training Loss: 6.706568717956543\n",
            "Epoch: 1, Training Loss: 6.706528663635254\n",
            "Epoch: 1, Training Loss: 6.592241287231445\n",
            "Epoch: 1, Training Loss: 6.605266571044922\n",
            "Epoch: 1, Training Loss: 6.608693599700928\n",
            "Epoch: 1, Training Loss: 6.416293621063232\n",
            "Epoch: 1, Training Loss: 6.387775421142578\n",
            "Epoch: 1, Training Loss: 6.289871692657471\n",
            "Epoch: 1, Training Loss: 6.16506814956665\n",
            "Epoch: 1, Training Loss: 6.160472393035889\n",
            "Epoch: 1, Training Loss: 6.128838062286377\n",
            "Epoch: 1, Training Loss: 6.001903533935547\n",
            "Epoch: 1, Training Loss: 5.95288610458374\n",
            "Epoch: 1, Training Loss: 6.011224746704102\n",
            "Epoch: 1, Training Loss: 5.92606258392334\n",
            "Epoch: 1, Training Loss: 5.917424201965332\n",
            "Epoch: 1, Training Loss: 5.894441604614258\n",
            "Epoch: 1, Training Loss: 5.9525017738342285\n",
            "Epoch: 1, Training Loss: 5.854806900024414\n",
            "Epoch: 1, Training Loss: 5.746310234069824\n",
            "Epoch: 1, Training Loss: 5.603918075561523\n",
            "Epoch: 1, Training Loss: 5.842898368835449\n",
            "Epoch: 1, Training Loss: 5.658228397369385\n",
            "Epoch: 1, Training Loss: 5.563551425933838\n",
            "Epoch: 1, Training Loss: 5.574185848236084\n",
            "Epoch: 1, Training Loss: 5.527190685272217\n",
            "Epoch: 1, Training Loss: 5.538973808288574\n",
            "Epoch: 1, Training Loss: 5.466865062713623\n",
            "Epoch: 1, Training Loss: 5.665063381195068\n",
            "Epoch: 1, Training Loss: 5.450301647186279\n",
            "Epoch: 1, Training Loss: 5.368351459503174\n",
            "Epoch: 1, Training Loss: 5.601306915283203\n",
            "Epoch: 1, Training Loss: 5.540123462677002\n",
            "Epoch: 1, Training Loss: 5.374138832092285\n",
            "Epoch: 1, Training Loss: 5.306178569793701\n",
            "Epoch: 1, Training Loss: 5.231319427490234\n",
            "Epoch: 1, Training Loss: 5.13252067565918\n",
            "Epoch: 1, Training Loss: 5.27112340927124\n",
            "Epoch: 1, Training Loss: 5.314255237579346\n",
            "Epoch: 1, Training Loss: 5.112507343292236\n",
            "Epoch: 1, Training Loss: 5.289547443389893\n",
            "Epoch: 1, Training Loss: 5.211573600769043\n",
            "Epoch: 1, Training Loss: 5.199231147766113\n",
            "Epoch: 1, Training Loss: 5.3199310302734375\n",
            "Epoch: 1, Training Loss: 5.2030029296875\n",
            "Epoch: 1, Training Loss: 5.0112385749816895\n",
            "Epoch: 1, Training Loss: 5.214076519012451\n",
            "Epoch: 1, Training Loss: 5.37655782699585\n",
            "Epoch: 1, Training Loss: 5.121565341949463\n",
            "Epoch: 1, Training Loss: 5.1437907218933105\n",
            "Epoch: 1, Training Loss: 5.109987258911133\n",
            "Epoch: 1, Training Loss: 5.213784217834473\n",
            "Epoch: 1, Training Loss: 5.180843830108643\n",
            "Epoch: 1, Training Loss: 5.15739631652832\n",
            "Epoch: 1, Training Loss: 5.11706018447876\n",
            "Epoch: 1, Training Loss: 5.140696048736572\n",
            "Epoch: 1, Training Loss: 4.938387393951416\n",
            "Epoch: 1, Training Loss: 4.947815418243408\n",
            "Epoch: 1, Training Loss: 5.05727481842041\n",
            "Epoch: 1, Training Loss: 5.0468058586120605\n",
            "Epoch: 1, Training Loss: 4.815572738647461\n",
            "Epoch: 1, Training Loss: 4.97964334487915\n",
            "Epoch: 1, Training Loss: 4.931560039520264\n",
            "Epoch: 1, Training Loss: 4.848146915435791\n",
            "Epoch: 1, Training Loss: 5.072480201721191\n",
            "Epoch: 1, Training Loss: 4.587759494781494\n",
            "Epoch: 1, Training Loss: 4.773246765136719\n",
            "Epoch: 1, Training Loss: 4.7988152503967285\n",
            "Epoch: 1, Training Loss: 4.767733097076416\n",
            "Epoch: 1, Training Loss: 4.803086757659912\n",
            "Epoch: 1, Training Loss: 4.7659735679626465\n",
            "Epoch: 1, Training Loss: 4.744707107543945\n",
            "Epoch: 1, Training Loss: 4.720986366271973\n",
            "Epoch: 1, Training Loss: 4.736055850982666\n",
            "Epoch: 1, Training Loss: 4.789964199066162\n",
            "Epoch: 1, Training Loss: 4.9145331382751465\n",
            "Epoch: 1, Training Loss: 4.935871124267578\n",
            "Epoch: 1, Training Loss: 4.780964374542236\n",
            "Epoch: 1, Training Loss: 4.7041826248168945\n",
            "Epoch: 1, Training Loss: 4.6417999267578125\n",
            "Epoch: 1, Training Loss: 4.619790077209473\n",
            "Epoch: 1, Training Loss: 4.62238883972168\n",
            "Epoch: 1, Training Loss: 4.634514808654785\n",
            "Epoch: 1, Training Loss: 4.57612419128418\n",
            "Epoch: 1, Training Loss: 4.547948360443115\n",
            "Epoch: 1, Training Loss: 4.53038215637207\n",
            "Epoch: 1, Training Loss: 4.954715728759766\n",
            "Epoch: 1, Training Loss: 4.561553478240967\n",
            "Epoch: 1, Training Loss: 4.605979919433594\n",
            "Epoch: 1, Training Loss: 4.611997127532959\n",
            "Epoch: 1, Training Loss: 4.7420759201049805\n",
            "Epoch: 1, Training Loss: 4.666171550750732\n",
            "Epoch: 1, Training Loss: 4.588199615478516\n",
            "Epoch: 1, Training Loss: 4.645635604858398\n",
            "Epoch: 1, Training Loss: 4.69231653213501\n",
            "Epoch: 1, Training Loss: 4.480892181396484\n",
            "Epoch: 1, Training Loss: 4.551595211029053\n",
            "Epoch: 1, Training Loss: 4.861743450164795\n",
            "Epoch: 1, Training Loss: 4.428236961364746\n",
            "Epoch: 1, Training Loss: 4.626055717468262\n",
            "Epoch: 1, Training Loss: 4.5197625160217285\n",
            "Epoch: 1, Training Loss: 4.546754360198975\n",
            "Epoch: 1, Training Loss: 4.680944442749023\n",
            "Epoch: 1, Training Loss: 4.664273738861084\n",
            "Epoch: 1, Training Loss: 4.652593612670898\n",
            "Epoch: 1, Training Loss: 4.494357585906982\n",
            "Epoch: 1, Training Loss: 4.425839424133301\n",
            "Epoch: 1, Training Loss: 4.522644996643066\n",
            "Epoch: 1, Training Loss: 4.1892852783203125\n",
            "Epoch: 1, Training Loss: 4.543370246887207\n",
            "Epoch: 1, Training Loss: 4.537678241729736\n",
            "Epoch: 1, Training Loss: 4.428606986999512\n",
            "Epoch: 1, Training Loss: 4.534051418304443\n",
            "Epoch: 1, Training Loss: 4.366109371185303\n",
            "Epoch: 1, Training Loss: 4.641193866729736\n",
            "Epoch: 1, Training Loss: 4.4748992919921875\n",
            "Epoch: 1, Training Loss: 4.570138454437256\n",
            "Epoch: 1, Training Loss: 4.5777201652526855\n",
            "Epoch: 1, Training Loss: 4.297269344329834\n",
            "Epoch: 1, Training Loss: 4.293389320373535\n",
            "Epoch: 1, Training Loss: 4.5872650146484375\n",
            "Epoch: 1, Training Loss: 4.511226654052734\n",
            "Epoch: 1, Training Loss: 4.506414890289307\n",
            "Epoch: 1, Training Loss: 4.359826564788818\n",
            "Epoch: 1, Training Loss: 4.29917573928833\n",
            "Epoch: 1, Training Loss: 4.387111663818359\n",
            "Epoch: 1, Training Loss: 4.4535231590271\n",
            "Epoch: 1, Training Loss: 4.411858558654785\n",
            "Epoch: 1, Training Loss: 4.376107215881348\n",
            "Epoch: 1, Training Loss: 4.409189224243164\n",
            "Epoch: 1, Training Loss: 4.328949928283691\n",
            "Epoch: 1, Training Loss: 4.3713765144348145\n",
            "Epoch: 1, Training Loss: 4.4540910720825195\n",
            "Epoch: 1, Training Loss: 4.289361000061035\n",
            "Epoch: 1, Training Loss: 4.5150885581970215\n",
            "Epoch: 1, Training Loss: 4.325617790222168\n",
            "Epoch: 1, Training Loss: 4.129029273986816\n",
            "Epoch: 1, Training Loss: 4.411458492279053\n",
            "Epoch: 1, Training Loss: 4.224605560302734\n",
            "Epoch: 1, Training Loss: 4.225409507751465\n",
            "Epoch: 1, Training Loss: 4.249528408050537\n",
            "Epoch: 1, Training Loss: 4.168125152587891\n",
            "Epoch: 1, Training Loss: 4.1987504959106445\n",
            "Epoch: 1, Training Loss: 4.151604175567627\n",
            "Epoch: 1, Training Loss: 3.9700634479522705\n",
            "Epoch: 1, Training Loss: 4.205268383026123\n",
            "Epoch: 1, Training Loss: 4.152658939361572\n",
            "Epoch: 1, Training Loss: 4.379372596740723\n",
            "Epoch: 1, Training Loss: 4.383915424346924\n",
            "Epoch: 1, Training Loss: 4.090460300445557\n",
            "Epoch: 1, Training Loss: 3.885199785232544\n",
            "Epoch: 1, Training Loss: 4.251901149749756\n",
            "Epoch: 1, Training Loss: 3.9108681678771973\n",
            "Epoch: 1, Training Loss: 4.434309959411621\n",
            "Epoch: 1, Training Loss: 4.2470197677612305\n",
            "Epoch: 1, Training Loss: 4.076903343200684\n",
            "Epoch: 1, Training Loss: 4.208611965179443\n",
            "Epoch: 1, Training Loss: 3.963050127029419\n",
            "Epoch: 1, Training Loss: 4.143332481384277\n",
            "Epoch: 1, Training Loss: 4.1729302406311035\n",
            "Epoch: 1, Training Loss: 4.161673545837402\n",
            "Epoch: 1, Training Loss: 4.236130237579346\n",
            "Epoch: 1, Training Loss: 4.065256118774414\n",
            "Epoch: 1, Training Loss: 4.020145893096924\n",
            "Epoch: 1, Training Loss: 4.0109148025512695\n",
            "Epoch: 1, Training Loss: 4.202998161315918\n",
            "Epoch: 1, Training Loss: 4.3011579513549805\n",
            "Epoch: 1, Training Loss: 3.9730021953582764\n",
            "Epoch: 1, Training Loss: 4.084373950958252\n",
            "Epoch: 1, Training Loss: 4.060416221618652\n",
            "Epoch: 1, Training Loss: 4.108172416687012\n",
            "Epoch: 1, Training Loss: 4.112640857696533\n",
            "Epoch: 1, Training Loss: 4.133275985717773\n",
            "Epoch: 1, Training Loss: 4.206546783447266\n",
            "Epoch: 1, Training Loss: 4.026747226715088\n",
            "Epoch: 1, Training Loss: 4.056698799133301\n",
            "Epoch: 1, Training Loss: 4.061124801635742\n",
            "Epoch: 1, Training Loss: 4.101285457611084\n",
            "Epoch: 1, Training Loss: 4.089285373687744\n",
            "Epoch: 1, Training Loss: 4.209795951843262\n",
            "Epoch: 1, Training Loss: 4.020046710968018\n",
            "Epoch: 1, Training Loss: 3.8827829360961914\n",
            "Epoch: 1, Training Loss: 3.8190677165985107\n",
            "Epoch: 1, Training Loss: 4.1144208908081055\n",
            "Epoch: 1, Training Loss: 4.107166290283203\n",
            "Epoch: 1, Training Loss: 4.077253818511963\n",
            "Epoch: 1, Training Loss: 4.146552085876465\n",
            "Epoch: 1, Training Loss: 4.072381496429443\n",
            "Epoch: 1, Training Loss: 4.016816139221191\n",
            "Epoch: 1, Training Loss: 4.219664573669434\n",
            "Epoch: 1, Training Loss: 4.03127384185791\n",
            "Epoch: 1, Training Loss: 4.1705756187438965\n",
            "Epoch: 1, Training Loss: 4.028417587280273\n",
            "Epoch: 1, Training Loss: 3.993708372116089\n",
            "Epoch: 1, Training Loss: 4.105165481567383\n",
            "Epoch: 1, Training Loss: 4.290200710296631\n",
            "Epoch: 1, Training Loss: 3.8921585083007812\n",
            "Epoch: 1, Training Loss: 3.9703643321990967\n",
            "Epoch: 1, Training Loss: 4.082740783691406\n",
            "Epoch: 1, Training Loss: 4.233105182647705\n",
            "Epoch: 1, Training Loss: 4.194464683532715\n",
            "Epoch: 1, Training Loss: 4.051018238067627\n",
            "Epoch: 1, Training Loss: 4.094948768615723\n",
            "Epoch: 1, Training Loss: 3.9414849281311035\n",
            "Epoch: 1, Training Loss: 4.156607627868652\n",
            "Epoch: 1, Training Loss: 4.305948257446289\n",
            "Epoch: 1, Training Loss: 4.158862590789795\n",
            "Epoch: 1, Training Loss: 4.328627586364746\n",
            "Epoch: 1, Training Loss: 4.094035625457764\n",
            "Epoch: 1, Training Loss: 3.8764991760253906\n",
            "Epoch: 1, Training Loss: 3.9458765983581543\n",
            "Epoch: 1, Training Loss: 4.052855491638184\n",
            "Epoch: 1, Training Loss: 3.911705732345581\n",
            "Epoch: 1, Training Loss: 4.091125965118408\n",
            "Epoch: 1, Training Loss: 4.134169101715088\n",
            "Epoch: 1, Training Loss: 3.954617977142334\n",
            "Epoch: 1, Training Loss: 3.9572958946228027\n",
            "Epoch: 1, Training Loss: 4.077025413513184\n",
            "Epoch: 1, Training Loss: 4.14249849319458\n",
            "Epoch: 1, Training Loss: 3.932399272918701\n",
            "Epoch: 1, Training Loss: 3.8388078212738037\n",
            "Epoch: 1, Training Loss: 3.9531311988830566\n",
            "Epoch: 1, Training Loss: 3.839712381362915\n",
            "Epoch: 1, Training Loss: 4.086449146270752\n",
            "Epoch: 1, Training Loss: 3.6454670429229736\n",
            "Epoch: 1, Training Loss: 4.063736915588379\n",
            "Epoch: 1, Training Loss: 3.89424467086792\n",
            "Epoch: 1, Training Loss: 3.5922060012817383\n",
            "Epoch: 1, Training Loss: 3.741518020629883\n",
            "Epoch: 1, Training Loss: 3.9202721118927\n",
            "Epoch: 1, Training Loss: 4.040398120880127\n",
            "Epoch: 1, Training Loss: 4.101371765136719\n",
            "Epoch: 1, Training Loss: 3.9594600200653076\n",
            "Epoch: 1, Training Loss: 3.7831926345825195\n",
            "Epoch: 1, Training Loss: 3.652099847793579\n",
            "Epoch: 1, Training Loss: 3.7460460662841797\n",
            "Epoch: 1, Training Loss: 3.906193971633911\n",
            "Epoch: 1, Training Loss: 3.911557912826538\n",
            "Epoch: 1, Training Loss: 4.10878849029541\n",
            "Epoch: 1, Training Loss: 3.8570172786712646\n",
            "Epoch: 1, Training Loss: 3.7737386226654053\n",
            "Epoch: 1, Training Loss: 3.9812324047088623\n",
            "Epoch: 1, Training Loss: 3.9102792739868164\n",
            "Epoch: 1, Training Loss: 3.8127317428588867\n",
            "Epoch: 1, Training Loss: 3.893171548843384\n",
            "Epoch: 1, Training Loss: 3.770315408706665\n",
            "Epoch: 1, Training Loss: 3.830125093460083\n",
            "Epoch: 1, Training Loss: 3.864760637283325\n",
            "Epoch: 1, Training Loss: 3.880361318588257\n",
            "Epoch: 1, Training Loss: 3.757120370864868\n",
            "Epoch: 1, Training Loss: 3.6684412956237793\n",
            "Epoch: 1, Training Loss: 3.8675739765167236\n",
            "Epoch: 1, Training Loss: 4.051454544067383\n",
            "Epoch: 1, Training Loss: 3.7173800468444824\n",
            "Epoch: 1, Training Loss: 3.861724853515625\n",
            "Epoch: 1, Training Loss: 3.7911460399627686\n",
            "Epoch: 1, Training Loss: 3.8810861110687256\n",
            "Epoch: 1, Training Loss: 3.986771583557129\n",
            "Epoch: 1, Training Loss: 3.9676458835601807\n",
            "Epoch: 1, Training Loss: 4.046768665313721\n",
            "Epoch: 1, Training Loss: 3.65857195854187\n",
            "Epoch: 1, Training Loss: 3.965993642807007\n",
            "Epoch: 1, Training Loss: 3.6804919242858887\n",
            "Epoch: 1, Training Loss: 3.821232795715332\n",
            "Epoch: 1, Training Loss: 3.957747459411621\n",
            "Epoch: 1, Training Loss: 3.9093923568725586\n",
            "Epoch: 1, Training Loss: 3.5818912982940674\n",
            "Epoch: 1, Training Loss: 3.9536678791046143\n",
            "Epoch: 1, Training Loss: 3.9505937099456787\n",
            "Epoch: 1, Training Loss: 3.7534618377685547\n",
            "Epoch: 1, Training Loss: 3.98331880569458\n",
            "Epoch: 1, Training Loss: 3.9217803478240967\n",
            "Epoch: 1, Training Loss: 3.9079973697662354\n",
            "Epoch: 1, Training Loss: 3.872636318206787\n",
            "Epoch: 1, Training Loss: 3.6284611225128174\n",
            "Epoch: 1, Training Loss: 3.876607656478882\n",
            "Epoch: 1, Training Loss: 3.935441732406616\n",
            "Epoch: 1, Training Loss: 3.806194305419922\n",
            "Epoch: 1, Training Loss: 3.6793057918548584\n",
            "Epoch: 1, Training Loss: 3.849038600921631\n",
            "Epoch: 1, Training Loss: 3.841850757598877\n",
            "Epoch: 1, Training Loss: 3.8711581230163574\n",
            "Epoch: 1, Training Loss: 3.9014384746551514\n",
            "Epoch: 1, Training Loss: 3.9630675315856934\n",
            "Epoch: 1, Training Loss: 3.616009473800659\n",
            "Epoch: 1, Training Loss: 3.8186826705932617\n",
            "Epoch: 1, Training Loss: 3.5440616607666016\n",
            "Epoch: 1, Training Loss: 3.746030569076538\n",
            "Epoch: 1, Training Loss: 3.794666051864624\n",
            "Epoch: 1, Training Loss: 3.6475648880004883\n",
            "Epoch: 1, Training Loss: 3.538822889328003\n",
            "Epoch: 1, Training Loss: 3.8024139404296875\n",
            "Epoch: 1, Training Loss: 3.6645424365997314\n",
            "Epoch: 1, Training Loss: 3.7601656913757324\n",
            "Epoch: 1, Training Loss: 3.799290418624878\n",
            "Epoch: 1, Training Loss: 3.5724048614501953\n",
            "Epoch: 1, Training Loss: 3.9210431575775146\n",
            "Epoch: 1, Training Loss: 3.8433051109313965\n",
            "Epoch: 1, Training Loss: 3.816807985305786\n",
            "Epoch: 1, Training Loss: 3.644343137741089\n",
            "Epoch: 1, Training Loss: 3.6078696250915527\n",
            "Epoch: 1, Training Loss: 3.7833423614501953\n",
            "Epoch: 1, Training Loss: 3.473682403564453\n",
            "Epoch: 1, Training Loss: 3.7311224937438965\n",
            "Epoch: 1, Training Loss: 3.667550563812256\n",
            "Epoch: 1, Training Loss: 3.807072401046753\n",
            "Epoch: 1, Training Loss: 3.599959135055542\n",
            "Epoch: 1, Training Loss: 3.927528142929077\n",
            "Epoch: 1, Training Loss: 3.5781989097595215\n",
            "Epoch: 1, Training Loss: 3.3739588260650635\n",
            "Epoch: 1, Training Loss: 3.759209156036377\n",
            "Epoch: 1, Training Loss: 3.7258763313293457\n",
            "Epoch: 1, Training Loss: 3.587986707687378\n",
            "Epoch: 1, Training Loss: 4.0545220375061035\n",
            "Epoch: 1, Training Loss: 3.839351177215576\n",
            "Epoch: 1, Training Loss: 3.748828649520874\n",
            "Epoch: 1, Training Loss: 3.4974825382232666\n",
            "Epoch: 1, Training Loss: 3.9003894329071045\n",
            "Epoch: 1, Training Loss: 3.801558256149292\n",
            "Epoch: 1, Training Loss: 3.755582809448242\n",
            "Epoch: 1, Training Loss: 3.6690173149108887\n",
            "Epoch: 1, Training Loss: 3.9400298595428467\n",
            "Epoch: 1, Training Loss: 3.8471806049346924\n",
            "Epoch: 1, Training Loss: 3.6119093894958496\n",
            "Epoch: 1, Training Loss: 3.640841245651245\n",
            "Epoch: 1, Training Loss: 3.5794124603271484\n",
            "Epoch: 1, Training Loss: 3.603912115097046\n",
            "Epoch: 1, Training Loss: 3.66676664352417\n",
            "Epoch: 1, Training Loss: 3.4715073108673096\n",
            "Epoch: 1, Training Loss: 3.7109405994415283\n",
            "Epoch: 1, Training Loss: 3.4457435607910156\n",
            "Epoch: 1, Training Loss: 3.5853428840637207\n",
            "Epoch: 1, Training Loss: 3.6492233276367188\n",
            "Epoch: 1, Training Loss: 3.6710879802703857\n",
            "Epoch: 1, Training Loss: 3.9194700717926025\n",
            "Epoch: 1, Training Loss: 3.7142653465270996\n",
            "Epoch: 1, Training Loss: 3.736327886581421\n",
            "Epoch: 1, Training Loss: 3.78371524810791\n",
            "Epoch: 1, Training Loss: 3.5776243209838867\n",
            "Epoch: 1, Training Loss: 3.618257761001587\n",
            "Epoch: 1, Training Loss: 3.387171983718872\n",
            "Epoch: 1, Training Loss: 3.762009859085083\n",
            "Epoch: 1, Training Loss: 3.4962782859802246\n",
            "Epoch: 1, Training Loss: 3.3632254600524902\n",
            "Epoch: 1, Training Loss: 3.672304630279541\n",
            "Epoch: 1, Training Loss: 3.968005657196045\n",
            "Epoch: 1, Training Loss: 3.560563325881958\n",
            "Epoch: 1, Training Loss: 3.294816732406616\n",
            "Epoch: 1, Training Loss: 3.416668653488159\n",
            "Epoch: 1, Training Loss: 3.5111615657806396\n",
            "Epoch: 1, Training Loss: 3.4217233657836914\n",
            "Epoch: 1, Training Loss: 3.7551450729370117\n",
            "Epoch: 1, Training Loss: 3.4260942935943604\n",
            "Epoch: 1, Training Loss: 3.877107620239258\n",
            "Epoch: 1, Training Loss: 3.665515899658203\n",
            "Epoch: 1, Training Loss: 3.844160795211792\n",
            "Epoch: 1, Training Loss: 3.459155321121216\n",
            "Epoch: 1, Training Loss: 3.4822580814361572\n",
            "Epoch: 1, Training Loss: 3.4255738258361816\n",
            "Epoch: 1, Training Loss: 3.524878978729248\n",
            "Epoch: 1, Training Loss: 3.635960578918457\n",
            "Epoch: 1, Training Loss: 3.39046311378479\n",
            "Epoch: 1, Training Loss: 3.5773653984069824\n",
            "Epoch: 1, Training Loss: 3.799180507659912\n",
            "Epoch: 1, Training Loss: 3.8160901069641113\n",
            "Epoch: 1, Training Loss: 3.5861873626708984\n",
            "Epoch: 1, Training Loss: 3.680814743041992\n",
            "Epoch: 1, Training Loss: 3.7756385803222656\n",
            "Epoch: 1, Training Loss: 3.5381977558135986\n",
            "Epoch: 1, Training Loss: 3.877332925796509\n",
            "Epoch: 1, Training Loss: 3.4303135871887207\n",
            "Epoch: 1, Training Loss: 3.793619394302368\n",
            "Epoch: 1, Training Loss: 3.6301865577697754\n",
            "Epoch: 1, Training Loss: 3.6734750270843506\n",
            "Epoch: 1, Training Loss: 3.4962992668151855\n",
            "Epoch: 1, Training Loss: 3.8178656101226807\n",
            "Epoch: 1, Training Loss: 3.3821120262145996\n",
            "Epoch: 1, Training Loss: 3.5520291328430176\n",
            "Epoch: 1, Training Loss: 3.512512445449829\n",
            "Epoch: 1, Training Loss: 3.710217237472534\n",
            "Epoch: 1, Training Loss: 3.5874743461608887\n",
            "Epoch: 1, Training Loss: 3.369835376739502\n",
            "Epoch: 1, Training Loss: 3.5932488441467285\n",
            "Epoch: 1, Training Loss: 3.397427797317505\n",
            "Epoch: 1, Training Loss: 3.4526376724243164\n",
            "Epoch: 1, Training Loss: 3.1385505199432373\n",
            "Epoch: 1, Training Loss: 3.766124963760376\n",
            "Epoch: 1, Training Loss: 3.567150592803955\n",
            "Epoch: 1, Training Loss: 3.6668648719787598\n",
            "Epoch: 1, Training Loss: 3.4332454204559326\n",
            "Epoch: 1, Training Loss: 3.642742395401001\n",
            "Epoch: 1, Training Loss: 3.3845200538635254\n",
            "Epoch: 1, Training Loss: 3.5329489707946777\n",
            "Epoch: 1, Training Loss: 3.8219456672668457\n",
            "Epoch: 1, Training Loss: 3.3666367530822754\n",
            "Epoch: 1, Training Loss: 3.598017454147339\n",
            "Epoch: 1, Training Loss: 3.3542239665985107\n",
            "Epoch: 1, Training Loss: 3.2745399475097656\n",
            "Epoch: 1, Training Loss: 3.4690349102020264\n",
            "Epoch: 1, Training Loss: 3.429145336151123\n",
            "Epoch: 1, Training Loss: 3.4501399993896484\n",
            "Epoch: 1, Training Loss: 3.4973268508911133\n",
            "Epoch: 1, Training Loss: 3.341520309448242\n",
            "Epoch: 1, Training Loss: 3.753986358642578\n",
            "Epoch: 1, Training Loss: 3.390590190887451\n",
            "Epoch: 1, Training Loss: 3.390601396560669\n",
            "Epoch: 1, Training Loss: 3.4622976779937744\n",
            "Epoch: 1, Training Loss: 3.7315845489501953\n",
            "Epoch: 1, Training Loss: 3.6241652965545654\n",
            "Epoch: 1, Training Loss: 3.2032034397125244\n",
            "Epoch: 1, Training Loss: 3.2466533184051514\n",
            "Epoch: 1, Training Loss: 3.586773633956909\n",
            "Epoch: 1, Training Loss: 3.6226658821105957\n",
            "Epoch: 1, Training Loss: 3.6256141662597656\n",
            "Epoch: 1, Training Loss: 3.2687907218933105\n",
            "Epoch: 1, Training Loss: 3.510573625564575\n",
            "Epoch: 1, Training Loss: 3.3618927001953125\n",
            "Epoch: 1, Training Loss: 3.4544026851654053\n",
            "Epoch: 1, Training Loss: 3.5020065307617188\n",
            "Epoch: 1, Training Loss: 3.6029269695281982\n",
            "Epoch: 1, Training Loss: 3.0703821182250977\n",
            "Epoch: 1, Training Loss: 3.3712568283081055\n",
            "Epoch: 1, Training Loss: 3.5955545902252197\n",
            "Epoch: 1, Training Loss: 3.378514051437378\n",
            "Epoch: 1, Training Loss: 3.3713605403900146\n",
            "Epoch: 1, Training Loss: 3.4731674194335938\n",
            "Epoch: 1, Training Loss: 3.3015270233154297\n",
            "Epoch: 1, Training Loss: 3.360869884490967\n",
            "Epoch: 1, Training Loss: 3.3481600284576416\n",
            "Epoch: 1, Training Loss: 3.460118532180786\n",
            "Epoch: 1, Training Loss: 3.288909912109375\n",
            "Epoch: 1, Training Loss: 3.617007255554199\n",
            "Epoch: 1, Training Loss: 3.457024335861206\n",
            "Epoch: 1, Training Loss: 3.2593119144439697\n",
            "Epoch: 1, Training Loss: 3.5084447860717773\n",
            "Epoch: 1, Training Loss: 3.3764822483062744\n",
            "Epoch: 1, Training Loss: 3.3836300373077393\n",
            "Epoch: 1, Training Loss: 3.3572609424591064\n",
            "Epoch: 1, Training Loss: 3.578038454055786\n",
            "Epoch: 1, Training Loss: 3.2900638580322266\n",
            "Epoch: 1, Training Loss: 3.477311611175537\n",
            "Epoch: 1, Training Loss: 3.564026355743408\n",
            "Epoch: 1, Training Loss: 3.4050490856170654\n",
            "Epoch: 1, Training Loss: 3.2629570960998535\n",
            "Epoch: 1, Training Loss: 3.428861379623413\n",
            "Epoch: 1, Training Loss: 3.4992010593414307\n",
            "Epoch: 1, Training Loss: 3.1823837757110596\n",
            "Epoch: 1, Training Loss: 3.333592414855957\n",
            "Epoch: 1, Training Loss: 3.5238800048828125\n",
            "Epoch: 1, Training Loss: 3.34076189994812\n",
            "Epoch: 1, Training Loss: 3.5908052921295166\n",
            "Epoch: 1, Training Loss: 3.6250486373901367\n",
            "Epoch: 1, Training Loss: 3.304870367050171\n",
            "Epoch: 1, Training Loss: 3.45302414894104\n",
            "Epoch: 1, Training Loss: 3.5512537956237793\n",
            "Epoch: 1, Training Loss: 3.420198440551758\n",
            "Epoch: 1, Training Loss: 3.670194387435913\n",
            "Epoch: 1, Training Loss: 3.4059906005859375\n",
            "Epoch: 1, Training Loss: 3.3808345794677734\n",
            "Epoch: 1, Training Loss: 3.8157095909118652\n",
            "Epoch: 1, Training Loss: 3.1351418495178223\n",
            "Epoch: 1, Training Loss: 3.5993311405181885\n",
            "Epoch: 1, Training Loss: 3.412248134613037\n",
            "Epoch: 1, Training Loss: 3.32119083404541\n",
            "Epoch: 1, Training Loss: 3.4516713619232178\n",
            "Epoch: 1, Training Loss: 3.3962132930755615\n",
            "Epoch: 1, Training Loss: 3.2317285537719727\n",
            "Epoch: 1, Training Loss: 3.3250725269317627\n",
            "Epoch: 1, Training Loss: 3.2653822898864746\n",
            "Epoch: 1, Training Loss: 3.4614226818084717\n",
            "Epoch: 1, Training Loss: 3.4636166095733643\n",
            "Epoch: 1, Training Loss: 3.4658522605895996\n",
            "Epoch: 1, Training Loss: 3.3850057125091553\n",
            "Epoch: 1, Training Loss: 3.463452100753784\n",
            "Epoch: 1, Training Loss: 3.465637683868408\n",
            "Epoch: 1, Training Loss: 3.5851008892059326\n",
            "Epoch: 1, Training Loss: 3.2923290729522705\n",
            "Epoch: 1, Training Loss: 3.4353437423706055\n",
            "Epoch: 1, Training Loss: 3.463010787963867\n",
            "Epoch: 1, Training Loss: 3.238662004470825\n",
            "Epoch: 1, Training Loss: 3.444624423980713\n",
            "Epoch: 1, Training Loss: 3.2890846729278564\n",
            "Epoch: 1, Training Loss: 3.452152729034424\n",
            "Epoch: 1, Training Loss: 3.242102861404419\n",
            "Epoch: 1, Training Loss: 3.6097657680511475\n",
            "Epoch: 1, Training Loss: 3.4094722270965576\n",
            "Epoch: 1, Training Loss: 3.283050298690796\n",
            "Epoch: 1, Training Loss: 3.652951717376709\n",
            "Epoch: 1, Training Loss: 3.3903703689575195\n",
            "Epoch: 1, Training Loss: 3.59979510307312\n",
            "Epoch: 1, Training Loss: 3.124552011489868\n",
            "Epoch: 1, Training Loss: 3.297760248184204\n",
            "Epoch: 1, Training Loss: 3.451582908630371\n",
            "Epoch: 1, Training Loss: 3.4929275512695312\n",
            "Epoch: 1, Training Loss: 3.307955026626587\n",
            "Epoch: 1, Training Loss: 3.4582276344299316\n",
            "Epoch: 1, Training Loss: 3.2822535037994385\n",
            "Epoch: 1, Training Loss: 3.2685163021087646\n",
            "Epoch: 1, Training Loss: 3.37934947013855\n",
            "Epoch: 1, Training Loss: 3.5357213020324707\n",
            "Epoch: 1, Training Loss: 3.170128107070923\n",
            "Epoch: 1, Training Loss: 3.5009992122650146\n",
            "Epoch: 1, Training Loss: 3.5343592166900635\n",
            "Epoch: 1, Training Loss: 3.1279728412628174\n",
            "Epoch: 1, Training Loss: 3.240885019302368\n",
            "Epoch: 1, Training Loss: 3.076712131500244\n",
            "Epoch: 1, Training Loss: 3.2758965492248535\n",
            "Epoch: 1, Training Loss: 3.2873709201812744\n",
            "Epoch: 1, Training Loss: 3.1753697395324707\n",
            "Epoch: 1, Training Loss: 3.2909159660339355\n",
            "Epoch: 1, Training Loss: 3.115474224090576\n",
            "Epoch: 1, Training Loss: 3.4188735485076904\n",
            "Epoch: 1, Training Loss: 3.035282850265503\n",
            "Epoch: 1, Training Loss: 3.1458184719085693\n",
            "Epoch: 1, Training Loss: 3.6816940307617188\n",
            "Epoch: 1, Training Loss: 3.030283212661743\n",
            "Epoch: 1, Training Loss: 3.1898021697998047\n",
            "Epoch: 1, Training Loss: 3.485198497772217\n",
            "Epoch: 1, Training Loss: 3.487701416015625\n",
            "Epoch: 1, Training Loss: 3.0804481506347656\n",
            "Epoch: 1, Training Loss: 3.4768264293670654\n",
            "Epoch: 1, Training Loss: 3.418503522872925\n",
            "Epoch: 1, Training Loss: 3.515988349914551\n",
            "Epoch: 1, Training Loss: 3.1391758918762207\n",
            "Epoch: 1, Training Loss: 3.1578712463378906\n",
            "Epoch: 1, Training Loss: 3.4348950386047363\n",
            "Epoch: 1, Training Loss: 3.115623712539673\n",
            "Epoch: 1, Training Loss: 3.275634527206421\n",
            "Epoch: 1, Training Loss: 3.0659353733062744\n",
            "Epoch: 1, Training Loss: 3.344352960586548\n",
            "Epoch: 1, Training Loss: 3.3519446849823\n",
            "Epoch: 1, Training Loss: 2.9718449115753174\n",
            "Epoch: 1, Training Loss: 3.3924105167388916\n",
            "Epoch: 1, Training Loss: 3.3155124187469482\n",
            "Epoch: 1, Training Loss: 3.588691234588623\n",
            "Epoch: 1, Training Loss: 3.0106446743011475\n",
            "Epoch: 1, Training Loss: 3.2999117374420166\n",
            "Epoch: 1, Training Loss: 3.4870200157165527\n",
            "Epoch: 1, Training Loss: 3.180785894393921\n",
            "Epoch: 1, Training Loss: 3.29573917388916\n",
            "Epoch: 1, Training Loss: 3.270829439163208\n",
            "Epoch: 1, Training Loss: 3.009948253631592\n",
            "Epoch: 1, Training Loss: 3.259249210357666\n",
            "Epoch: 1, Training Loss: 3.1461760997772217\n",
            "Epoch: 1, Training Loss: 3.504326581954956\n",
            "Epoch: 1, Training Loss: 3.065699815750122\n",
            "Epoch: 1, Training Loss: 3.1130988597869873\n",
            "Epoch: 1, Training Loss: 3.194298505783081\n",
            "Epoch: 1, Training Loss: 3.5180368423461914\n",
            "Epoch: 1, Training Loss: 3.406399726867676\n",
            "Epoch: 1, Training Loss: 3.1629364490509033\n",
            "Epoch: 1, Training Loss: 3.2184791564941406\n",
            "Epoch: 1, Training Loss: 3.2489700317382812\n",
            "Epoch: 1, Training Loss: 3.451374053955078\n",
            "Epoch: 1, Training Loss: 3.221795082092285\n",
            "Epoch: 1, Training Loss: 3.516836404800415\n",
            "Epoch: 1, Training Loss: 3.4531071186065674\n",
            "Epoch: 1, Training Loss: 3.1565699577331543\n",
            "Epoch: 1, Training Loss: 3.0745689868927\n",
            "Epoch: 1, Training Loss: 3.2271971702575684\n",
            "Epoch: 1, Training Loss: 3.3715217113494873\n",
            "Epoch: 1, Training Loss: 3.279728889465332\n",
            "Epoch: 1, Training Loss: 3.3560397624969482\n",
            "Epoch: 1, Training Loss: 3.1564717292785645\n",
            "Epoch: 1, Training Loss: 3.3222854137420654\n",
            "Epoch: 1, Training Loss: 3.1547398567199707\n",
            "Epoch: 1, Training Loss: 3.1251678466796875\n",
            "Epoch: 1, Training Loss: 3.600442886352539\n",
            "Epoch: 1, Training Loss: 3.168755292892456\n",
            "Epoch: 1, Training Loss: 3.092251777648926\n",
            "Epoch: 1, Training Loss: 3.423536539077759\n",
            "Epoch: 1, Training Loss: 2.988964319229126\n",
            "Epoch: 1, Training Loss: 3.5360114574432373\n",
            "Epoch: 1, Training Loss: 3.4052133560180664\n",
            "Epoch: 1, Training Loss: 3.236459970474243\n",
            "Epoch: 1, Training Loss: 3.4765095710754395\n",
            "Epoch: 1, Training Loss: 3.219825029373169\n",
            "Epoch: 1, Training Loss: 3.2837026119232178\n",
            "Epoch: 1, Training Loss: 3.006248950958252\n",
            "Epoch: 1, Training Loss: 2.9485013484954834\n",
            "Epoch: 1, Training Loss: 3.081895589828491\n",
            "Epoch: 1, Training Loss: 3.2660913467407227\n",
            "Epoch: 1, Training Loss: 3.246701717376709\n",
            "Epoch: 1, Training Loss: 3.077315092086792\n",
            "Epoch: 1, Training Loss: 3.359652519226074\n",
            "Epoch: 1, Training Loss: 2.9683408737182617\n",
            "Epoch: 1, Training Loss: 3.1358346939086914\n",
            "Epoch: 1, Training Loss: 3.013622522354126\n",
            "Epoch: 1, Training Loss: 3.084252119064331\n",
            "Epoch: 1, Training Loss: 3.178255319595337\n",
            "Epoch: 1, Training Loss: 3.2905964851379395\n",
            "Epoch: 1, Training Loss: 3.485567331314087\n",
            "Epoch: 1, Training Loss: 3.01802659034729\n",
            "Epoch: 1, Training Loss: 3.3371918201446533\n",
            "Epoch: 1, Training Loss: 3.028801441192627\n",
            "Epoch: 1, Training Loss: 3.2165658473968506\n",
            "Epoch: 1, Training Loss: 3.2002673149108887\n",
            "Epoch: 1, Training Loss: 2.957906484603882\n",
            "Epoch: 1, Training Loss: 3.2038207054138184\n",
            "Epoch: 1, Training Loss: 3.044262170791626\n",
            "Epoch: 1, Training Loss: 3.193606376647949\n",
            "Epoch: 1, Training Loss: 3.2281017303466797\n",
            "Epoch: 1, Training Loss: 3.5183627605438232\n",
            "Epoch: 1, Training Loss: 2.9016802310943604\n",
            "Epoch: 1, Training Loss: 3.31850004196167\n",
            "Epoch: 1, Training Loss: 3.2591264247894287\n",
            "Epoch: 1, Training Loss: 3.119497537612915\n",
            "Epoch: 1, Training Loss: 3.1053338050842285\n",
            "Epoch: 1, Training Loss: 3.130171775817871\n",
            "Epoch: 1, Training Loss: 3.334085464477539\n",
            "Epoch: 1, Training Loss: 3.2530357837677\n",
            "Epoch: 1, Training Loss: 3.155836343765259\n",
            "Epoch: 1, Training Loss: 3.3288145065307617\n",
            "Epoch: 1, Training Loss: 3.1824302673339844\n",
            "Epoch: 1, Training Loss: 3.0623557567596436\n",
            "Epoch: 1, Training Loss: 2.9767346382141113\n",
            "Epoch: 1, Training Loss: 2.9755451679229736\n",
            "Epoch: 1, Training Loss: 2.9038054943084717\n",
            "Epoch: 1, Training Loss: 3.4192116260528564\n",
            "Epoch: 1, Training Loss: 3.1864964962005615\n",
            "Epoch: 1, Training Loss: 3.2173454761505127\n",
            "Epoch: 1, Training Loss: 3.1707162857055664\n",
            "Epoch: 1, Training Loss: 3.515810966491699\n",
            "Epoch: 1, Training Loss: 3.146925926208496\n",
            "Epoch: 1, Training Loss: 3.4807326793670654\n",
            "Epoch: 1, Training Loss: 3.1464896202087402\n",
            "Epoch: 1, Training Loss: 3.1251914501190186\n",
            "Epoch: 1, Training Loss: 3.2437386512756348\n",
            "Epoch: 1, Training Loss: 3.1997554302215576\n",
            "Epoch: 1, Training Loss: 3.0954155921936035\n",
            "Epoch: 1, Training Loss: 3.1785056591033936\n",
            "Epoch: 1, Training Loss: 3.0967047214508057\n",
            "Epoch: 1, Training Loss: 3.4075286388397217\n",
            "Epoch: 1, Training Loss: 3.325659990310669\n",
            "Epoch: 1, Training Loss: 3.055511474609375\n",
            "Epoch: 1, Training Loss: 3.0503957271575928\n",
            "Epoch: 1, Training Loss: 3.0112380981445312\n",
            "Epoch: 1, Training Loss: 3.4412240982055664\n",
            "Epoch: 1, Training Loss: 3.006113052368164\n",
            "Epoch: 1, Training Loss: 3.014763832092285\n",
            "Epoch: 1, Training Loss: 3.265681505203247\n",
            "Epoch: 1, Training Loss: 3.256939649581909\n",
            "Epoch: 1, Training Loss: 3.202925682067871\n",
            "Epoch: 1, Training Loss: 3.451366662979126\n",
            "Epoch: 1, Training Loss: 3.2034895420074463\n",
            "Epoch: 1, Training Loss: 3.174002170562744\n",
            "Epoch: 1, Training Loss: 2.888960838317871\n",
            "Epoch: 1, Training Loss: 3.4588162899017334\n",
            "Epoch: 1, Training Loss: 2.9546585083007812\n",
            "Epoch: 1, Training Loss: 3.257990598678589\n",
            "Epoch: 1, Training Loss: 2.867234230041504\n",
            "Epoch: 1, Training Loss: 3.091486930847168\n",
            "Epoch: 1, Training Loss: 2.9984021186828613\n",
            "Epoch: 1, Training Loss: 3.263672113418579\n",
            "Epoch: 1, Training Loss: 3.3166189193725586\n",
            "Epoch: 1, Training Loss: 3.137502908706665\n",
            "Epoch: 1, Training Loss: 3.048452377319336\n",
            "Epoch: 1, Training Loss: 3.108086109161377\n",
            "Epoch: 1, Training Loss: 2.8384406566619873\n",
            "Epoch: 1, Training Loss: 3.065786361694336\n",
            "Epoch: 1, Training Loss: 3.1993227005004883\n",
            "Epoch: 1, Training Loss: 3.3414766788482666\n",
            "Epoch: 1, Training Loss: 3.050738573074341\n",
            "Epoch: 1, Training Loss: 3.163374185562134\n",
            "Epoch: 1, Training Loss: 2.9474329948425293\n",
            "Epoch: 1, Training Loss: 3.241443395614624\n",
            "Epoch: 1, Training Loss: 3.0435259342193604\n",
            "Epoch: 1, Training Loss: 3.246903896331787\n",
            "Epoch: 1, Training Loss: 3.0817408561706543\n",
            "Epoch: 1, Training Loss: 2.874922752380371\n",
            "Epoch: 1, Training Loss: 2.9635732173919678\n",
            "Epoch: 1, Training Loss: 2.877274513244629\n",
            "Epoch: 1, Training Loss: 3.0926496982574463\n",
            "Epoch: 1, Training Loss: 3.086139440536499\n",
            "Epoch: 1, Training Loss: 2.9452552795410156\n",
            "Epoch: 1, Training Loss: 3.262047052383423\n",
            "Epoch: 1, Training Loss: 3.0946156978607178\n",
            "Epoch: 1, Training Loss: 3.23319935798645\n",
            "Epoch: 1, Training Loss: 2.7639667987823486\n",
            "Epoch: 1, Training Loss: 3.0088744163513184\n",
            "Epoch: 1, Training Loss: 3.0792553424835205\n",
            "Epoch: 1, Training Loss: 3.1297268867492676\n",
            "Epoch: 1, Training Loss: 3.1008148193359375\n",
            "Epoch: 1, Training Loss: 3.0370519161224365\n",
            "Epoch: 1, Training Loss: 2.9299001693725586\n",
            "Epoch: 1, Training Loss: 2.8377246856689453\n",
            "Epoch: 1, Training Loss: 3.3585662841796875\n",
            "Epoch: 1, Training Loss: 3.133122205734253\n",
            "Epoch: 1, Training Loss: 3.0117340087890625\n",
            "Epoch: 1, Training Loss: 3.1624233722686768\n",
            "Epoch: 1, Training Loss: 3.237685441970825\n",
            "Epoch: 1, Training Loss: 3.147097110748291\n",
            "Epoch: 1, Training Loss: 3.1917474269866943\n",
            "Epoch: 1, Training Loss: 3.0401811599731445\n",
            "Epoch: 1, Training Loss: 2.909531593322754\n",
            "Epoch: 1, Training Loss: 3.0203888416290283\n",
            "Epoch: 1, Training Loss: 3.2066705226898193\n",
            "Epoch: 1, Training Loss: 3.264634847640991\n",
            "Epoch: 1, Training Loss: 3.132643222808838\n",
            "Epoch: 1, Training Loss: 2.9269142150878906\n",
            "Epoch: 1, Training Loss: 3.086904287338257\n",
            "Epoch: 1, Training Loss: 3.1913952827453613\n",
            "Epoch: 1, Training Loss: 3.2138774394989014\n",
            "Epoch: 1, Training Loss: 2.796786308288574\n",
            "Epoch: 1, Training Loss: 2.8885741233825684\n",
            "Epoch: 1, Training Loss: 3.1529452800750732\n",
            "Epoch: 1, Training Loss: 3.36246395111084\n",
            "Epoch: 1, Training Loss: 3.449443817138672\n",
            "Epoch: 1, Training Loss: 3.3460845947265625\n",
            "Epoch: 1, Training Loss: 2.9282240867614746\n",
            "Epoch: 1, Training Loss: 3.0123398303985596\n",
            "Epoch: 1, Training Loss: 3.092567205429077\n",
            "Epoch: 1, Training Loss: 2.789459705352783\n",
            "Epoch: 1, Training Loss: 2.970484733581543\n",
            "Epoch: 1, Training Loss: 3.3974664211273193\n",
            "Epoch: 1, Training Loss: 3.076154947280884\n",
            "Epoch: 1, Training Loss: 3.177842617034912\n",
            "Epoch: 1, Training Loss: 3.220309257507324\n",
            "Epoch: 1, Training Loss: 2.79710054397583\n",
            "Epoch: 1, Training Loss: 3.1746983528137207\n",
            "Epoch: 1, Training Loss: 2.913389205932617\n",
            "Epoch: 1, Training Loss: 3.0202138423919678\n",
            "Epoch: 1, Training Loss: 3.1425352096557617\n",
            "Epoch: 1, Training Loss: 3.1753649711608887\n",
            "Epoch: 1, Training Loss: 2.8805010318756104\n",
            "Epoch: 1, Training Loss: 3.0772464275360107\n",
            "Epoch: 1, Training Loss: 3.003255844116211\n",
            "Epoch: 1, Training Loss: 3.063002824783325\n",
            "Epoch: 1, Training Loss: 2.959226369857788\n",
            "Epoch: 1, Training Loss: 2.8133704662323\n",
            "Epoch: 1, Training Loss: 2.6757395267486572\n",
            "Epoch: 1, Training Loss: 3.1945126056671143\n",
            "Epoch: 1, Training Loss: 3.1294548511505127\n",
            "Epoch: 1, Training Loss: 3.1003220081329346\n",
            "Epoch: 1, Training Loss: 2.9945120811462402\n",
            "Epoch: 1, Training Loss: 3.2946088314056396\n",
            "Epoch: 1, Training Loss: 3.115253210067749\n",
            "Epoch: 1, Training Loss: 3.059284210205078\n",
            "Epoch: 1, Training Loss: 3.0915563106536865\n",
            "Epoch: 1, Training Loss: 3.117016315460205\n",
            "Epoch: 1, Training Loss: 3.113234043121338\n",
            "Epoch: 1, Training Loss: 3.1245944499969482\n",
            "Epoch: 1, Training Loss: 2.899461507797241\n",
            "Epoch: 1, Training Loss: 3.1118452548980713\n",
            "Epoch: 1, Training Loss: 2.8443305492401123\n",
            "Epoch: 1, Training Loss: 3.3059840202331543\n",
            "Epoch: 1, Training Loss: 2.811542272567749\n",
            "Epoch: 1, Training Loss: 3.135852575302124\n",
            "Epoch: 1, Training Loss: 2.910113573074341\n",
            "Epoch: 1, Training Loss: 3.1002066135406494\n",
            "Epoch: 1, Training Loss: 2.8899128437042236\n",
            "Epoch: 1, Training Loss: 2.839088201522827\n",
            "Epoch: 1, Training Loss: 3.021977186203003\n",
            "Epoch: 1, Training Loss: 2.7669661045074463\n",
            "Epoch: 1, Training Loss: 3.397355318069458\n",
            "Epoch: 1, Training Loss: 2.9525063037872314\n",
            "Epoch: 1, Training Loss: 3.0598583221435547\n",
            "Epoch: 1, Training Loss: 3.1614034175872803\n",
            "Epoch: 1, Training Loss: 3.219512462615967\n",
            "Epoch: 1, Training Loss: 2.937739133834839\n",
            "Epoch: 1, Training Loss: 2.8971409797668457\n",
            "Epoch: 1, Training Loss: 2.9579110145568848\n",
            "Epoch: 1, Training Loss: 2.8391273021698\n",
            "Epoch: 1, Training Loss: 2.868896007537842\n",
            "Epoch: 1, Training Loss: 3.092656373977661\n",
            "Epoch: 1, Training Loss: 2.973121404647827\n",
            "Epoch: 1, Training Loss: 2.622239112854004\n",
            "Epoch: 1, Training Loss: 3.2908782958984375\n",
            "Epoch: 1, Training Loss: 3.037271022796631\n",
            "Epoch: 1, Training Loss: 3.0153467655181885\n",
            "Epoch: 1, Training Loss: 3.1125433444976807\n",
            "Epoch: 1, Training Loss: 2.981231212615967\n",
            "Epoch: 1, Training Loss: 2.9823999404907227\n",
            "Epoch: 1, Training Loss: 2.8953051567077637\n",
            "Epoch: 1, Training Loss: 2.9189529418945312\n",
            "Epoch: 1, Training Loss: 3.2395122051239014\n",
            "Epoch: 1, Training Loss: 3.056100368499756\n",
            "Epoch: 1, Training Loss: 3.1130454540252686\n",
            "Epoch: 1, Training Loss: 3.219247817993164\n",
            "Epoch: 1, Training Loss: 3.1933178901672363\n",
            "Epoch: 1, Training Loss: 3.0461294651031494\n",
            "Epoch: 1, Training Loss: 2.783881902694702\n",
            "Epoch: 1, Training Loss: 3.1383614540100098\n",
            "Epoch: 1, Training Loss: 3.0012850761413574\n",
            "Epoch: 1, Training Loss: 2.857461929321289\n",
            "Epoch: 1, Training Loss: 2.5629074573516846\n",
            "Epoch: 1, Training Loss: 3.0207366943359375\n",
            "Epoch: 1, Training Loss: 2.733079195022583\n",
            "Epoch: 1, Training Loss: 2.9473979473114014\n",
            "Epoch: 1, Training Loss: 2.9837429523468018\n",
            "Epoch: 1, Training Loss: 2.7598395347595215\n",
            "Epoch: 1, Training Loss: 3.2553505897521973\n",
            "Epoch: 1, Training Loss: 2.9288909435272217\n",
            "Epoch: 1, Training Loss: 2.8700671195983887\n",
            "Epoch: 1, Training Loss: 2.9193928241729736\n",
            "Epoch: 1, Training Loss: 3.049983501434326\n",
            "Epoch: 1, Training Loss: 3.091219425201416\n",
            "Epoch: 1, Training Loss: 2.9976041316986084\n",
            "Epoch: 1, Training Loss: 3.2664124965667725\n",
            "Epoch: 1, Training Loss: 3.100555419921875\n",
            "Epoch: 1, Training Loss: 3.049745559692383\n",
            "Epoch: 1, Training Loss: 3.059086561203003\n",
            "Epoch: 1, Training Loss: 3.028258800506592\n",
            "Epoch: 1, Training Loss: 3.2932450771331787\n",
            "Epoch: 1, Training Loss: 3.0063138008117676\n",
            "Epoch: 1, Training Loss: 3.030210494995117\n",
            "Epoch: 1, Training Loss: 2.7873659133911133\n",
            "Epoch: 1, Training Loss: 2.95709490776062\n",
            "Epoch: 1, Training Loss: 2.9793639183044434\n",
            "Epoch: 1, Training Loss: 3.0130014419555664\n",
            "Epoch: 1, Training Loss: 2.9041566848754883\n",
            "Epoch: 1, Training Loss: 2.8919339179992676\n",
            "Epoch: 1, Training Loss: 2.8289248943328857\n",
            "Epoch: 1, Training Loss: 3.0948379039764404\n",
            "Epoch: 1, Training Loss: 3.0478739738464355\n",
            "Epoch: 1, Training Loss: 2.921267509460449\n",
            "Epoch: 1, Training Loss: 2.944401502609253\n",
            "Epoch: 1, Training Loss: 3.212912082672119\n",
            "Epoch: 1, Training Loss: 2.887129545211792\n",
            "Epoch: 1, Training Loss: 3.2381699085235596\n",
            "Epoch: 1, Training Loss: 2.688361644744873\n",
            "Epoch: 1, Training Loss: 3.0652332305908203\n",
            "Epoch: 1, Training Loss: 2.8131155967712402\n",
            "Epoch: 1, Training Loss: 3.1109163761138916\n",
            "Epoch: 1, Training Loss: 2.588486671447754\n",
            "Epoch: 1, Training Loss: 2.8780994415283203\n",
            "Epoch: 1, Training Loss: 2.8886427879333496\n",
            "Epoch: 1, Training Loss: 2.582812786102295\n",
            "Epoch: 1, Training Loss: 3.1767239570617676\n",
            "Epoch: 1, Training Loss: 2.7297234535217285\n",
            "Epoch: 1, Training Loss: 2.9387989044189453\n",
            "Epoch: 1, Training Loss: 2.8925960063934326\n",
            "Epoch: 1, Training Loss: 2.898560047149658\n",
            "Epoch: 1, Training Loss: 2.8882429599761963\n",
            "Epoch: 1, Training Loss: 2.6508285999298096\n",
            "Epoch: 1, Training Loss: 3.0729892253875732\n",
            "Epoch: 1, Training Loss: 2.9087915420532227\n",
            "Epoch: 1, Training Loss: 3.191209554672241\n",
            "Epoch: 1, Training Loss: 2.7845449447631836\n",
            "Epoch: 1, Training Loss: 2.9585883617401123\n",
            "Epoch: 1, Training Loss: 2.7858948707580566\n",
            "Epoch: 1, Training Loss: 3.1125922203063965\n",
            "Epoch: 1, Training Loss: 2.6193084716796875\n",
            "Epoch: 1, Training Loss: 3.0685088634490967\n",
            "Epoch: 1, Training Loss: 2.797898292541504\n",
            "Epoch: 1, Training Loss: 2.761113405227661\n",
            "Epoch: 1, Training Loss: 2.838817596435547\n",
            "Epoch: 1, Training Loss: 2.982423782348633\n",
            "Epoch: 1, Training Loss: 2.68873929977417\n",
            "Epoch: 1, Training Loss: 3.144266366958618\n",
            "Epoch: 1, Training Loss: 3.136676788330078\n",
            "Epoch: 1, Training Loss: 3.060724973678589\n",
            "Epoch: 1, Training Loss: 2.5340042114257812\n",
            "Epoch: 1, Training Loss: 2.882427215576172\n",
            "Epoch: 1, Training Loss: 3.023367404937744\n",
            "Epoch: 1, Training Loss: 2.7822208404541016\n",
            "Epoch: 1, Training Loss: 2.659961700439453\n",
            "Epoch: 1, Training Loss: 2.7122981548309326\n",
            "Epoch: 1, Training Loss: 2.892512559890747\n",
            "Epoch: 1, Training Loss: 3.29878830909729\n",
            "Epoch: 1, Training Loss: 2.92533540725708\n",
            "Epoch: 1, Training Loss: 3.040109872817993\n",
            "Epoch: 1, Training Loss: 2.96478533744812\n",
            "Epoch: 1, Training Loss: 2.7838330268859863\n",
            "Epoch: 1, Training Loss: 2.8838541507720947\n",
            "Epoch: 1, Training Loss: 3.0663981437683105\n",
            "Epoch: 1, Training Loss: 3.1921722888946533\n",
            "Epoch: 1, Training Loss: 3.0374233722686768\n",
            "Epoch: 1, Training Loss: 2.79046368598938\n",
            "Epoch: 1, Training Loss: 3.003378391265869\n",
            "Epoch: 1, Training Loss: 2.805426597595215\n",
            "Epoch: 1, Training Loss: 2.878340244293213\n",
            "Epoch: 1, Training Loss: 2.9379642009735107\n",
            "Epoch: 1, Training Loss: 2.7810556888580322\n",
            "Epoch: 1, Training Loss: 3.078014373779297\n",
            "Epoch: 1, Training Loss: 2.7671632766723633\n",
            "Epoch: 1, Training Loss: 2.9598278999328613\n",
            "Epoch: 1, Training Loss: 3.0951993465423584\n",
            "Epoch: 1, Training Loss: 2.7818260192871094\n",
            "Epoch: 1, Training Loss: 2.68853497505188\n",
            "Epoch: 1, Training Loss: 2.9315755367279053\n",
            "Epoch: 1, Training Loss: 2.92140793800354\n",
            "Epoch: 1, Training Loss: 2.943255662918091\n",
            "Epoch: 1, Training Loss: 3.1563680171966553\n",
            "Epoch: 1, Training Loss: 2.935178756713867\n",
            "Epoch: 1, Training Loss: 2.7585129737854004\n",
            "Epoch: 1, Training Loss: 2.8681910037994385\n",
            "Epoch: 1, Training Loss: 3.0963873863220215\n",
            "Epoch: 1, Training Loss: 2.81367564201355\n",
            "Epoch: 1, Training Loss: 3.05242919921875\n",
            "Epoch: 1, Training Loss: 2.6771914958953857\n",
            "Epoch: 1, Training Loss: 2.619377851486206\n",
            "Epoch: 1, Training Loss: 2.7269105911254883\n",
            "Epoch: 1, Training Loss: 2.705090284347534\n",
            "Epoch: 1, Training Loss: 3.0162899494171143\n",
            "Epoch: 1, Training Loss: 2.7653160095214844\n",
            "Epoch: 1, Training Loss: 2.78029727935791\n",
            "Epoch: 1, Training Loss: 2.7115392684936523\n",
            "Epoch: 1, Training Loss: 2.8110227584838867\n",
            "Epoch: 1, Training Loss: 3.0696494579315186\n",
            "Epoch: 1, Training Loss: 2.934311628341675\n",
            "Epoch: 1, Training Loss: 2.915064573287964\n",
            "Epoch: 1, Training Loss: 2.6626083850860596\n",
            "Epoch: 1, Training Loss: 2.8200950622558594\n",
            "Epoch: 1, Training Loss: 2.9810073375701904\n",
            "Epoch: 1, Training Loss: 2.9038493633270264\n",
            "Epoch: 1, Training Loss: 2.7949585914611816\n",
            "Epoch: 1, Training Loss: 2.809645175933838\n",
            "Epoch: 1, Training Loss: 2.969709873199463\n",
            "Epoch: 1, Training Loss: 2.862029552459717\n",
            "Epoch: 1, Training Loss: 3.170703887939453\n",
            "Epoch: 1, Training Loss: 2.821302652359009\n",
            "Epoch: 1, Training Loss: 2.562502384185791\n",
            "Epoch: 1, Training Loss: 2.9977784156799316\n",
            "Epoch: 1, Training Loss: 3.0732192993164062\n",
            "Epoch: 1, Training Loss: 2.8148016929626465\n",
            "Epoch: 1, Training Loss: 2.7603557109832764\n",
            "Epoch: 1, Training Loss: 2.8836841583251953\n",
            "Epoch: 1, Training Loss: 3.0000360012054443\n",
            "Epoch: 1, Training Loss: 2.8054139614105225\n",
            "Epoch: 1, Training Loss: 2.9649996757507324\n",
            "Epoch: 1, Training Loss: 2.8356034755706787\n",
            "Epoch: 1, Training Loss: 3.0813469886779785\n",
            "Epoch: 1, Training Loss: 2.373920440673828\n",
            "Epoch: 1, Training Loss: 2.9433884620666504\n",
            "Epoch: 1, Training Loss: 3.0676767826080322\n",
            "Epoch: 1, Training Loss: 2.9267871379852295\n",
            "Epoch: 1, Training Loss: 2.7484524250030518\n",
            "Epoch: 1, Training Loss: 2.6765565872192383\n",
            "Epoch: 1, Training Loss: 3.0362234115600586\n",
            "Epoch: 1, Training Loss: 2.8779232501983643\n",
            "Epoch: 1, Training Loss: 2.570404291152954\n",
            "Epoch: 1, Training Loss: 2.9579060077667236\n",
            "Epoch: 1, Training Loss: 3.0890655517578125\n",
            "Epoch: 1, Training Loss: 2.9922702312469482\n",
            "Epoch: 1, Training Loss: 2.9268240928649902\n",
            "Epoch: 1, Training Loss: 2.756527900695801\n",
            "Epoch: 1, Training Loss: 3.020418882369995\n",
            "Epoch: 1, Training Loss: 2.6025023460388184\n",
            "Epoch: 1, Training Loss: 2.7070562839508057\n",
            "Epoch: 1, Training Loss: 2.6104073524475098\n",
            "Epoch: 1, Training Loss: 3.2105114459991455\n",
            "Epoch: 1, Training Loss: 2.936021089553833\n",
            "Epoch: 1, Training Loss: 2.871063232421875\n",
            "Epoch: 1, Training Loss: 2.7415032386779785\n",
            "Epoch: 1, Training Loss: 2.9447996616363525\n",
            "Epoch: 1, Training Loss: 3.137207508087158\n",
            "Epoch: 1, Training Loss: 3.0023794174194336\n",
            "Epoch: 1, Training Loss: 2.7000017166137695\n",
            "Epoch: 1, Training Loss: 3.077024221420288\n",
            "Epoch: 1, Training Loss: 2.7581653594970703\n",
            "Epoch: 1, Training Loss: 2.924420118331909\n",
            "Epoch: 1, Training Loss: 2.559026002883911\n",
            "Epoch: 1, Training Loss: 2.8984744548797607\n",
            "Epoch: 1, Training Loss: 2.954869508743286\n",
            "Epoch: 1, Training Loss: 2.5893990993499756\n",
            "Epoch: 1, Training Loss: 2.7725279331207275\n",
            "Epoch: 1, Training Loss: 2.640570640563965\n",
            "Epoch: 1, Training Loss: 3.070554256439209\n",
            "Epoch: 1, Training Loss: 2.529635190963745\n",
            "Epoch: 1, Training Loss: 2.9233086109161377\n",
            "Epoch: 1, Training Loss: 2.8916289806365967\n",
            "Epoch: 1, Training Loss: 3.0408108234405518\n",
            "Epoch: 1, Training Loss: 2.9172351360321045\n",
            "Epoch: 1, Training Loss: 2.9237608909606934\n",
            "Epoch: 1, Training Loss: 2.7023539543151855\n",
            "Epoch: 1, Training Loss: 2.7888550758361816\n",
            "Epoch: 1, Training Loss: 2.9687671661376953\n",
            "Epoch: 1, Training Loss: 3.020930290222168\n",
            "Epoch: 1, Training Loss: 2.9665515422821045\n",
            "Epoch: 1, Training Loss: 2.542534112930298\n",
            "Epoch: 1, Training Loss: 3.0412487983703613\n",
            "Epoch: 1, Training Loss: 2.6225719451904297\n",
            "Epoch: 1, Training Loss: 2.9442362785339355\n",
            "Epoch: 1, Training Loss: 3.038280963897705\n",
            "Epoch: 1, Training Loss: 2.8451473712921143\n",
            "Epoch: 1, Training Loss: 3.0969760417938232\n",
            "Epoch: 1, Training Loss: 2.6929495334625244\n",
            "Epoch: 1, Training Loss: 2.9312896728515625\n",
            "Epoch: 1, Training Loss: 2.7573556900024414\n",
            "Epoch: 1, Training Loss: 2.7240846157073975\n",
            "Epoch: 1, Training Loss: 2.8866028785705566\n",
            "Epoch: 1, Training Loss: 2.740509271621704\n",
            "Epoch: 1, Training Loss: 2.725653648376465\n",
            "Epoch: 1, Training Loss: 2.839721918106079\n",
            "Epoch: 1, Training Loss: 2.845846176147461\n",
            "Epoch: 1, Training Loss: 2.762848377227783\n",
            "Epoch: 1, Training Loss: 2.6856353282928467\n",
            "Epoch: 1, Training Loss: 2.5931777954101562\n",
            "Epoch: 1, Training Loss: 2.8134765625\n",
            "Epoch: 1, Training Loss: 2.8039233684539795\n",
            "Epoch: 1, Training Loss: 2.9434990882873535\n",
            "Epoch: 1, Training Loss: 2.6589303016662598\n",
            "Epoch: 1, Training Loss: 2.7417898178100586\n",
            "Epoch: 1, Training Loss: 2.908214569091797\n",
            "Epoch: 1, Training Loss: 2.906730890274048\n",
            "Epoch: 1, Training Loss: 2.84161376953125\n",
            "Epoch: 1, Training Loss: 2.697941541671753\n",
            "Epoch: 1, Training Loss: 2.8244917392730713\n",
            "Epoch: 1, Training Loss: 2.8144800662994385\n",
            "Epoch: 1, Training Loss: 2.7721469402313232\n",
            "Epoch: 1, Training Loss: 2.793966054916382\n",
            "Epoch: 1, Training Loss: 2.9491381645202637\n",
            "Epoch: 1, Training Loss: 2.8144166469573975\n",
            "Epoch: 1, Training Loss: 2.828226089477539\n",
            "Epoch: 1, Training Loss: 2.779391288757324\n",
            "Epoch: 1, Training Loss: 2.931668758392334\n",
            "Epoch: 1, Training Loss: 2.850006580352783\n",
            "Epoch: 1, Training Loss: 2.7981162071228027\n",
            "Epoch: 1, Training Loss: 2.8934812545776367\n",
            "Epoch: 1, Training Loss: 2.6309871673583984\n",
            "Epoch: 1, Training Loss: 2.7080910205841064\n",
            "Epoch: 1, Training Loss: 2.771200180053711\n",
            "Epoch: 1, Training Loss: 2.5361859798431396\n",
            "Epoch: 1, Training Loss: 2.4406070709228516\n",
            "Epoch: 1, Training Loss: 2.9649486541748047\n",
            "Epoch: 1, Training Loss: 3.0257513523101807\n",
            "Epoch: 1, Training Loss: 2.71598219871521\n",
            "Epoch: 1, Training Loss: 2.7704105377197266\n",
            "Epoch: 1, Training Loss: 2.7859959602355957\n",
            "Epoch: 1, Training Loss: 2.797517776489258\n",
            "Epoch: 1, Training Loss: 2.904949426651001\n",
            "Epoch: 1, Training Loss: 2.63173770904541\n",
            "Epoch: 1, Training Loss: 2.831817626953125\n",
            "Epoch: 1, Training Loss: 2.7568228244781494\n",
            "Epoch: 1, Training Loss: 2.9000189304351807\n",
            "Epoch: 1, Training Loss: 2.68776798248291\n",
            "Epoch: 1, Training Loss: 2.59381103515625\n",
            "Epoch: 1, Training Loss: 3.106715679168701\n",
            "Epoch: 1, Training Loss: 2.817502975463867\n",
            "Epoch: 1, Training Loss: 2.565058708190918\n",
            "Epoch: 1, Training Loss: 2.6953868865966797\n",
            "Epoch: 1, Training Loss: 2.795060873031616\n",
            "Epoch: 1, Training Loss: 2.8599002361297607\n",
            "Epoch: 1, Training Loss: 2.5633022785186768\n",
            "Epoch: 1, Training Loss: 2.9685280323028564\n",
            "Epoch: 1, Training Loss: 2.6852970123291016\n",
            "Epoch: 1, Training Loss: 2.661694049835205\n",
            "Epoch: 1, Training Loss: 2.9302761554718018\n",
            "Epoch: 1, Training Loss: 2.559025764465332\n",
            "Epoch: 1, Training Loss: 2.77463436126709\n",
            "Epoch: 1, Training Loss: 2.8263237476348877\n",
            "Epoch: 1, Training Loss: 2.87613582611084\n",
            "Epoch: 1, Training Loss: 2.3691627979278564\n",
            "Epoch: 1, Training Loss: 3.038238525390625\n",
            "Epoch: 1, Training Loss: 2.7385964393615723\n",
            "Epoch: 1, Training Loss: 3.0658657550811768\n",
            "Epoch: 1, Training Loss: 2.7827303409576416\n",
            "Epoch: 1, Training Loss: 3.0423552989959717\n",
            "Epoch: 1, Training Loss: 2.5341696739196777\n",
            "Epoch: 1, Training Loss: 2.821239948272705\n",
            "Epoch: 1, Training Loss: 2.782813549041748\n",
            "Epoch: 1, Training Loss: 3.0009844303131104\n",
            "Epoch: 1, Training Loss: 3.0113961696624756\n",
            "Epoch: 1, Training Loss: 2.5561363697052\n",
            "Epoch: 1, Training Loss: 3.02308988571167\n",
            "Epoch: 1, Training Loss: 2.962498664855957\n",
            "Epoch: 1, Training Loss: 2.6015186309814453\n",
            "Epoch: 1, Training Loss: 2.845186233520508\n",
            "Epoch: 1, Training Loss: 2.83955717086792\n",
            "Epoch: 1, Training Loss: 2.794492721557617\n",
            "Epoch: 1, Training Loss: 2.798997640609741\n",
            "Epoch: 1, Training Loss: 3.0754308700561523\n",
            "Epoch: 1, Training Loss: 2.7298262119293213\n",
            "Epoch: 1, Training Loss: 2.728689432144165\n",
            "Epoch: 1, Training Loss: 2.7975339889526367\n",
            "Epoch: 1, Training Loss: 2.7794644832611084\n",
            "Epoch: 1, Training Loss: 2.552663564682007\n",
            "Epoch: 1, Training Loss: 2.8077104091644287\n",
            "Epoch: 1, Training Loss: 2.733158826828003\n",
            "Epoch: 1, Training Loss: 2.6363322734832764\n",
            "Epoch: 1, Training Loss: 2.8397858142852783\n",
            "Epoch: 1, Training Loss: 2.927823305130005\n",
            "Epoch: 1, Training Loss: 2.842695474624634\n",
            "Epoch: 1, Training Loss: 2.6568374633789062\n",
            "Epoch: 1, Training Loss: 2.9000935554504395\n",
            "Epoch: 1, Training Loss: 2.9883182048797607\n",
            "Epoch: 1, Training Loss: 3.1431283950805664\n",
            "Epoch: 1, Training Loss: 2.8504440784454346\n",
            "Epoch: 1, Training Loss: 2.9911606311798096\n",
            "Epoch: 1, Training Loss: 2.8259596824645996\n",
            "Epoch: 1, Training Loss: 2.4009361267089844\n",
            "Epoch: 1, Training Loss: 2.713927984237671\n",
            "Epoch: 1, Training Loss: 2.636343479156494\n",
            "Epoch: 1, Training Loss: 2.7601070404052734\n",
            "Epoch: 1, Training Loss: 2.5626275539398193\n",
            "Epoch: 1, Training Loss: 2.9289743900299072\n",
            "Epoch: 1, Training Loss: 2.71553635597229\n",
            "Epoch: 1, Training Loss: 2.52059006690979\n",
            "Epoch: 1, Training Loss: 2.6218013763427734\n",
            "Epoch: 1, Training Loss: 2.8118321895599365\n",
            "Epoch: 1, Training Loss: 2.689910411834717\n",
            "Epoch: 1, Training Loss: 2.848426580429077\n",
            "Epoch: 1, Training Loss: 2.9060757160186768\n",
            "Epoch: 1, Training Loss: 2.6318845748901367\n",
            "Epoch: 1, Training Loss: 2.585817337036133\n",
            "Epoch: 1, Training Loss: 2.9122281074523926\n",
            "Epoch: 1, Training Loss: 2.596007823944092\n",
            "Epoch: 1, Training Loss: 2.5393571853637695\n",
            "Epoch: 1, Training Loss: 2.6199758052825928\n",
            "Epoch: 1, Training Loss: 2.6380250453948975\n",
            "Epoch: 1, Training Loss: 2.7853126525878906\n",
            "Epoch: 1, Training Loss: 2.6656334400177\n",
            "Epoch: 1, Training Loss: 2.5529685020446777\n",
            "Epoch: 1, Training Loss: 2.803705930709839\n",
            "Epoch: 1, Training Loss: 2.688136339187622\n",
            "Epoch: 1, Training Loss: 3.072871685028076\n",
            "Epoch: 1, Training Loss: 2.5444157123565674\n",
            "Epoch: 1, Training Loss: 2.7835168838500977\n",
            "Epoch: 1, Training Loss: 2.399634838104248\n",
            "Epoch: 1, Training Loss: 2.4380860328674316\n",
            "Epoch: 1, Training Loss: 2.576092004776001\n",
            "Epoch: 1, Training Loss: 2.726069450378418\n",
            "Epoch: 1, Training Loss: 2.662036895751953\n",
            "Epoch: 1, Training Loss: 2.530285120010376\n",
            "Epoch: 1, Training Loss: 2.744551658630371\n",
            "Epoch: 1, Training Loss: 2.482577323913574\n",
            "Epoch: 1, Training Loss: 2.682262659072876\n",
            "Epoch: 1, Training Loss: 2.6685755252838135\n",
            "Epoch: 1, Training Loss: 2.614161729812622\n",
            "Epoch: 1, Training Loss: 2.6543312072753906\n",
            "Epoch: 1, Training Loss: 2.655623435974121\n",
            "Epoch: 1, Training Loss: 2.600939989089966\n",
            "Epoch: 1, Training Loss: 3.1020073890686035\n",
            "Epoch: 1, Training Loss: 2.833706855773926\n",
            "Epoch: 1, Training Loss: 2.681671380996704\n",
            "Epoch: 1, Training Loss: 2.781759262084961\n",
            "Epoch: 1, Training Loss: 2.610168218612671\n",
            "Epoch: 1, Training Loss: 2.456965208053589\n",
            "Epoch: 1, Training Loss: 2.684295654296875\n",
            "Epoch: 1, Training Loss: 2.815976858139038\n",
            "Epoch: 1, Training Loss: 2.6861233711242676\n",
            "Epoch: 1, Training Loss: 2.7946856021881104\n",
            "Epoch: 1, Training Loss: 2.9536004066467285\n",
            "Epoch: 1, Training Loss: 2.8087844848632812\n",
            "Epoch: 1, Training Loss: 2.4202098846435547\n",
            "Epoch: 1, Training Loss: 2.710944652557373\n",
            "Epoch: 1, Training Loss: 2.786397695541382\n",
            "Epoch: 1, Training Loss: 2.831031322479248\n",
            "Epoch: 1, Training Loss: 2.7207374572753906\n",
            "Epoch: 1, Training Loss: 2.7395780086517334\n",
            "Epoch: 1, Training Loss: 2.7091219425201416\n",
            "Epoch: 1, Training Loss: 2.997601270675659\n",
            "Epoch: 1, Training Loss: 2.532085418701172\n",
            "Epoch: 1, Training Loss: 2.7848169803619385\n",
            "Epoch: 1, Training Loss: 2.541818380355835\n",
            "Epoch: 1, Training Loss: 2.6978020668029785\n",
            "Epoch: 1, Training Loss: 2.624066114425659\n",
            "Epoch: 1, Training Loss: 2.7686450481414795\n",
            "Epoch: 1, Training Loss: 2.882549524307251\n",
            "Epoch: 1, Training Loss: 2.7464396953582764\n",
            "Epoch: 1, Training Loss: 2.557941436767578\n",
            "Epoch: 1, Training Loss: 2.7071545124053955\n",
            "Epoch: 1, Training Loss: 2.416332244873047\n",
            "Epoch: 1, Training Loss: 2.5826306343078613\n",
            "Epoch: 1, Training Loss: 2.6096789836883545\n",
            "Epoch: 1, Training Loss: 2.5139875411987305\n",
            "Epoch: 1, Training Loss: 2.789276599884033\n",
            "Epoch: 1, Training Loss: 2.61745548248291\n",
            "Epoch: 1, Training Loss: 2.509631395339966\n",
            "Epoch: 1, Training Loss: 2.728449821472168\n",
            "Epoch: 1, Training Loss: 2.753251075744629\n",
            "Epoch: 1, Training Loss: 2.6652913093566895\n",
            "Epoch: 1, Training Loss: 2.859264850616455\n",
            "Epoch: 1, Training Loss: 2.513359785079956\n",
            "Epoch: 1, Training Loss: 2.7126121520996094\n",
            "Epoch: 1, Training Loss: 2.6999351978302\n",
            "Epoch: 1, Training Loss: 2.5100185871124268\n",
            "Epoch: 1, Training Loss: 2.5415539741516113\n",
            "Epoch: 1, Training Loss: 2.757340431213379\n",
            "Epoch: 1, Training Loss: 2.700824022293091\n",
            "Epoch: 1, Training Loss: 2.552496910095215\n",
            "Epoch: 1, Training Loss: 2.746809720993042\n",
            "Epoch: 1, Training Loss: 2.5969090461730957\n",
            "Epoch: 1, Training Loss: 3.099836826324463\n",
            "Epoch: 1, Training Loss: 2.649832010269165\n",
            "Epoch: 1, Training Loss: 2.5327703952789307\n",
            "Epoch: 1, Training Loss: 2.7582101821899414\n",
            "Epoch: 1, Training Loss: 2.6640751361846924\n",
            "Epoch: 1, Training Loss: 2.4564120769500732\n",
            "Epoch: 1, Training Loss: 2.7575173377990723\n",
            "Epoch: 1, Training Loss: 2.688443660736084\n",
            "Epoch: 1, Training Loss: 2.5225183963775635\n",
            "Epoch: 1, Training Loss: 2.7593328952789307\n",
            "Epoch: 1, Training Loss: 2.5636041164398193\n",
            "Epoch: 1, Training Loss: 2.7769858837127686\n",
            "Epoch: 1, Training Loss: 2.8009283542633057\n",
            "Epoch: 1, Training Loss: 2.647109270095825\n",
            "Epoch: 1, Training Loss: 2.5522711277008057\n",
            "Epoch: 1, Training Loss: 2.902085304260254\n",
            "Epoch: 1, Training Loss: 2.8662149906158447\n",
            "Epoch: 1, Training Loss: 2.3783226013183594\n",
            "Epoch: 1, Training Loss: 2.6441245079040527\n",
            "Epoch: 1, Training Loss: 2.796074867248535\n",
            "Epoch: 1, Training Loss: 2.8422319889068604\n",
            "Epoch: 1, Training Loss: 2.511988401412964\n",
            "Epoch: 1, Training Loss: 2.561884641647339\n",
            "Epoch: 1, Training Loss: 2.717569589614868\n",
            "Epoch: 1, Training Loss: 2.475989818572998\n",
            "Epoch: 1, Training Loss: 2.759168863296509\n",
            "Epoch: 1, Training Loss: 2.603679656982422\n",
            "Epoch: 1, Training Loss: 2.6842615604400635\n",
            "Epoch: 1, Training Loss: 2.229008436203003\n",
            "Epoch: 1, Training Loss: 2.650465488433838\n",
            "Epoch: 1, Training Loss: 2.6579718589782715\n",
            "Epoch: 1, Training Loss: 2.5394532680511475\n",
            "Epoch: 1, Training Loss: 2.4838364124298096\n",
            "Epoch: 1, Training Loss: 2.50325608253479\n",
            "Epoch: 1, Training Loss: 2.621922254562378\n",
            "Epoch: 1, Training Loss: 2.401392698287964\n",
            "Epoch: 1, Training Loss: 2.580768585205078\n",
            "Epoch: 1, Training Loss: 2.7050371170043945\n",
            "Epoch: 1, Training Loss: 2.9144070148468018\n",
            "Epoch: 1, Training Loss: 2.964942455291748\n",
            "Epoch: 1, Training Loss: 2.4624738693237305\n",
            "Epoch: 1, Training Loss: 2.618340253829956\n",
            "Epoch: 1, Training Loss: 2.722360134124756\n",
            "Epoch: 1, Training Loss: 2.6618733406066895\n",
            "Epoch: 1, Training Loss: 2.5558767318725586\n",
            "Epoch: 1, Training Loss: 2.645627498626709\n",
            "Epoch: 1, Training Loss: 2.801525115966797\n",
            "Epoch: 1, Training Loss: 2.482776641845703\n",
            "Epoch: 1, Training Loss: 2.696059465408325\n",
            "Epoch: 1, Training Loss: 2.7469608783721924\n",
            "Epoch: 1, Training Loss: 2.8070590496063232\n",
            "Epoch: 1, Training Loss: 2.5792903900146484\n",
            "Epoch: 1, Training Loss: 2.591010570526123\n",
            "Epoch: 1, Training Loss: 2.454155206680298\n",
            "Epoch: 1, Training Loss: 2.7211132049560547\n",
            "Epoch: 1, Training Loss: 2.5992071628570557\n",
            "Epoch: 1, Training Loss: 2.4011483192443848\n",
            "Epoch: 1, Training Loss: 2.824112892150879\n",
            "Epoch: 1, Training Loss: 2.954739809036255\n",
            "Epoch: 1, Training Loss: 2.668104648590088\n",
            "Epoch: 1, Training Loss: 2.4460690021514893\n",
            "Epoch: 1, Training Loss: 2.6606228351593018\n",
            "Epoch: 1, Training Loss: 2.438082695007324\n",
            "Epoch: 1, Training Loss: 2.6601147651672363\n",
            "Epoch: 1, Training Loss: 2.7344980239868164\n",
            "Epoch: 1, Training Loss: 2.63726544380188\n",
            "Epoch: 1, Training Loss: 2.5804343223571777\n",
            "Epoch: 1, Training Loss: 2.610586643218994\n",
            "Epoch: 1, Training Loss: 2.344625473022461\n",
            "Epoch: 1, Training Loss: 2.6840932369232178\n",
            "Epoch: 1, Training Loss: 2.5278992652893066\n",
            "Epoch: 1, Training Loss: 2.9258251190185547\n",
            "Epoch: 1, Training Loss: 2.5702738761901855\n",
            "Epoch: 1, Training Loss: 2.7806766033172607\n",
            "Epoch: 1, Training Loss: 2.5917694568634033\n",
            "Epoch: 1, Training Loss: 2.667711019515991\n",
            "Epoch: 1, Training Loss: 2.8776779174804688\n",
            "Epoch: 1, Training Loss: 2.608232259750366\n",
            "Epoch: 1, Training Loss: 2.5800132751464844\n",
            "Epoch: 1, Training Loss: 2.9259629249572754\n",
            "Epoch: 1, Training Loss: 2.3343052864074707\n",
            "Epoch: 1, Training Loss: 2.5029783248901367\n",
            "Epoch: 1, Training Loss: 2.5289111137390137\n",
            "Epoch: 1, Training Loss: 2.6578290462493896\n",
            "Epoch: 1, Training Loss: 2.839353084564209\n",
            "Epoch: 1, Training Loss: 2.2767035961151123\n",
            "Epoch: 1, Training Loss: 2.3637588024139404\n",
            "Epoch: 1, Training Loss: 2.6815810203552246\n",
            "Epoch: 1, Training Loss: 2.105973958969116\n",
            "Epoch: 1, Training Loss: 2.8423521518707275\n",
            "Epoch: 1, Training Loss: 2.726956605911255\n",
            "Epoch: 1, Training Loss: 2.60810923576355\n",
            "Epoch: 1, Training Loss: 2.8312556743621826\n",
            "Epoch: 1, Training Loss: 2.695754289627075\n",
            "Epoch: 1, Training Loss: 2.543806314468384\n",
            "Epoch: 1, Training Loss: 2.6726744174957275\n",
            "Epoch: 1, Training Loss: 2.533764362335205\n",
            "Epoch: 1, Training Loss: 2.531917095184326\n",
            "Epoch: 1, Training Loss: 2.876709461212158\n",
            "Epoch: 1, Training Loss: 2.5055501461029053\n",
            "Epoch: 1, Training Loss: 2.78583025932312\n",
            "Epoch: 1, Training Loss: 2.7378950119018555\n",
            "Epoch: 1, Training Loss: 2.483513593673706\n",
            "Epoch: 1, Training Loss: 2.4722650051116943\n",
            "Epoch: 1, Training Loss: 2.725226879119873\n",
            "Epoch: 1, Training Loss: 2.674288272857666\n",
            "Epoch: 1, Training Loss: 2.826498031616211\n",
            "Epoch: 1, Training Loss: 2.8513100147247314\n",
            "Epoch: 1, Training Loss: 2.457812547683716\n",
            "Epoch: 1, Training Loss: 2.762378454208374\n",
            "Epoch: 1, Training Loss: 2.752378225326538\n",
            "Epoch: 1, Training Loss: 2.6765618324279785\n",
            "Epoch: 1, Training Loss: 2.581571340560913\n",
            "Epoch: 1, Training Loss: 2.492488145828247\n",
            "Epoch: 1, Training Loss: 2.4548699855804443\n",
            "Epoch: 1, Training Loss: 2.7307636737823486\n",
            "Epoch: 1, Training Loss: 2.547590970993042\n",
            "Epoch: 1, Training Loss: 2.2950432300567627\n",
            "Epoch: 1, Training Loss: 2.5675833225250244\n",
            "Epoch: 1, Training Loss: 2.6260478496551514\n",
            "Epoch: 1, Training Loss: 2.750807762145996\n",
            "Epoch: 1, Training Loss: 2.4540436267852783\n",
            "Epoch: 1, Training Loss: 2.724031925201416\n",
            "Epoch: 1, Training Loss: 2.819091320037842\n",
            "Epoch: 1, Training Loss: 2.6316142082214355\n",
            "Epoch: 1, Training Loss: 2.588285207748413\n",
            "Epoch: 1, Training Loss: 2.6522319316864014\n",
            "Epoch: 1, Training Loss: 2.6908648014068604\n",
            "Epoch: 1, Training Loss: 2.4768998622894287\n",
            "Epoch: 1, Training Loss: 2.6598289012908936\n",
            "Epoch: 1, Training Loss: 2.806851625442505\n",
            "Epoch: 1, Training Loss: 2.8815979957580566\n",
            "Epoch: 1, Training Loss: 2.5514070987701416\n",
            "Epoch: 1, Training Loss: 2.6067631244659424\n",
            "Epoch: 1, Training Loss: 2.7808547019958496\n",
            "Epoch: 1, Training Loss: 2.339658260345459\n",
            "Epoch: 1, Training Loss: 2.6729209423065186\n",
            "Epoch: 1, Training Loss: 2.568000316619873\n",
            "Epoch: 1, Training Loss: 2.797835350036621\n",
            "Epoch: 1, Training Loss: 2.789747714996338\n",
            "Epoch: 1, Training Loss: 2.5993800163269043\n",
            "Epoch: 1, Training Loss: 2.7337467670440674\n",
            "Epoch: 1, Training Loss: 2.5184171199798584\n",
            "Epoch: 1, Training Loss: 2.7126593589782715\n",
            "Epoch: 1, Training Loss: 2.723588705062866\n",
            "Epoch: 1, Training Loss: 2.553396701812744\n",
            "Epoch: 1, Training Loss: 2.7352077960968018\n",
            "Epoch: 1, Training Loss: 2.2855937480926514\n",
            "Epoch: 1, Training Loss: 2.479001998901367\n",
            "Epoch: 1, Training Loss: 2.521881103515625\n",
            "Epoch: 1, Training Loss: 2.435832977294922\n",
            "Epoch: 1, Training Loss: 2.711660861968994\n",
            "Epoch: 1, Training Loss: 2.6250476837158203\n",
            "Epoch: 1, Training Loss: 2.567927122116089\n",
            "Epoch: 1, Training Loss: 2.4007678031921387\n",
            "Epoch: 1, Training Loss: 2.545773506164551\n",
            "Epoch: 1, Training Loss: 2.8687329292297363\n",
            "Epoch: 1, Training Loss: 2.2944741249084473\n",
            "Epoch: 1, Training Loss: 2.6518898010253906\n",
            "Epoch: 1, Training Loss: 2.701237678527832\n",
            "Epoch: 1, Training Loss: 2.2350564002990723\n",
            "Epoch: 1, Training Loss: 2.8172519207000732\n",
            "Epoch: 1, Training Loss: 2.503336191177368\n",
            "Epoch: 1, Training Loss: 2.58943510055542\n",
            "Epoch: 1, Training Loss: 2.4459235668182373\n",
            "Epoch: 1, Training Loss: 2.308716297149658\n",
            "Epoch: 1, Training Loss: 2.392853260040283\n",
            "Epoch: 1, Training Loss: 2.524099826812744\n",
            "Epoch: 1, Training Loss: 2.6039955615997314\n",
            "Epoch: 1, Training Loss: 2.253499984741211\n",
            "Epoch: 1, Training Loss: 2.574479579925537\n",
            "Epoch: 1, Training Loss: 2.508272886276245\n",
            "Epoch: 1, Training Loss: 2.567885398864746\n",
            "Epoch: 1, Training Loss: 2.278244733810425\n",
            "Epoch: 1, Training Loss: 2.5571603775024414\n",
            "Epoch: 1, Training Loss: 2.5192809104919434\n",
            "Epoch: 1, Training Loss: 2.873288631439209\n",
            "Epoch: 1, Training Loss: 2.4468119144439697\n",
            "Epoch: 1, Training Loss: 2.686117649078369\n",
            "Epoch: 1, Training Loss: 2.4147849082946777\n",
            "Epoch: 1, Training Loss: 2.3914356231689453\n",
            "Epoch: 1, Training Loss: 2.7634317874908447\n",
            "Epoch: 1, Training Loss: 2.490363597869873\n",
            "Epoch: 1, Training Loss: 2.676149845123291\n",
            "Epoch: 1, Training Loss: 2.3343021869659424\n",
            "Epoch: 1, Training Loss: 2.3622584342956543\n",
            "Epoch: 1, Training Loss: 2.7306413650512695\n",
            "Epoch: 1, Training Loss: 2.649083137512207\n",
            "Epoch: 1, Training Loss: 2.380438804626465\n",
            "Epoch: 1, Training Loss: 2.3595619201660156\n",
            "Epoch: 1, Training Loss: 2.5851080417633057\n",
            "Epoch: 1, Training Loss: 2.6827797889709473\n",
            "Epoch: 1, Training Loss: 2.481797695159912\n",
            "Epoch: 1, Training Loss: 2.473679780960083\n",
            "Epoch: 1, Training Loss: 2.7240333557128906\n",
            "Epoch: 1, Training Loss: 2.6738007068634033\n",
            "Epoch: 1, Training Loss: 2.621635913848877\n",
            "Epoch: 1, Training Loss: 2.458442449569702\n",
            "Epoch: 1, Training Loss: 2.7600367069244385\n",
            "Epoch: 1, Training Loss: 2.743772506713867\n",
            "Epoch: 1, Training Loss: 2.6908557415008545\n",
            "Epoch: 1, Training Loss: 2.4928951263427734\n",
            "Epoch: 1, Training Loss: 2.5720267295837402\n",
            "Epoch: 1, Training Loss: 2.3292407989501953\n",
            "Epoch: 1, Training Loss: 2.520618200302124\n",
            "Epoch: 1, Training Loss: 2.4517252445220947\n",
            "Epoch: 1, Training Loss: 2.9815125465393066\n",
            "Epoch: 1, Training Loss: 2.711852550506592\n",
            "Epoch: 1, Training Loss: 2.5564610958099365\n",
            "Epoch: 1, Training Loss: 2.882228374481201\n",
            "Epoch: 1, Training Loss: 2.8083102703094482\n",
            "Epoch: 1, Training Loss: 2.5875372886657715\n",
            "Epoch: 1, Training Loss: 2.4596054553985596\n",
            "Epoch: 1, Training Loss: 2.2387750148773193\n",
            "Epoch: 1, Training Loss: 2.5334789752960205\n",
            "Epoch: 1, Training Loss: 2.3925085067749023\n",
            "Epoch: 1, Training Loss: 2.3415801525115967\n",
            "Epoch: 1, Training Loss: 2.272120237350464\n",
            "Epoch: 1, Training Loss: 2.7532806396484375\n",
            "Epoch: 1, Training Loss: 2.788038492202759\n",
            "Epoch: 1, Training Loss: 2.3410286903381348\n",
            "Epoch: 1, Training Loss: 2.469820737838745\n",
            "Epoch: 1, Training Loss: 2.4240660667419434\n",
            "Epoch: 1, Training Loss: 2.181925058364868\n",
            "Epoch: 1, Training Loss: 2.4626519680023193\n",
            "Epoch: 1, Training Loss: 2.725102186203003\n",
            "Epoch: 1, Training Loss: 2.24985408782959\n",
            "Epoch: 1, Training Loss: 2.4519760608673096\n",
            "Epoch: 1, Training Loss: 2.6961443424224854\n",
            "Epoch: 1, Training Loss: 2.461867094039917\n",
            "Epoch: 1, Training Loss: 2.5739188194274902\n",
            "Epoch: 1, Training Loss: 2.495284080505371\n",
            "Epoch: 1, Training Loss: 2.5581681728363037\n",
            "Epoch: 1, Training Loss: 2.5816564559936523\n",
            "Epoch: 1, Training Loss: 2.4342100620269775\n",
            "Epoch: 1, Training Loss: 2.359790325164795\n",
            "Epoch: 1, Training Loss: 2.4596312046051025\n",
            "Epoch: 1, Training Loss: 2.458263635635376\n",
            "Epoch: 1, Training Loss: 2.8008224964141846\n",
            "Epoch: 1, Training Loss: 2.6522274017333984\n",
            "Epoch: 1, Training Loss: 2.8359155654907227\n",
            "Epoch: 1, Training Loss: 2.8865182399749756\n",
            "Epoch: 1, Training Loss: 2.382307291030884\n",
            "Epoch: 1, Training Loss: 2.6014060974121094\n",
            "Epoch: 1, Training Loss: 2.465078115463257\n",
            "Epoch: 1, Training Loss: 2.346386194229126\n",
            "Epoch: 1, Training Loss: 2.4965622425079346\n",
            "Epoch: 1, Training Loss: 2.5658462047576904\n",
            "Epoch: 1, Training Loss: 2.5488407611846924\n",
            "Epoch: 1, Training Loss: 2.8561723232269287\n",
            "Epoch: 1, Training Loss: 2.607600450515747\n",
            "Epoch: 1, Training Loss: 2.680938243865967\n",
            "Epoch: 1, Training Loss: 2.527279853820801\n",
            "Epoch: 1, Training Loss: 2.5924084186553955\n",
            "Epoch: 1, Training Loss: 2.487525224685669\n",
            "Epoch: 1, Training Loss: 2.315338373184204\n",
            "Epoch: 1, Training Loss: 2.4346930980682373\n",
            "Epoch: 1, Training Loss: 2.6945104598999023\n",
            "Epoch: 1, Training Loss: 2.574054002761841\n",
            "Epoch: 1, Training Loss: 2.512645721435547\n",
            "Epoch: 1, Training Loss: 2.3110392093658447\n",
            "Epoch: 1, Training Loss: 2.0696091651916504\n",
            "Epoch: 1, Training Loss: 2.8107566833496094\n",
            "Epoch: 1, Training Loss: 2.327867269515991\n",
            "Epoch: 1, Training Loss: 2.5289270877838135\n",
            "Epoch: 1, Training Loss: 2.3516547679901123\n",
            "Epoch: 1, Training Loss: 2.5693247318267822\n",
            "Epoch: 1, Training Loss: 2.682243824005127\n",
            "Epoch: 1, Training Loss: 2.5040924549102783\n",
            "Epoch: 1, Training Loss: 2.3206448554992676\n",
            "Epoch: 1, Training Loss: 2.5556395053863525\n",
            "Epoch: 1, Training Loss: 2.4280221462249756\n",
            "Epoch: 1, Training Loss: 2.206185817718506\n",
            "Epoch: 1, Training Loss: 2.3103013038635254\n",
            "Epoch: 1, Training Loss: 2.673741102218628\n",
            "Epoch: 1, Training Loss: 2.4975051879882812\n",
            "Epoch: 1, Training Loss: 2.5317697525024414\n",
            "Epoch: 1, Training Loss: 2.3941335678100586\n",
            "Epoch: 1, Training Loss: 2.6217730045318604\n",
            "Epoch: 1, Training Loss: 2.5877809524536133\n",
            "Epoch: 1, Training Loss: 2.561333417892456\n",
            "Epoch: 1, Training Loss: 2.360978841781616\n",
            "Epoch: 1, Training Loss: 2.5259740352630615\n",
            "Epoch: 1, Training Loss: 2.706273317337036\n",
            "Epoch: 1, Training Loss: 2.3994693756103516\n",
            "Epoch: 1, Training Loss: 2.6518971920013428\n",
            "Epoch: 1, Training Loss: 2.6452319622039795\n",
            "Epoch: 1, Training Loss: 2.373420476913452\n",
            "Epoch: 1, Training Loss: 2.4720497131347656\n",
            "Epoch: 1, Training Loss: 2.43359637260437\n",
            "Epoch: 1, Training Loss: 2.500234365463257\n",
            "Epoch: 1, Training Loss: 2.6580629348754883\n",
            "Epoch: 1, Training Loss: 2.6350290775299072\n",
            "Epoch: 1, Training Loss: 2.8144619464874268\n",
            "Epoch: 1, Training Loss: 2.678657054901123\n",
            "Epoch: 1, Training Loss: 2.324948787689209\n",
            "Epoch: 1, Training Loss: 2.309448003768921\n",
            "Epoch: 1, Training Loss: 2.4209039211273193\n",
            "Epoch: 1, Training Loss: 2.3588857650756836\n",
            "Epoch: 1, Training Loss: 2.248248338699341\n",
            "Epoch: 1, Training Loss: 2.402829885482788\n",
            "Epoch: 1, Training Loss: 2.3904008865356445\n",
            "Epoch: 1, Training Loss: 2.4084460735321045\n",
            "Epoch: 1, Training Loss: 2.4907033443450928\n",
            "Epoch: 1, Training Loss: 2.5780036449432373\n",
            "Epoch: 1, Training Loss: 2.5402626991271973\n",
            "Epoch: 1, Training Loss: 2.681009292602539\n",
            "Epoch: 1, Training Loss: 2.418436288833618\n",
            "Epoch: 1, Training Loss: 2.5890188217163086\n",
            "Epoch: 1, Training Loss: 2.29093861579895\n",
            "Epoch: 1, Training Loss: 2.2878170013427734\n",
            "Epoch: 1, Training Loss: 2.48842191696167\n",
            "Epoch: 1, Training Loss: 2.784670352935791\n",
            "Epoch: 1, Training Loss: 2.429701089859009\n",
            "Epoch: 1, Training Loss: 2.3822286128997803\n",
            "Epoch: 1, Training Loss: 2.393773078918457\n",
            "Epoch: 1, Training Loss: 2.752516984939575\n",
            "Epoch: 1, Training Loss: 2.3418526649475098\n",
            "Epoch: 1, Training Loss: 2.332326650619507\n",
            "Epoch: 1, Training Loss: 2.8358986377716064\n",
            "Epoch: 1, Training Loss: 2.643756866455078\n",
            "Epoch: 1, Training Loss: 2.6130545139312744\n",
            "Epoch: 1, Training Loss: 2.337027072906494\n",
            "Epoch: 1, Training Loss: 2.272475004196167\n",
            "Epoch: 1, Training Loss: 2.5223631858825684\n",
            "Epoch: 1, Training Loss: 2.4209606647491455\n",
            "Epoch: 1, Training Loss: 2.5925121307373047\n",
            "Epoch: 1, Training Loss: 2.2852396965026855\n",
            "Epoch: 1, Training Loss: 2.561830759048462\n",
            "Epoch: 1, Training Loss: 2.3400559425354004\n",
            "Epoch: 1, Training Loss: 2.5156302452087402\n",
            "Epoch: 1, Training Loss: 2.5206661224365234\n",
            "Epoch: 1, Training Loss: 2.572239875793457\n",
            "Epoch: 1, Training Loss: 2.473867177963257\n",
            "Epoch: 1, Training Loss: 2.314563751220703\n",
            "Epoch: 1, Training Loss: 2.3667144775390625\n",
            "Epoch: 1, Training Loss: 2.466708183288574\n",
            "Epoch: 1, Training Loss: 2.2292227745056152\n",
            "Epoch: 1, Training Loss: 2.5547897815704346\n",
            "Epoch: 1, Training Loss: 2.602141857147217\n",
            "Epoch: 1, Training Loss: 2.9265308380126953\n",
            "Epoch: 1, Training Loss: 2.666506290435791\n",
            "Epoch: 1, Training Loss: 2.4135141372680664\n",
            "Epoch: 1, Training Loss: 2.577413558959961\n",
            "Epoch: 1, Training Loss: 2.5384392738342285\n",
            "Epoch: 1, Training Loss: 2.169346332550049\n",
            "Epoch: 1, Training Loss: 2.326463222503662\n",
            "Epoch: 1, Training Loss: 2.731654167175293\n",
            "Epoch: 1, Training Loss: 2.7127163410186768\n",
            "Epoch: 1, Training Loss: 2.3993096351623535\n",
            "Epoch: 1, Training Loss: 2.4697210788726807\n",
            "Epoch: 1, Training Loss: 2.6123881340026855\n",
            "Epoch: 1, Training Loss: 2.480792284011841\n",
            "Epoch: 1, Training Loss: 2.7407634258270264\n",
            "Epoch: 1, Training Loss: 2.5073459148406982\n",
            "Epoch: 1, Training Loss: 2.4890639781951904\n",
            "Epoch: 1, Training Loss: 2.5560200214385986\n",
            "Epoch: 1, Training Loss: 2.5100624561309814\n",
            "Epoch: 1, Training Loss: 2.507683515548706\n",
            "Epoch: 1, Training Loss: 2.4597949981689453\n",
            "Epoch: 1, Training Loss: 2.295269012451172\n",
            "Epoch: 1, Training Loss: 2.509977340698242\n",
            "Epoch: 1, Training Loss: 2.5789005756378174\n",
            "Epoch: 1, Training Loss: 2.5238616466522217\n",
            "Epoch: 1, Training Loss: 2.2550101280212402\n",
            "Epoch: 1, Training Loss: 2.1293725967407227\n",
            "Epoch: 1, Training Loss: 2.7190518379211426\n",
            "Epoch: 1, Training Loss: 2.6059999465942383\n",
            "Epoch: 1, Training Loss: 2.495215654373169\n",
            "Epoch: 1, Training Loss: 2.397617816925049\n",
            "Epoch: 1, Training Loss: 2.5779943466186523\n",
            "Epoch: 1, Training Loss: 2.496941328048706\n",
            "Epoch: 1, Training Loss: 2.5865163803100586\n",
            "Epoch: 1, Training Loss: 2.4794628620147705\n",
            "Epoch: 1, Training Loss: 2.6057565212249756\n",
            "Epoch: 1, Training Loss: 2.4208292961120605\n",
            "Epoch: 1, Training Loss: 2.5282363891601562\n",
            "Epoch: 1, Training Loss: 2.0413224697113037\n",
            "Epoch: 1, Training Loss: 2.491478443145752\n",
            "Epoch: 1, Training Loss: 2.64078950881958\n",
            "Epoch: 1, Training Loss: 2.6416573524475098\n",
            "Epoch: 1, Training Loss: 2.554903984069824\n",
            "Epoch: 1, Training Loss: 2.520440101623535\n",
            "Epoch: 1, Training Loss: 2.5609524250030518\n",
            "Epoch: 1, Training Loss: 2.6358914375305176\n",
            "Epoch: 1, Training Loss: 2.1784796714782715\n",
            "Epoch: 1, Training Loss: 2.2704555988311768\n",
            "Epoch: 1, Training Loss: 2.5033624172210693\n",
            "Epoch: 1, Training Loss: 2.371058225631714\n",
            "Epoch: 1, Training Loss: 2.583559274673462\n",
            "Epoch: 1, Training Loss: 2.5776731967926025\n",
            "Epoch: 1, Training Loss: 2.3959431648254395\n",
            "Epoch: 1, Training Loss: 2.6369237899780273\n",
            "Epoch: 1, Training Loss: 2.647690773010254\n",
            "Epoch: 1, Training Loss: 2.386773109436035\n",
            "Epoch: 1, Training Loss: 2.513507127761841\n",
            "Epoch: 1, Training Loss: 2.244081974029541\n",
            "Epoch: 1, Training Loss: 2.4052343368530273\n",
            "Epoch: 1, Training Loss: 2.1695151329040527\n",
            "Epoch: 1, Training Loss: 2.418994188308716\n",
            "Epoch: 1, Training Loss: 2.4467334747314453\n",
            "Epoch: 1, Training Loss: 2.613758087158203\n",
            "Epoch: 1, Training Loss: 2.492814779281616\n",
            "Epoch: 1, Training Loss: 2.318755626678467\n",
            "Epoch: 1, Training Loss: 2.2537293434143066\n",
            "Epoch: 1, Training Loss: 2.549683094024658\n",
            "Epoch: 1, Training Loss: 2.4529335498809814\n",
            "Epoch: 1, Training Loss: 2.501357316970825\n",
            "Epoch: 1, Training Loss: 2.528369188308716\n",
            "Epoch: 1, Training Loss: 2.5590152740478516\n",
            "Epoch: 1, Training Loss: 2.270052194595337\n",
            "Epoch: 1, Training Loss: 2.218547821044922\n",
            "Epoch: 1, Training Loss: 2.5380117893218994\n",
            "Epoch: 1, Training Loss: 2.623884916305542\n",
            "Epoch: 1, Training Loss: 2.5786936283111572\n",
            "Epoch: 1, Training Loss: 2.374323844909668\n",
            "Epoch: 1, Training Loss: 2.732743978500366\n",
            "Epoch: 1, Training Loss: 2.370905637741089\n",
            "Epoch: 1, Training Loss: 2.446077585220337\n",
            "Epoch: 1, Training Loss: 2.6174750328063965\n",
            "Epoch: 1, Training Loss: 2.2889292240142822\n",
            "Epoch: 1, Training Loss: 2.3414158821105957\n",
            "Epoch: 1, Training Loss: 2.357710123062134\n",
            "Epoch: 1, Training Loss: 2.424826145172119\n",
            "Epoch: 1, Training Loss: 3.163304090499878\n",
            "Epoch: 1, Training Loss: 2.747767925262451\n",
            "Epoch: 1, Training Loss: 2.518023729324341\n",
            "Epoch: 1, Training Loss: 2.16278076171875\n",
            "Epoch: 1, Training Loss: 2.464374542236328\n",
            "Epoch: 1, Training Loss: 2.396995782852173\n",
            "Epoch: 1, Training Loss: 2.4958457946777344\n",
            "Epoch: 1, Training Loss: 2.423689842224121\n",
            "Epoch: 1, Training Loss: 2.4351680278778076\n",
            "Epoch: 1, Training Loss: 2.313798189163208\n",
            "Epoch: 1, Training Loss: 2.3692409992218018\n",
            "Epoch: 1, Training Loss: 2.1090571880340576\n",
            "Epoch: 1, Training Loss: 2.5700154304504395\n",
            "Epoch: 1, Training Loss: 2.4663445949554443\n",
            "Epoch: 1, Training Loss: 2.3346917629241943\n",
            "Epoch: 1, Training Loss: 2.5258123874664307\n",
            "Epoch: 1, Training Loss: 2.407991647720337\n",
            "Epoch: 1, Training Loss: 2.149188995361328\n",
            "Epoch: 1, Training Loss: 2.6418728828430176\n",
            "Epoch: 1, Training Loss: 2.4073257446289062\n",
            "Epoch: 1, Training Loss: 2.4355690479278564\n",
            "Epoch: 1, Training Loss: 2.6056764125823975\n",
            "Epoch: 1, Training Loss: 2.4642162322998047\n",
            "Epoch: 1, Training Loss: 2.6739020347595215\n",
            "Epoch: 1, Training Loss: 2.7765300273895264\n",
            "Epoch: 1, Training Loss: 2.3441059589385986\n",
            "Epoch: 1, Training Loss: 2.365863800048828\n",
            "Epoch: 1, Training Loss: 2.4227421283721924\n",
            "Epoch: 1, Training Loss: 2.41760516166687\n",
            "Epoch: 1, Training Loss: 2.3764383792877197\n",
            "Epoch: 1, Training Loss: 2.304396867752075\n",
            "Epoch: 1, Training Loss: 2.058042049407959\n",
            "Epoch: 1, Training Loss: 2.2441658973693848\n",
            "Epoch: 1, Training Loss: 2.507131338119507\n",
            "Epoch: 1, Training Loss: 2.3721771240234375\n",
            "Epoch: 1, Training Loss: 2.4453160762786865\n",
            "Epoch: 1, Training Loss: 2.3808255195617676\n",
            "Epoch: 1, Training Loss: 2.5079891681671143\n",
            "Epoch: 1, Training Loss: 2.2894067764282227\n",
            "Epoch: 1, Training Loss: 2.480421304702759\n",
            "Epoch: 1, Training Loss: 2.4284658432006836\n",
            "Epoch: 1, Training Loss: 2.674516439437866\n",
            "Epoch: 1, Training Loss: 2.3192458152770996\n",
            "Epoch: 1, Training Loss: 2.302293062210083\n",
            "Epoch: 1, Training Loss: 2.2126200199127197\n",
            "Epoch: 1, Training Loss: 2.2814879417419434\n",
            "Epoch: 1, Training Loss: 2.483264923095703\n",
            "Epoch: 1, Training Loss: 2.5837020874023438\n",
            "Epoch: 1, Training Loss: 2.61250638961792\n",
            "Epoch: 1, Training Loss: 2.396789073944092\n",
            "Epoch: 1, Training Loss: 2.2167632579803467\n",
            "Epoch: 1, Training Loss: 2.2130837440490723\n",
            "Epoch: 1, Training Loss: 2.3629021644592285\n",
            "Epoch: 1, Training Loss: 2.3491415977478027\n",
            "Epoch: 1, Training Loss: 2.632546901702881\n",
            "Epoch: 1, Training Loss: 2.5010244846343994\n",
            "Epoch: 1, Training Loss: 2.622107744216919\n",
            "Epoch: 1, Training Loss: 2.3684096336364746\n",
            "Epoch: 1, Training Loss: 2.645442247390747\n",
            "Epoch: 1, Training Loss: 2.3331639766693115\n",
            "Epoch: 1, Training Loss: 2.4704699516296387\n",
            "Epoch: 1, Training Loss: 2.541565179824829\n",
            "Epoch: 1, Training Loss: 2.6083521842956543\n",
            "Epoch: 1, Training Loss: 2.5500435829162598\n",
            "Epoch: 1, Training Loss: 2.0419459342956543\n",
            "Epoch: 1, Training Loss: 2.6089746952056885\n",
            "Epoch: 1, Training Loss: 2.635528087615967\n",
            "Epoch: 1, Training Loss: 2.4090235233306885\n",
            "Epoch: 1, Training Loss: 2.325528621673584\n",
            "Epoch: 1, Training Loss: 2.509241819381714\n",
            "Epoch: 1, Training Loss: 2.4901278018951416\n",
            "Epoch: 1, Training Loss: 2.3244469165802\n",
            "Epoch: 1, Training Loss: 2.419510841369629\n",
            "Epoch: 1, Training Loss: 2.279160737991333\n",
            "Epoch: 1, Training Loss: 2.4610090255737305\n",
            "Epoch: 1, Training Loss: 2.3790650367736816\n",
            "Epoch: 1, Training Loss: 2.1607353687286377\n",
            "Epoch: 1, Training Loss: 2.3831841945648193\n",
            "Epoch: 1, Training Loss: 2.447300672531128\n",
            "Epoch: 1, Training Loss: 2.144660711288452\n",
            "Epoch: 1, Training Loss: 2.4730746746063232\n",
            "Epoch: 1, Training Loss: 2.4292426109313965\n",
            "Epoch: 1, Training Loss: 2.4473209381103516\n",
            "Epoch: 1, Training Loss: 2.4986863136291504\n",
            "Epoch: 1, Training Loss: 2.4316771030426025\n",
            "Epoch: 1, Training Loss: 2.292917251586914\n",
            "Epoch: 1, Training Loss: 2.3095662593841553\n",
            "Epoch: 1, Training Loss: 2.5751540660858154\n",
            "Epoch: 1, Training Loss: 2.333548069000244\n",
            "Epoch: 1, Training Loss: 2.72902774810791\n",
            "Epoch: 1, Training Loss: 2.5190608501434326\n",
            "Epoch: 1, Training Loss: 2.5493242740631104\n",
            "Epoch: 1, Training Loss: 2.539419651031494\n",
            "Epoch: 1, Training Loss: 2.4984424114227295\n",
            "Epoch: 1, Training Loss: 2.2640419006347656\n",
            "Epoch: 1, Training Loss: 2.2809526920318604\n",
            "Epoch: 1, Training Loss: 2.2869722843170166\n",
            "Epoch: 1, Training Loss: 2.4823546409606934\n",
            "Epoch: 1, Training Loss: 2.2651901245117188\n",
            "Epoch: 1, Training Loss: 2.4181928634643555\n",
            "Epoch: 1, Training Loss: 2.4756646156311035\n",
            "Epoch: 1, Training Loss: 2.4724783897399902\n",
            "Epoch: 1, Training Loss: 2.7438347339630127\n",
            "Epoch: 1, Training Loss: 2.324003219604492\n",
            "Epoch: 1, Training Loss: 2.3340351581573486\n",
            "Epoch: 1, Training Loss: 2.6197397708892822\n",
            "Epoch: 1, Training Loss: 2.332545757293701\n",
            "Epoch: 1, Training Loss: 2.455317974090576\n",
            "Epoch: 1, Training Loss: 2.5121657848358154\n",
            "Epoch: 1, Training Loss: 2.7462098598480225\n",
            "Epoch: 1, Training Loss: 2.3935885429382324\n",
            "Epoch: 1, Training Loss: 2.3237221240997314\n",
            "Epoch: 1, Training Loss: 2.421447992324829\n",
            "Epoch: 1, Training Loss: 2.5101089477539062\n",
            "Epoch: 1, Training Loss: 2.121243953704834\n",
            "Epoch: 1, Training Loss: 2.180802345275879\n",
            "Epoch: 1, Training Loss: 2.257467746734619\n",
            "Epoch: 1, Training Loss: 2.520190954208374\n",
            "Epoch: 1, Training Loss: 2.5358994007110596\n",
            "Epoch: 1, Training Loss: 2.209686279296875\n",
            "Epoch: 1, Training Loss: 2.583251476287842\n",
            "Epoch: 1, Training Loss: 2.2493016719818115\n",
            "Epoch: 1, Training Loss: 2.6217105388641357\n",
            "Epoch: 1, Training Loss: 2.550036907196045\n",
            "Epoch: 1, Training Loss: 2.115147590637207\n",
            "Epoch: 1, Training Loss: 2.2347278594970703\n",
            "Epoch: 1, Training Loss: 2.2279419898986816\n",
            "Epoch: 1, Training Loss: 2.401717185974121\n",
            "Epoch: 1, Training Loss: 2.7343974113464355\n",
            "Epoch: 1, Training Loss: 2.4204373359680176\n",
            "Epoch: 1, Training Loss: 2.286101818084717\n",
            "Epoch: 1, Training Loss: 2.184380531311035\n",
            "Epoch: 1, Training Loss: 2.3523638248443604\n",
            "Epoch: 1, Training Loss: 2.2476391792297363\n",
            "Epoch: 1, Training Loss: 2.4678914546966553\n",
            "Epoch: 1, Training Loss: 2.518059253692627\n",
            "Epoch: 1, Training Loss: 2.6857032775878906\n",
            "Epoch: 1, Training Loss: 2.349151134490967\n",
            "Epoch: 1, Training Loss: 2.1692845821380615\n",
            "Epoch: 1, Training Loss: 2.2942051887512207\n",
            "Epoch: 1, Training Loss: 2.378199338912964\n",
            "Epoch: 1, Training Loss: 2.426309585571289\n",
            "Epoch: 1, Training Loss: 2.6893303394317627\n",
            "Epoch: 1, Training Loss: 2.6233677864074707\n",
            "Epoch: 1, Training Loss: 2.1219475269317627\n",
            "Epoch: 1, Training Loss: 2.3208751678466797\n",
            "Epoch: 1, Training Loss: 2.459064245223999\n",
            "Epoch: 1, Training Loss: 2.3488271236419678\n",
            "Epoch: 1, Training Loss: 2.494960069656372\n",
            "Epoch: 1, Training Loss: 2.2632620334625244\n",
            "Epoch: 1, Training Loss: 2.377622127532959\n",
            "Epoch: 1, Training Loss: 2.2523181438446045\n",
            "Epoch: 1, Training Loss: 2.5943078994750977\n",
            "Epoch: 1, Training Loss: 2.7338900566101074\n",
            "Epoch: 1, Training Loss: 2.6434521675109863\n",
            "Epoch: 1, Training Loss: 2.2755560874938965\n",
            "Epoch: 1, Training Loss: 2.7366347312927246\n",
            "Epoch: 1, Training Loss: 2.3622806072235107\n",
            "Epoch: 1, Training Loss: 2.050173282623291\n",
            "Epoch: 1, Training Loss: 2.6343979835510254\n",
            "Epoch: 1, Training Loss: 2.3654537200927734\n",
            "Epoch: 1, Training Loss: 2.147853374481201\n",
            "Epoch: 1, Training Loss: 2.3116931915283203\n",
            "Epoch: 1, Training Loss: 2.1378133296966553\n",
            "Epoch: 1, Training Loss: 2.5734310150146484\n",
            "Epoch: 1, Training Loss: 2.0100741386413574\n",
            "Epoch: 1, Training Loss: 2.4019417762756348\n",
            "Epoch: 1, Training Loss: 2.6137075424194336\n",
            "Epoch: 1, Training Loss: 2.7909791469573975\n",
            "Epoch: 1, Training Loss: 2.0452654361724854\n",
            "Epoch: 1, Training Loss: 2.4943959712982178\n",
            "Epoch: 1, Training Loss: 2.338944911956787\n",
            "Epoch: 1, Training Loss: 2.5563924312591553\n",
            "Epoch: 1, Training Loss: 2.5877578258514404\n",
            "Epoch: 1, Training Loss: 2.277721643447876\n",
            "Epoch: 1, Training Loss: 2.24949049949646\n",
            "Epoch: 1, Training Loss: 2.18025541305542\n",
            "Epoch: 1, Training Loss: 2.313486337661743\n",
            "Epoch: 1, Training Loss: 2.242091178894043\n",
            "Epoch: 1, Training Loss: 2.367905616760254\n",
            "Epoch: 1, Training Loss: 2.4390573501586914\n",
            "Epoch: 1, Training Loss: 2.526975393295288\n",
            "Epoch: 1, Training Loss: 2.388981580734253\n",
            "Epoch: 1, Training Loss: 2.423276662826538\n",
            "Epoch: 1, Training Loss: 2.5610721111297607\n",
            "Epoch: 1, Training Loss: 2.180943727493286\n",
            "Epoch: 1, Training Loss: 2.1632137298583984\n",
            "Epoch: 1, Training Loss: 2.7552967071533203\n",
            "Epoch: 1, Training Loss: 2.2187182903289795\n",
            "Epoch: 1, Training Loss: 2.483767032623291\n",
            "Epoch: 1, Training Loss: 2.4866068363189697\n",
            "Epoch: 1, Training Loss: 2.201597213745117\n",
            "Epoch: 1, Training Loss: 2.2446768283843994\n",
            "Epoch: 1, Training Loss: 2.4531068801879883\n",
            "Epoch: 1, Training Loss: 2.3309390544891357\n",
            "Epoch: 1, Training Loss: 2.255122423171997\n",
            "Epoch: 1, Training Loss: 2.3438880443573\n",
            "Epoch: 1, Training Loss: 2.65630841255188\n",
            "Epoch: 1, Training Loss: 2.465090751647949\n",
            "Epoch: 1, Training Loss: 2.2180066108703613\n",
            "Epoch: 1, Training Loss: 2.5240039825439453\n",
            "Epoch: 1, Training Loss: 2.160890579223633\n",
            "Epoch: 1, Training Loss: 2.3389477729797363\n",
            "Epoch: 1, Training Loss: 2.277919292449951\n",
            "Epoch: 1, Training Loss: 2.2476258277893066\n",
            "Epoch: 1, Training Loss: 2.381772041320801\n",
            "Epoch: 1, Training Loss: 2.1871798038482666\n",
            "Epoch: 1, Training Loss: 2.2985029220581055\n",
            "Epoch: 1, Training Loss: 2.170006513595581\n",
            "Epoch: 1, Training Loss: 2.3715708255767822\n",
            "Epoch: 1, Training Loss: 2.3764450550079346\n",
            "Epoch: 1, Training Loss: 2.3083529472351074\n",
            "Epoch: 1, Training Loss: 2.622692823410034\n",
            "Epoch: 1, Training Loss: 2.161494255065918\n",
            "Epoch: 1, Training Loss: 2.4971072673797607\n",
            "Epoch: 1, Training Loss: 2.071293830871582\n",
            "Epoch: 1, Training Loss: 2.390324592590332\n",
            "Epoch: 1, Training Loss: 2.8288111686706543\n",
            "Epoch: 1, Training Loss: 2.275563955307007\n",
            "Epoch: 1, Training Loss: 2.2111213207244873\n",
            "Epoch: 1, Training Loss: 2.313204288482666\n",
            "Epoch: 1, Training Loss: 2.3580403327941895\n",
            "Epoch: 1, Training Loss: 2.445708990097046\n",
            "Epoch: 1, Training Loss: 2.365088939666748\n",
            "Epoch: 1, Training Loss: 2.6567347049713135\n",
            "Epoch: 1, Training Loss: 2.1900100708007812\n",
            "Epoch: 1, Training Loss: 2.0312836170196533\n",
            "Epoch: 1, Training Loss: 2.452826976776123\n",
            "Epoch: 1, Training Loss: 2.267070770263672\n",
            "Epoch: 1, Training Loss: 2.6887221336364746\n",
            "Epoch: 1, Training Loss: 2.2721829414367676\n",
            "Epoch: 1, Training Loss: 2.528903007507324\n",
            "Epoch: 1, Training Loss: 2.324591636657715\n",
            "Epoch: 1, Training Loss: 2.1703641414642334\n",
            "Epoch: 1, Training Loss: 2.410719156265259\n",
            "Epoch: 1, Training Loss: 2.3542487621307373\n",
            "Epoch: 1, Training Loss: 2.31235933303833\n",
            "Epoch: 1, Training Loss: 2.1212151050567627\n",
            "Epoch: 1, Training Loss: 2.668614625930786\n",
            "Epoch: 1, Training Loss: 2.2344298362731934\n",
            "Epoch: 1, Training Loss: 2.6101467609405518\n",
            "Epoch: 1, Training Loss: 2.305863380432129\n",
            "Epoch: 1, Training Loss: 2.709780216217041\n",
            "Epoch: 1, Training Loss: 2.547027587890625\n",
            "Epoch: 1, Training Loss: 2.359426259994507\n",
            "Epoch: 1, Training Loss: 2.2236344814300537\n",
            "Epoch: 1, Training Loss: 2.485783815383911\n",
            "Epoch: 1, Training Loss: 2.288114070892334\n",
            "Epoch: 1, Training Loss: 2.3071014881134033\n",
            "Epoch: 1, Training Loss: 2.085690498352051\n",
            "Epoch: 1, Training Loss: 2.284419059753418\n",
            "Epoch: 1, Training Loss: 2.3119611740112305\n",
            "Epoch: 1, Training Loss: 2.2672810554504395\n",
            "Epoch: 1, Training Loss: 2.2515504360198975\n",
            "Epoch: 1, Training Loss: 2.3252220153808594\n",
            "Epoch: 1, Training Loss: 2.5221896171569824\n",
            "Epoch: 1, Training Loss: 2.278550624847412\n",
            "Epoch: 1, Training Loss: 2.4052062034606934\n",
            "Epoch: 1, Training Loss: 2.300123691558838\n",
            "Epoch: 1, Training Loss: 2.1109540462493896\n",
            "Epoch: 1, Training Loss: 2.4382123947143555\n",
            "Epoch: 1, Training Loss: 2.4615378379821777\n",
            "Epoch: 1, Training Loss: 2.476857900619507\n",
            "Epoch: 1, Training Loss: 2.3013815879821777\n",
            "Epoch: 1, Training Loss: 2.1466777324676514\n",
            "Epoch: 1, Training Loss: 2.657039165496826\n",
            "Epoch: 1, Training Loss: 2.372551202774048\n",
            "Epoch: 1, Training Loss: 2.1504204273223877\n",
            "Epoch: 1, Training Loss: 2.146700859069824\n",
            "Epoch: 1, Training Loss: 2.3946545124053955\n",
            "Epoch: 1, Training Loss: 2.720184326171875\n",
            "Epoch: 1, Training Loss: 2.425579071044922\n",
            "Epoch: 1, Training Loss: 2.4137744903564453\n",
            "Epoch: 1, Training Loss: 2.213759422302246\n",
            "Epoch: 1, Training Loss: 2.307347297668457\n",
            "Epoch: 1, Training Loss: 2.5317678451538086\n",
            "Epoch: 1, Training Loss: 2.6806626319885254\n",
            "Epoch: 1, Training Loss: 2.464761257171631\n",
            "Epoch: 1, Training Loss: 2.2300527095794678\n",
            "Epoch: 1, Training Loss: 2.0881123542785645\n",
            "Epoch: 1, Training Loss: 2.3040449619293213\n",
            "Epoch: 1, Training Loss: 2.3806240558624268\n",
            "Epoch: 1, Training Loss: 2.4997901916503906\n",
            "Epoch: 1, Training Loss: 2.34175181388855\n",
            "Epoch: 1, Training Loss: 2.21762752532959\n",
            "Epoch: 1, Training Loss: 2.1316990852355957\n",
            "Epoch: 1, Training Loss: 2.2703607082366943\n",
            "Epoch: 1, Training Loss: 2.4729835987091064\n",
            "Epoch: 1, Training Loss: 2.1986465454101562\n",
            "Epoch: 1, Training Loss: 2.294844150543213\n",
            "Epoch: 1, Training Loss: 2.2075657844543457\n",
            "Epoch: 1, Training Loss: 2.601972818374634\n",
            "Epoch: 1, Training Loss: 2.313363790512085\n",
            "Epoch: 1, Training Loss: 1.9567989110946655\n",
            "Epoch: 1, Training Loss: 2.4262566566467285\n",
            "Epoch: 1, Training Loss: 2.158073663711548\n",
            "Epoch: 1, Training Loss: 2.2002251148223877\n",
            "Epoch: 1, Training Loss: 2.600360631942749\n",
            "Epoch: 1, Training Loss: 2.1793150901794434\n",
            "Epoch: 1, Training Loss: 2.489286184310913\n",
            "Epoch: 1, Training Loss: 2.1829183101654053\n",
            "Epoch: 1, Training Loss: 2.2042109966278076\n",
            "Epoch: 1, Training Loss: 2.2827515602111816\n",
            "Epoch: 1, Training Loss: 2.3609845638275146\n",
            "Epoch: 1, Training Loss: 2.3160171508789062\n",
            "Epoch: 1, Training Loss: 2.4827988147735596\n",
            "Epoch: 1, Training Loss: 2.184978485107422\n",
            "Epoch: 1, Training Loss: 2.40588641166687\n",
            "Epoch: 1, Training Loss: 2.0751304626464844\n",
            "Epoch: 1, Training Loss: 2.3804948329925537\n",
            "Epoch: 1, Training Loss: 1.9921965599060059\n",
            "Epoch: 1, Training Loss: 2.3047361373901367\n",
            "Epoch: 1, Training Loss: 2.33480167388916\n",
            "Epoch: 1, Training Loss: 2.2348403930664062\n",
            "Epoch: 1, Training Loss: 2.110687255859375\n",
            "Epoch: 1, Training Loss: 2.218479633331299\n",
            "Epoch: 1, Training Loss: 2.4246976375579834\n",
            "Epoch: 1, Training Loss: 2.2367708683013916\n",
            "Epoch: 1, Training Loss: 2.5168614387512207\n",
            "Epoch: 1, Training Loss: 2.6674509048461914\n",
            "Epoch: 1, Training Loss: 2.3801426887512207\n",
            "Epoch: 1, Training Loss: 2.212660789489746\n",
            "Epoch: 1, Training Loss: 2.457230567932129\n",
            "Epoch: 1, Training Loss: 1.9581458568572998\n",
            "Epoch: 1, Training Loss: 2.665807008743286\n",
            "Epoch: 1, Training Loss: 2.470597505569458\n",
            "Epoch: 1, Training Loss: 2.257122278213501\n",
            "Epoch: 1, Training Loss: 2.3586478233337402\n",
            "Epoch: 1, Training Loss: 2.27502703666687\n",
            "Epoch: 1, Training Loss: 2.262180805206299\n",
            "Epoch: 1, Training Loss: 2.319242238998413\n",
            "Epoch: 1, Training Loss: 2.311957836151123\n",
            "Epoch: 1, Training Loss: 2.4097628593444824\n",
            "Epoch: 1, Training Loss: 1.9675372838974\n",
            "Epoch: 1, Training Loss: 2.341059684753418\n",
            "Epoch: 1, Training Loss: 1.982832431793213\n",
            "Epoch: 1, Training Loss: 2.5568268299102783\n",
            "Epoch: 1, Training Loss: 2.1316778659820557\n",
            "Epoch: 1, Training Loss: 2.288616895675659\n",
            "Epoch: 1, Training Loss: 2.5214591026306152\n",
            "Epoch: 1, Training Loss: 2.240225315093994\n",
            "Epoch: 1, Training Loss: 2.2093260288238525\n",
            "Epoch: 1, Training Loss: 2.4727401733398438\n",
            "Epoch: 1, Training Loss: 1.972926378250122\n",
            "Epoch: 1, Training Loss: 2.368840217590332\n",
            "Epoch: 1, Training Loss: 2.336822986602783\n",
            "Epoch: 1, Training Loss: 2.456573486328125\n",
            "Epoch: 1, Training Loss: 2.155107021331787\n",
            "Epoch: 1, Training Loss: 2.5553033351898193\n",
            "Epoch: 1, Training Loss: 2.3200340270996094\n",
            "Epoch: 1, Training Loss: 2.4650425910949707\n",
            "Epoch: 1, Training Loss: 2.314962387084961\n",
            "Epoch: 1, Training Loss: 2.2518656253814697\n",
            "Epoch: 1, Training Loss: 2.4121193885803223\n",
            "Epoch: 1, Training Loss: 2.221350908279419\n",
            "Epoch: 1, Training Loss: 2.2574872970581055\n",
            "Epoch: 1, Training Loss: 2.464574098587036\n",
            "Epoch: 1, Training Loss: 1.9162142276763916\n",
            "Epoch: 1, Training Loss: 2.1686034202575684\n",
            "Epoch: 1, Training Loss: 2.3967199325561523\n",
            "Epoch: 1, Training Loss: 2.2341973781585693\n",
            "Epoch: 1, Training Loss: 2.264798641204834\n",
            "Epoch: 1, Training Loss: 2.1149444580078125\n",
            "Epoch: 1, Training Loss: 2.8479485511779785\n",
            "Epoch: 1, Training Loss: 2.378553867340088\n",
            "Epoch: 1, Training Loss: 2.1155028343200684\n",
            "Epoch: 1, Training Loss: 2.3357784748077393\n",
            "Epoch: 1, Training Loss: 2.5384976863861084\n",
            "Epoch: 1, Training Loss: 2.3287858963012695\n",
            "Epoch: 1, Training Loss: 2.3490896224975586\n",
            "Epoch: 1, Training Loss: 2.102133274078369\n",
            "Epoch: 1, Training Loss: 2.41321063041687\n",
            "Epoch: 1, Training Loss: 2.3994905948638916\n",
            "Epoch: 1, Training Loss: 2.365550994873047\n",
            "Epoch: 1, Training Loss: 2.192903757095337\n",
            "Epoch: 1, Training Loss: 2.5407662391662598\n",
            "Epoch: 1, Training Loss: 2.243940591812134\n",
            "Epoch: 1, Training Loss: 2.358488082885742\n",
            "Epoch: 1, Training Loss: 2.262665271759033\n",
            "Epoch: 1, Training Loss: 2.279557466506958\n",
            "Epoch: 1, Training Loss: 2.5888102054595947\n",
            "Epoch: 1, Training Loss: 2.235360622406006\n",
            "Epoch: 1, Training Loss: 2.2019200325012207\n",
            "Epoch: 1, Training Loss: 2.2574024200439453\n",
            "Epoch: 1, Training Loss: 2.3880109786987305\n",
            "Epoch: 1, Training Loss: 2.2035977840423584\n",
            "Epoch: 1, Training Loss: 2.160768747329712\n",
            "Epoch: 1, Training Loss: 2.3743667602539062\n",
            "Epoch: 1, Training Loss: 2.4907760620117188\n",
            "Epoch: 1, Training Loss: 2.4446969032287598\n",
            "Epoch: 1, Training Loss: 2.0462403297424316\n",
            "Epoch: 1, Training Loss: 2.3811194896698\n",
            "Epoch: 1, Training Loss: 2.200833320617676\n",
            "Epoch: 1, Training Loss: 2.459397315979004\n",
            "Epoch: 1, Training Loss: 2.29467511177063\n",
            "Epoch: 1, Training Loss: 2.6864113807678223\n",
            "Epoch: 1, Training Loss: 2.0358457565307617\n",
            "Epoch: 1, Training Loss: 2.0995969772338867\n",
            "Epoch: 1, Training Loss: 2.2555348873138428\n",
            "Epoch: 1, Training Loss: 2.1433489322662354\n",
            "Epoch: 1, Training Loss: 2.0048396587371826\n",
            "Epoch: 1, Training Loss: 2.318310499191284\n",
            "Epoch: 1, Training Loss: 2.302046537399292\n",
            "Epoch: 1, Training Loss: 2.248302459716797\n",
            "Epoch: 1, Training Loss: 2.3386194705963135\n",
            "Epoch: 1, Training Loss: 2.374788999557495\n",
            "Epoch: 1, Training Loss: 2.1632280349731445\n",
            "Epoch: 1, Training Loss: 2.4674465656280518\n",
            "Epoch: 1, Training Loss: 2.2031161785125732\n",
            "Epoch: 1, Training Loss: 2.1841752529144287\n",
            "Epoch: 1, Training Loss: 2.309483051300049\n",
            "Epoch: 1, Training Loss: 2.203383684158325\n",
            "Epoch: 1, Training Loss: 2.331685781478882\n",
            "Epoch: 1, Training Loss: 2.1280603408813477\n",
            "Epoch: 1, Training Loss: 2.210646629333496\n",
            "Epoch: 1, Training Loss: 2.237755060195923\n",
            "Epoch: 1, Training Loss: 2.452009439468384\n",
            "Epoch: 1, Training Loss: 1.995864987373352\n",
            "Epoch: 1, Training Loss: 2.4289088249206543\n",
            "Epoch: 1, Training Loss: 2.095808506011963\n",
            "Epoch: 1, Training Loss: 2.1086525917053223\n",
            "Epoch: 1, Training Loss: 2.086188316345215\n",
            "Epoch: 1, Training Loss: 2.2648956775665283\n",
            "Epoch: 1, Training Loss: 2.206348180770874\n",
            "Epoch: 1, Training Loss: 2.432741641998291\n",
            "Epoch: 1, Training Loss: 2.291057825088501\n",
            "Epoch: 1, Training Loss: 2.3388187885284424\n",
            "Epoch: 1, Training Loss: 2.159487009048462\n",
            "Epoch: 1, Training Loss: 2.192042112350464\n",
            "Epoch: 1, Training Loss: 2.195831775665283\n",
            "Epoch: 1, Training Loss: 2.3734748363494873\n",
            "Epoch: 1, Training Loss: 2.1827826499938965\n",
            "Epoch: 1, Training Loss: 1.9833401441574097\n",
            "Epoch: 1, Training Loss: 2.52840256690979\n",
            "Epoch: 1, Training Loss: 2.3066916465759277\n",
            "Epoch: 1, Training Loss: 2.2993874549865723\n",
            "Epoch: 1, Training Loss: 2.497793436050415\n",
            "Epoch: 1, Training Loss: 2.5328755378723145\n",
            "Epoch: 1, Training Loss: 2.3539938926696777\n",
            "Epoch: 1, Training Loss: 2.4000051021575928\n",
            "Epoch: 1, Training Loss: 2.166365623474121\n",
            "Epoch: 1, Training Loss: 2.3778629302978516\n",
            "Epoch: 1, Training Loss: 2.244995594024658\n",
            "Epoch: 1, Training Loss: 2.2302424907684326\n",
            "Epoch: 1, Training Loss: 2.259132146835327\n",
            "Epoch: 1, Training Loss: 2.2940263748168945\n",
            "Epoch: 1, Training Loss: 2.483761787414551\n",
            "Epoch: 1, Training Loss: 2.354053258895874\n",
            "Epoch: 1, Training Loss: 2.3194596767425537\n",
            "Epoch: 1, Training Loss: 2.04899001121521\n",
            "Epoch: 1, Training Loss: 2.341254234313965\n",
            "Epoch: 1, Training Loss: 2.2365827560424805\n",
            "Epoch: 1, Training Loss: 1.940687894821167\n",
            "Epoch: 1, Training Loss: 2.0107054710388184\n",
            "Epoch: 1, Training Loss: 2.2511885166168213\n",
            "Epoch: 1, Training Loss: 2.308190107345581\n",
            "Epoch: 1, Training Loss: 2.325751781463623\n",
            "Epoch: 1, Training Loss: 1.9516102075576782\n",
            "Epoch: 1, Training Loss: 2.47248911857605\n",
            "Epoch: 1, Training Loss: 1.9034720659255981\n",
            "Epoch: 1, Training Loss: 2.125129222869873\n",
            "Epoch: 1, Training Loss: 2.10331392288208\n",
            "Epoch: 1, Training Loss: 2.443075656890869\n",
            "Epoch: 1, Training Loss: 2.1520354747772217\n",
            "Epoch: 1, Training Loss: 2.2286243438720703\n",
            "Epoch: 1, Training Loss: 2.1417994499206543\n",
            "Epoch: 1, Training Loss: 2.1067888736724854\n",
            "Epoch: 1, Training Loss: 2.169675827026367\n",
            "Epoch: 1, Training Loss: 2.4832749366760254\n",
            "Epoch: 1, Training Loss: 2.184237480163574\n",
            "Epoch: 1, Training Loss: 2.1342263221740723\n",
            "Epoch: 1, Training Loss: 2.2099990844726562\n",
            "Epoch: 1, Training Loss: 2.393998622894287\n",
            "Epoch: 1, Training Loss: 2.400388240814209\n",
            "Epoch: 1, Training Loss: 2.322824001312256\n",
            "Epoch: 1, Training Loss: 2.253784656524658\n",
            "Epoch: 1, Training Loss: 2.274585247039795\n",
            "Epoch: 1, Training Loss: 2.01741886138916\n",
            "Epoch: 1, Training Loss: 2.409841299057007\n",
            "Epoch: 1, Training Loss: 2.037713050842285\n",
            "Epoch: 1, Training Loss: 2.2732372283935547\n",
            "Epoch: 1, Training Loss: 2.331329345703125\n",
            "Epoch: 1, Training Loss: 2.2928125858306885\n",
            "Epoch: 1, Training Loss: 2.187507152557373\n",
            "Epoch: 1, Training Loss: 2.1877124309539795\n",
            "Epoch: 1, Training Loss: 2.3382365703582764\n",
            "Epoch: 1, Training Loss: 2.0919697284698486\n",
            "Epoch: 1, Training Loss: 2.4404642581939697\n",
            "Epoch: 1, Training Loss: 2.2652595043182373\n",
            "Epoch: 1, Training Loss: 1.9953911304473877\n",
            "Epoch: 1, Training Loss: 2.1587955951690674\n",
            "Epoch: 1, Training Loss: 1.9187785387039185\n",
            "Epoch: 1, Training Loss: 2.1520094871520996\n",
            "Epoch: 1, Training Loss: 2.337529182434082\n",
            "Epoch: 1, Training Loss: 2.1644232273101807\n",
            "Epoch: 1, Training Loss: 2.421415090560913\n",
            "Epoch: 1, Training Loss: 2.298774480819702\n",
            "Epoch: 1, Training Loss: 2.165099859237671\n",
            "Epoch: 1, Training Loss: 2.2907724380493164\n",
            "Epoch: 1, Training Loss: 2.2184970378875732\n",
            "Epoch: 1, Training Loss: 2.0717222690582275\n",
            "Epoch: 1, Training Loss: 2.142529010772705\n",
            "Epoch: 1, Training Loss: 2.3531062602996826\n",
            "Epoch: 1, Training Loss: 2.1511242389678955\n",
            "Epoch: 1, Training Loss: 2.2789902687072754\n",
            "Epoch: 1, Training Loss: 2.41925311088562\n",
            "Epoch: 1, Training Loss: 2.3751285076141357\n",
            "Epoch: 1, Training Loss: 2.254498243331909\n",
            "Epoch: 1, Training Loss: 2.0645461082458496\n",
            "Epoch: 1, Training Loss: 2.040564775466919\n",
            "Epoch: 1, Training Loss: 2.291691780090332\n",
            "Epoch: 1, Training Loss: 2.3194103240966797\n",
            "Epoch: 1, Training Loss: 2.207474708557129\n",
            "Epoch: 1, Training Loss: 2.3225173950195312\n",
            "Epoch: 1, Training Loss: 2.371561288833618\n",
            "Epoch: 1, Training Loss: 2.141806125640869\n",
            "Epoch: 1, Training Loss: 1.8836628198623657\n",
            "Epoch: 1, Training Loss: 2.434908151626587\n",
            "Epoch: 1, Training Loss: 2.106771230697632\n",
            "Epoch: 1, Training Loss: 2.479543685913086\n",
            "Epoch: 1, Training Loss: 2.267761707305908\n",
            "Epoch: 1, Training Loss: 2.2161192893981934\n",
            "Epoch: 1, Training Loss: 2.371417999267578\n",
            "Epoch: 1, Training Loss: 2.330320119857788\n",
            "Epoch: 1, Training Loss: 2.416186809539795\n",
            "Epoch: 1, Training Loss: 2.2411906719207764\n",
            "Epoch: 1, Training Loss: 2.1949515342712402\n",
            "Epoch: 1, Training Loss: 2.043731451034546\n",
            "Epoch: 1, Training Loss: 2.1207478046417236\n",
            "Epoch: 1, Training Loss: 2.377838611602783\n",
            "Epoch: 1, Training Loss: 2.25606107711792\n",
            "Epoch: 1, Training Loss: 2.5624592304229736\n",
            "Epoch: 1, Training Loss: 1.9992550611495972\n",
            "Epoch: 1, Training Loss: 2.112532138824463\n",
            "Epoch: 1, Training Loss: 1.9373990297317505\n",
            "Epoch: 1, Training Loss: 2.39729642868042\n",
            "Epoch: 1, Training Loss: 2.179094076156616\n",
            "Epoch: 1, Training Loss: 2.1148226261138916\n",
            "Epoch: 1, Training Loss: 2.0987420082092285\n",
            "Epoch: 1, Training Loss: 2.395908832550049\n",
            "Epoch: 1, Training Loss: 2.3195390701293945\n",
            "Epoch: 1, Training Loss: 2.186025619506836\n",
            "Epoch: 1, Training Loss: 1.952557921409607\n",
            "Epoch: 1, Training Loss: 2.129988670349121\n",
            "Epoch: 1, Training Loss: 2.2562615871429443\n",
            "Epoch: 1, Training Loss: 2.3696346282958984\n",
            "Epoch: 1, Training Loss: 2.526036262512207\n",
            "Epoch: 1, Training Loss: 2.475221872329712\n",
            "Epoch: 1, Training Loss: 1.9681484699249268\n",
            "Epoch: 1, Training Loss: 2.2809247970581055\n",
            "Epoch: 1, Training Loss: 2.2047290802001953\n",
            "Epoch: 1, Training Loss: 2.186018466949463\n",
            "Epoch: 1, Training Loss: 2.3373074531555176\n",
            "Epoch: 1, Training Loss: 2.3346705436706543\n",
            "Epoch: 1, Training Loss: 2.251100778579712\n",
            "Epoch: 1, Training Loss: 2.3690695762634277\n",
            "Epoch: 1, Training Loss: 2.214895009994507\n",
            "Epoch: 1, Training Loss: 2.1623783111572266\n",
            "Epoch: 1, Training Loss: 2.3668904304504395\n",
            "Epoch: 1, Training Loss: 1.8959397077560425\n",
            "Epoch: 1, Training Loss: 2.276825428009033\n",
            "Epoch: 1, Training Loss: 2.374769926071167\n",
            "Epoch: 1, Training Loss: 2.1651954650878906\n",
            "Epoch: 1, Training Loss: 2.0932457447052\n",
            "Epoch: 1, Training Loss: 2.248551368713379\n",
            "Epoch: 1, Training Loss: 2.141446113586426\n",
            "Epoch: 1, Training Loss: 2.2842679023742676\n",
            "Epoch: 1, Training Loss: 2.0698390007019043\n",
            "Epoch: 1, Training Loss: 2.5848357677459717\n",
            "Epoch: 1, Training Loss: 2.231846809387207\n",
            "Epoch: 1, Training Loss: 2.388514995574951\n",
            "Epoch: 1, Training Loss: 2.1483802795410156\n",
            "Epoch: 1, Training Loss: 2.2760815620422363\n",
            "Epoch: 1, Training Loss: 2.4798593521118164\n",
            "Epoch: 1, Training Loss: 2.5407824516296387\n",
            "Epoch: 1, Training Loss: 2.236421823501587\n",
            "Epoch: 1, Training Loss: 2.0913681983947754\n",
            "Epoch: 1, Training Loss: 2.3665099143981934\n",
            "Epoch: 1, Training Loss: 1.9757928848266602\n",
            "Epoch: 1, Training Loss: 2.3059003353118896\n",
            "Epoch: 1, Training Loss: 2.0158984661102295\n",
            "Epoch: 1, Training Loss: 2.352362632751465\n",
            "Epoch: 1, Training Loss: 2.1963398456573486\n",
            "Epoch: 1, Training Loss: 2.509906530380249\n",
            "Epoch: 1, Training Loss: 2.039651870727539\n",
            "Epoch: 1, Training Loss: 2.438164710998535\n",
            "Epoch: 1, Training Loss: 2.105118989944458\n",
            "Epoch: 1, Training Loss: 2.0076255798339844\n",
            "Epoch: 1, Training Loss: 2.2960140705108643\n",
            "Epoch: 1, Training Loss: 2.470684766769409\n",
            "Epoch: 1, Training Loss: 2.2263023853302\n",
            "Epoch: 1, Training Loss: 2.3016622066497803\n",
            "Epoch: 1, Training Loss: 2.204793691635132\n",
            "Epoch: 1, Training Loss: 2.290006399154663\n",
            "Epoch: 1, Training Loss: 2.573885679244995\n",
            "Epoch: 1, Training Loss: 2.267483949661255\n",
            "Epoch: 1, Training Loss: 2.047873020172119\n",
            "Epoch: 1, Training Loss: 2.5579257011413574\n",
            "Epoch: 1, Training Loss: 2.104834794998169\n",
            "Epoch: 1, Training Loss: 2.3839826583862305\n",
            "Epoch: 1, Training Loss: 2.371492862701416\n",
            "Epoch: 1, Training Loss: 2.3016912937164307\n",
            "Epoch: 1, Training Loss: 2.335570812225342\n",
            "Epoch: 1, Training Loss: 2.1482911109924316\n",
            "Epoch: 1, Training Loss: 2.156461000442505\n",
            "Epoch: 1, Training Loss: 2.0398123264312744\n",
            "Epoch: 1, Training Loss: 2.0759172439575195\n",
            "Epoch: 1, Training Loss: 2.37316632270813\n",
            "Epoch: 1, Training Loss: 2.2913694381713867\n",
            "Epoch: 1, Training Loss: 2.8180196285247803\n",
            "Epoch: 1, Training Loss: 2.2975900173187256\n",
            "Epoch: 1, Training Loss: 2.0375564098358154\n",
            "Epoch: 1, Training Loss: 2.121230363845825\n",
            "Epoch: 1, Training Loss: 2.1663103103637695\n",
            "Epoch: 1, Training Loss: 2.221975564956665\n",
            "Epoch: 1, Training Loss: 2.079549789428711\n",
            "Epoch: 1, Training Loss: 2.3461310863494873\n",
            "Epoch: 1, Training Loss: 2.2279677391052246\n",
            "Epoch: 1, Training Loss: 1.8777058124542236\n",
            "Epoch: 1, Training Loss: 2.0826828479766846\n",
            "Epoch: 1, Training Loss: 2.409060001373291\n",
            "Epoch: 1, Training Loss: 2.194713592529297\n",
            "Epoch: 1, Training Loss: 2.173020362854004\n",
            "Epoch: 1, Training Loss: 2.1940109729766846\n",
            "Epoch: 1, Training Loss: 2.068389654159546\n",
            "Epoch: 1, Training Loss: 2.317286968231201\n",
            "Epoch: 1, Training Loss: 2.360715866088867\n",
            "Epoch: 1, Training Loss: 2.0729868412017822\n",
            "Epoch: 1, Training Loss: 2.1353530883789062\n",
            "Epoch: 1, Training Loss: 2.331620931625366\n",
            "Epoch: 1, Training Loss: 2.315761089324951\n",
            "Epoch: 1, Training Loss: 2.261126756668091\n",
            "Epoch: 1, Training Loss: 2.135369300842285\n",
            "Epoch: 1, Training Loss: 2.198160171508789\n",
            "Epoch: 1, Training Loss: 2.456272602081299\n",
            "Epoch: 1, Training Loss: 2.2192704677581787\n",
            "Epoch: 1, Training Loss: 2.2077488899230957\n",
            "Epoch: 1, Training Loss: 2.427346706390381\n",
            "Epoch: 1, Training Loss: 2.1536519527435303\n",
            "Epoch: 1, Training Loss: 1.7842615842819214\n",
            "Epoch: 1, Training Loss: 1.9884105920791626\n",
            "Epoch: 1, Training Loss: 2.1238980293273926\n",
            "Epoch: 1, Training Loss: 2.2155985832214355\n",
            "Epoch: 1, Training Loss: 2.381558656692505\n",
            "Epoch: 1, Training Loss: 2.235599994659424\n",
            "Epoch: 1, Training Loss: 2.2384982109069824\n",
            "Epoch: 1, Training Loss: 2.028359889984131\n",
            "Epoch: 1, Training Loss: 2.2569737434387207\n",
            "Epoch: 1, Training Loss: 2.312783718109131\n",
            "Epoch: 1, Training Loss: 2.036278486251831\n",
            "Epoch: 1, Training Loss: 2.37304425239563\n",
            "Epoch: 1, Training Loss: 2.2080605030059814\n",
            "Epoch: 1, Training Loss: 2.125997304916382\n",
            "Epoch: 1, Training Loss: 2.2021968364715576\n",
            "Epoch: 1, Training Loss: 2.0320351123809814\n",
            "Epoch: 1, Training Loss: 2.1678152084350586\n",
            "Epoch: 1, Training Loss: 2.2362282276153564\n",
            "Epoch: 1, Training Loss: 2.1307318210601807\n",
            "Epoch: 1, Training Loss: 2.203935384750366\n",
            "Epoch: 1, Training Loss: 2.5822315216064453\n",
            "Epoch: 1, Training Loss: 2.3197519779205322\n",
            "Epoch: 1, Training Loss: 2.3562514781951904\n",
            "Epoch: 1, Training Loss: 2.121830701828003\n",
            "Epoch: 1, Training Loss: 2.227198839187622\n",
            "Epoch: 1, Training Loss: 2.2451395988464355\n",
            "Epoch: 1, Training Loss: 2.3353967666625977\n",
            "Epoch: 1, Training Loss: 1.9989513158798218\n",
            "Epoch: 1, Training Loss: 2.3753268718719482\n",
            "Epoch: 1, Training Loss: 2.2611892223358154\n",
            "Epoch: 1, Training Loss: 2.0756239891052246\n",
            "Epoch: 1, Training Loss: 2.189249277114868\n",
            "Epoch: 1, Training Loss: 2.2483420372009277\n",
            "Epoch: 1, Training Loss: 2.03068470954895\n",
            "Epoch: 1, Training Loss: 2.067067861557007\n",
            "Epoch: 1, Training Loss: 1.9806344509124756\n",
            "Epoch: 1, Training Loss: 2.1436054706573486\n",
            "Epoch: 1, Training Loss: 2.150631904602051\n",
            "Epoch: 1, Training Loss: 2.0548226833343506\n",
            "Epoch: 1, Training Loss: 2.3012115955352783\n",
            "Epoch: 1, Training Loss: 2.1292245388031006\n",
            "Epoch: 1, Training Loss: 2.1332123279571533\n",
            "Epoch: 1, Training Loss: 2.053501605987549\n",
            "Epoch: 1, Training Loss: 2.003648281097412\n",
            "Epoch: 1, Training Loss: 2.1758761405944824\n",
            "Epoch: 1, Training Loss: 2.063981533050537\n",
            "Epoch: 1, Training Loss: 1.9784727096557617\n",
            "Epoch: 1, Training Loss: 1.963761806488037\n",
            "Epoch: 1, Training Loss: 2.2188820838928223\n",
            "Epoch: 1, Training Loss: 2.188833475112915\n",
            "Epoch: 1, Training Loss: 2.1542739868164062\n",
            "Epoch: 1, Training Loss: 2.183398723602295\n",
            "Epoch: 1, Training Loss: 2.0282225608825684\n",
            "Epoch: 1, Training Loss: 2.070693016052246\n",
            "Epoch: 1, Training Loss: 2.204327344894409\n",
            "Epoch: 1, Training Loss: 1.988811731338501\n",
            "Epoch: 1, Training Loss: 2.1681041717529297\n",
            "Epoch: 1, Training Loss: 1.8811360597610474\n",
            "Epoch: 1, Training Loss: 2.2618134021759033\n",
            "Epoch: 1, Training Loss: 1.8764361143112183\n",
            "Epoch: 1, Training Loss: 2.2798643112182617\n",
            "Epoch: 1, Training Loss: 2.173020839691162\n",
            "Epoch: 1, Training Loss: 2.133296251296997\n",
            "Epoch: 1, Training Loss: 1.9748773574829102\n",
            "Epoch: 1, Training Loss: 2.291978359222412\n",
            "Epoch: 1, Training Loss: 2.206744432449341\n",
            "Epoch: 1, Training Loss: 2.039036512374878\n",
            "Epoch: 1, Training Loss: 2.475351333618164\n",
            "Epoch: 1, Training Loss: 2.435046434402466\n",
            "Epoch: 1, Training Loss: 2.073464870452881\n",
            "Epoch: 1, Training Loss: 2.1764042377471924\n",
            "Epoch: 1, Training Loss: 1.9589488506317139\n",
            "Epoch: 1, Training Loss: 2.161686420440674\n",
            "Epoch: 1, Training Loss: 2.094426393508911\n",
            "Epoch: 1, Training Loss: 2.0911030769348145\n",
            "Epoch: 1, Training Loss: 2.1492209434509277\n",
            "Epoch: 1, Training Loss: 2.431715250015259\n",
            "Epoch: 1, Training Loss: 2.240967273712158\n",
            "Epoch: 1, Training Loss: 1.978157639503479\n",
            "Epoch: 1, Training Loss: 2.0678062438964844\n",
            "Epoch: 1, Training Loss: 2.009458303451538\n",
            "Epoch: 1, Training Loss: 1.8375942707061768\n",
            "Epoch: 1, Training Loss: 2.2916903495788574\n",
            "Epoch: 1, Training Loss: 2.0396344661712646\n",
            "Epoch: 1, Training Loss: 2.1869027614593506\n",
            "Epoch: 1, Training Loss: 2.1281991004943848\n",
            "Epoch: 1, Training Loss: 2.1820082664489746\n",
            "Epoch: 1, Training Loss: 1.923783779144287\n",
            "Epoch: 1, Training Loss: 2.4829013347625732\n",
            "Epoch: 1, Training Loss: 2.2853314876556396\n",
            "Epoch: 1, Training Loss: 2.032041072845459\n",
            "Epoch: 1, Training Loss: 2.0745725631713867\n",
            "Epoch: 1, Training Loss: 2.0097429752349854\n",
            "Epoch: 1, Training Loss: 2.352386474609375\n",
            "Epoch: 1, Training Loss: 2.32841420173645\n",
            "Epoch: 1, Training Loss: 2.311845302581787\n",
            "Epoch: 1, Training Loss: 2.1478190422058105\n",
            "Epoch: 1, Training Loss: 2.2176661491394043\n",
            "Epoch: 1, Training Loss: 2.414246082305908\n",
            "Epoch: 1, Training Loss: 2.3391120433807373\n",
            "Epoch: 1, Training Loss: 2.2826037406921387\n",
            "Epoch: 1, Training Loss: 2.287702798843384\n",
            "Epoch: 1, Training Loss: 2.1997082233428955\n",
            "Epoch: 1, Training Loss: 2.546414852142334\n",
            "Epoch: 1, Training Loss: 2.3566031455993652\n",
            "Epoch: 1, Training Loss: 2.254101276397705\n",
            "Epoch: 1, Training Loss: 2.3509886264801025\n",
            "Epoch: 1, Training Loss: 2.2574682235717773\n",
            "Epoch: 1, Training Loss: 2.2527124881744385\n",
            "Epoch: 1, Training Loss: 2.4925413131713867\n",
            "Epoch: 1, Training Loss: 2.0791566371917725\n",
            "Epoch: 1, Training Loss: 2.0606720447540283\n",
            "Epoch: 1, Training Loss: 2.0553016662597656\n",
            "Epoch: 1, Training Loss: 2.1544077396392822\n",
            "Epoch: 1, Training Loss: 2.007831573486328\n",
            "Epoch: 1, Training Loss: 2.244635820388794\n",
            "Epoch: 1, Training Loss: 2.3229782581329346\n",
            "Epoch: 1, Training Loss: 2.395174264907837\n",
            "Epoch: 1, Training Loss: 1.8772019147872925\n",
            "Epoch: 1, Training Loss: 2.214751958847046\n",
            "Epoch: 1, Training Loss: 1.9099831581115723\n",
            "Epoch: 1, Training Loss: 2.422478199005127\n",
            "Epoch: 1, Training Loss: 1.9176459312438965\n",
            "Epoch: 1, Training Loss: 1.9958118200302124\n",
            "Epoch: 1, Training Loss: 2.1534535884857178\n",
            "Epoch: 1, Training Loss: 1.9700111150741577\n",
            "Epoch: 1, Training Loss: 2.1372413635253906\n",
            "Epoch: 1, Training Loss: 2.2955524921417236\n",
            "Epoch: 1, Training Loss: 1.896112084388733\n",
            "Epoch: 1, Training Loss: 2.0793893337249756\n",
            "Epoch: 1, Training Loss: 2.3278541564941406\n",
            "Epoch: 1, Training Loss: 1.8702367544174194\n",
            "Epoch: 1, Training Loss: 2.1846823692321777\n",
            "Epoch: 1, Training Loss: 2.0027756690979004\n",
            "Epoch: 1, Training Loss: 2.1370325088500977\n",
            "Epoch: 1, Training Loss: 2.083343982696533\n",
            "Epoch: 1, Training Loss: 2.084838390350342\n",
            "Epoch: 1, Training Loss: 2.3258144855499268\n",
            "Epoch: 1, Training Loss: 2.254350185394287\n",
            "Epoch: 1, Training Loss: 1.9665544033050537\n",
            "Epoch: 1, Training Loss: 2.1208336353302\n",
            "Epoch: 1, Training Loss: 2.2814650535583496\n",
            "Epoch: 1, Training Loss: 2.3037219047546387\n",
            "Epoch: 1, Training Loss: 2.2967002391815186\n",
            "Epoch: 1, Training Loss: 2.056497097015381\n",
            "Epoch: 1, Training Loss: 2.1484715938568115\n",
            "Epoch: 1, Training Loss: 2.112097978591919\n",
            "Epoch: 1, Training Loss: 2.434710741043091\n",
            "Epoch: 1, Training Loss: 2.0701050758361816\n",
            "Epoch: 1, Training Loss: 2.1116206645965576\n",
            "Epoch: 1, Training Loss: 2.274869441986084\n",
            "Epoch: 1, Training Loss: 2.0243892669677734\n",
            "Epoch: 1, Training Loss: 1.9968934059143066\n",
            "Epoch: 1, Training Loss: 2.4386723041534424\n",
            "Epoch: 1, Training Loss: 2.3918190002441406\n",
            "Epoch: 1, Training Loss: 2.3246524333953857\n",
            "Epoch: 1, Training Loss: 2.120130777359009\n",
            "Epoch: 1, Training Loss: 2.175489902496338\n",
            "Epoch: 1, Training Loss: 2.032438278198242\n",
            "Epoch: 1, Training Loss: 2.2153589725494385\n",
            "Epoch: 1, Training Loss: 1.9534164667129517\n",
            "Epoch: 1, Training Loss: 2.2254390716552734\n",
            "Epoch: 1, Training Loss: 2.1231296062469482\n",
            "Epoch: 1, Training Loss: 2.150380849838257\n",
            "Epoch: 1, Training Loss: 2.361341714859009\n",
            "Epoch: 1, Training Loss: 2.0953915119171143\n",
            "Epoch: 1, Training Loss: 2.2442469596862793\n",
            "Epoch: 1, Training Loss: 2.222744941711426\n",
            "Epoch: 1, Training Loss: 2.1789798736572266\n",
            "Epoch: 1, Training Loss: 2.226409673690796\n",
            "Epoch: 1, Training Loss: 1.9379823207855225\n",
            "Epoch: 1, Training Loss: 2.135617733001709\n",
            "Epoch: 1, Training Loss: 2.296616315841675\n",
            "Epoch: 1, Training Loss: 2.3736557960510254\n",
            "Epoch: 1, Training Loss: 1.9387503862380981\n",
            "Epoch: 1, Training Loss: 1.8610903024673462\n",
            "Epoch: 1, Training Loss: 2.10557222366333\n",
            "Epoch: 1, Training Loss: 1.8869391679763794\n",
            "Epoch: 1, Training Loss: 2.1976230144500732\n",
            "Epoch: 1, Training Loss: 1.844498872756958\n",
            "Epoch: 1, Training Loss: 2.4180612564086914\n",
            "Epoch: 1, Training Loss: 2.11828875541687\n",
            "Epoch: 1, Training Loss: 2.118370771408081\n",
            "Epoch: 1, Training Loss: 1.8846955299377441\n",
            "Epoch: 1, Training Loss: 2.056702136993408\n",
            "Epoch: 1, Training Loss: 2.3945417404174805\n",
            "Epoch: 1, Training Loss: 2.0594959259033203\n",
            "Epoch: 1, Training Loss: 2.248788356781006\n",
            "Epoch: 1, Training Loss: 2.3523101806640625\n",
            "Epoch: 1, Training Loss: 2.134330987930298\n",
            "Epoch: 1, Training Loss: 2.057948112487793\n",
            "Epoch: 1, Training Loss: 2.208972454071045\n",
            "Epoch: 1, Training Loss: 2.0594661235809326\n",
            "Epoch: 1, Training Loss: 1.801425814628601\n",
            "Epoch: 1, Training Loss: 2.3064560890197754\n",
            "Epoch: 1, Training Loss: 1.818238377571106\n",
            "Epoch: 1, Training Loss: 1.9440187215805054\n",
            "Epoch: 1, Training Loss: 2.229151964187622\n",
            "Epoch: 1, Training Loss: 2.123508930206299\n",
            "Epoch: 1, Training Loss: 1.9224761724472046\n",
            "Epoch: 1, Training Loss: 2.0060606002807617\n",
            "Epoch: 1, Training Loss: 2.059582233428955\n",
            "Epoch: 1, Training Loss: 2.2042341232299805\n",
            "Epoch: 1, Training Loss: 2.423935651779175\n",
            "Epoch: 1, Training Loss: 1.9057042598724365\n",
            "Epoch: 1, Training Loss: 2.031960964202881\n",
            "Epoch: 1, Training Loss: 2.149679183959961\n",
            "Epoch: 1, Training Loss: 2.161491870880127\n",
            "Epoch: 1, Training Loss: 2.158284902572632\n",
            "Epoch: 1, Training Loss: 2.2162322998046875\n",
            "Epoch: 1, Training Loss: 2.434798002243042\n",
            "Epoch: 1, Training Loss: 2.114131212234497\n",
            "Epoch: 1, Training Loss: 1.9580135345458984\n",
            "Epoch: 1, Training Loss: 1.9213652610778809\n",
            "Epoch: 1, Training Loss: 2.0461795330047607\n",
            "Epoch: 1, Training Loss: 2.5323832035064697\n",
            "Epoch: 1, Training Loss: 2.1037395000457764\n",
            "Epoch: 1, Training Loss: 2.3001365661621094\n",
            "Epoch: 1, Training Loss: 2.2553253173828125\n",
            "Epoch: 1, Training Loss: 2.076287031173706\n",
            "Epoch: 1, Training Loss: 2.1496963500976562\n",
            "Epoch: 1, Training Loss: 2.021728515625\n",
            "Epoch: 1, Training Loss: 2.025381565093994\n",
            "Epoch: 1, Training Loss: 2.11924147605896\n",
            "Epoch: 1, Training Loss: 2.2178025245666504\n",
            "Epoch: 1, Training Loss: 2.1915249824523926\n",
            "Epoch: 1, Training Loss: 1.9946931600570679\n",
            "Epoch: 1, Training Loss: 1.9869263172149658\n",
            "Epoch: 1, Training Loss: 2.085982322692871\n",
            "Epoch: 1, Training Loss: 2.2733678817749023\n",
            "Epoch: 1, Training Loss: 1.9815735816955566\n",
            "Epoch: 1, Training Loss: 1.9650477170944214\n",
            "Epoch: 1, Training Loss: 2.035630464553833\n",
            "Epoch: 1, Training Loss: 1.8802874088287354\n",
            "Epoch: 1, Training Loss: 2.012913703918457\n",
            "Epoch: 1, Training Loss: 2.141857385635376\n",
            "Epoch: 1, Training Loss: 2.1722986698150635\n",
            "Epoch: 1, Training Loss: 2.165566921234131\n",
            "Epoch: 1, Training Loss: 2.13097882270813\n",
            "Epoch: 1, Training Loss: 1.9230718612670898\n",
            "Epoch: 1, Training Loss: 2.2505178451538086\n",
            "Epoch: 1, Training Loss: 2.0621142387390137\n",
            "Epoch: 1, Training Loss: 2.411397695541382\n",
            "Epoch: 1, Training Loss: 2.313856601715088\n",
            "Epoch: 1, Training Loss: 1.7743148803710938\n",
            "Epoch: 1, Training Loss: 2.2158796787261963\n",
            "Epoch: 1, Training Loss: 2.1670398712158203\n",
            "Epoch: 1, Training Loss: 2.23946475982666\n",
            "Epoch: 1, Training Loss: 2.1187617778778076\n",
            "Epoch: 1, Training Loss: 2.3762731552124023\n",
            "Epoch: 1, Training Loss: 2.1717617511749268\n",
            "Epoch: 1, Training Loss: 1.9447013139724731\n",
            "Epoch: 1, Training Loss: 2.0338289737701416\n",
            "Epoch: 1, Training Loss: 2.069939136505127\n",
            "Epoch: 1, Training Loss: 2.123260736465454\n",
            "Epoch: 1, Training Loss: 1.9995341300964355\n",
            "Epoch: 1, Training Loss: 2.0423433780670166\n",
            "Epoch: 1, Training Loss: 2.2028915882110596\n",
            "Epoch: 1, Training Loss: 2.2431557178497314\n",
            "Epoch: 1, Training Loss: 2.066067695617676\n",
            "Epoch: 1, Training Loss: 2.004262685775757\n",
            "Epoch: 1, Training Loss: 2.673454999923706\n",
            "Epoch: 1, Training Loss: 2.393939971923828\n",
            "Epoch: 1, Training Loss: 2.27847957611084\n",
            "Epoch: 1, Training Loss: 1.9683581590652466\n",
            "Epoch: 1, Training Loss: 1.768996238708496\n",
            "Epoch: 1, Training Loss: 2.0817112922668457\n",
            "Epoch: 1, Training Loss: 2.1993463039398193\n",
            "Epoch: 1, Training Loss: 1.9576427936553955\n",
            "Epoch: 1, Training Loss: 2.2196481227874756\n",
            "Epoch: 1, Training Loss: 1.8818055391311646\n",
            "Epoch: 1, Training Loss: 2.152460813522339\n",
            "Epoch: 1, Training Loss: 2.211167812347412\n",
            "Epoch: 1, Training Loss: 2.078873872756958\n",
            "Epoch: 1, Training Loss: 2.3931941986083984\n",
            "Epoch: 1, Training Loss: 2.0089962482452393\n",
            "Epoch: 1, Training Loss: 1.913118600845337\n",
            "Epoch: 1, Training Loss: 1.9627432823181152\n",
            "Epoch: 1, Training Loss: 1.9323346614837646\n",
            "Epoch: 1, Training Loss: 2.03723406791687\n",
            "Epoch: 1, Training Loss: 2.115476608276367\n",
            "Epoch: 1, Training Loss: 2.172994613647461\n",
            "Epoch: 1, Training Loss: 1.9065808057785034\n",
            "Epoch: 1, Training Loss: 2.1351499557495117\n",
            "Epoch: 1, Training Loss: 2.2164077758789062\n",
            "Epoch: 1, Training Loss: 2.037449598312378\n",
            "Epoch: 1, Training Loss: 2.0887370109558105\n",
            "Epoch: 1, Training Loss: 2.2761642932891846\n",
            "Epoch: 1, Training Loss: 2.2941770553588867\n",
            "Epoch: 1, Training Loss: 2.231471300125122\n",
            "Epoch: 1, Training Loss: 2.0260009765625\n",
            "Epoch: 1, Training Loss: 2.0929014682769775\n",
            "Epoch: 1, Training Loss: 2.0223734378814697\n",
            "Epoch: 1, Training Loss: 2.2855398654937744\n",
            "Epoch: 1, Training Loss: 1.8666750192642212\n",
            "Epoch: 1, Training Loss: 2.0586321353912354\n",
            "Epoch: 1, Training Loss: 2.2074639797210693\n",
            "Epoch: 1, Training Loss: 2.0717883110046387\n",
            "Epoch: 1, Training Loss: 2.0609829425811768\n",
            "Epoch: 1, Training Loss: 1.9749834537506104\n",
            "Epoch: 1, Training Loss: 2.2697298526763916\n",
            "Epoch: 1, Training Loss: 2.325777530670166\n",
            "Epoch: 1, Training Loss: 2.0839216709136963\n",
            "Epoch: 1, Training Loss: 2.2750935554504395\n",
            "Epoch: 1, Training Loss: 2.476317882537842\n",
            "Epoch: 1, Training Loss: 2.1060383319854736\n",
            "Epoch: 1, Training Loss: 2.016120433807373\n",
            "Epoch: 1, Training Loss: 1.8950601816177368\n",
            "Epoch: 1, Training Loss: 2.0632412433624268\n",
            "Epoch: 1, Training Loss: 2.1605911254882812\n",
            "Epoch: 1, Training Loss: 2.019301652908325\n",
            "Epoch: 1, Training Loss: 2.2014825344085693\n",
            "Epoch: 1, Training Loss: 2.2295637130737305\n",
            "Epoch: 1, Training Loss: 2.1872718334198\n",
            "Epoch: 1, Training Loss: 2.134796142578125\n",
            "Epoch: 1, Training Loss: 1.9581403732299805\n",
            "Epoch: 1, Training Loss: 2.1819682121276855\n",
            "Epoch: 1, Training Loss: 2.150134563446045\n",
            "Epoch: 1, Training Loss: 2.1584830284118652\n",
            "Epoch: 1, Training Loss: 2.1358540058135986\n",
            "Epoch: 1, Training Loss: 1.9742752313613892\n",
            "Epoch: 1, Training Loss: 2.1726367473602295\n",
            "Epoch: 1, Training Loss: 1.7666606903076172\n",
            "Epoch: 1, Training Loss: 2.0982701778411865\n",
            "Epoch: 1, Training Loss: 2.057187795639038\n",
            "Epoch: 1, Training Loss: 2.1601781845092773\n",
            "Epoch: 1, Training Loss: 2.358412742614746\n",
            "Epoch: 1, Training Loss: 1.8408540487289429\n",
            "Epoch: 1, Training Loss: 1.9752211570739746\n",
            "Epoch: 1, Training Loss: 1.932936191558838\n",
            "Epoch: 1, Training Loss: 1.9449957609176636\n",
            "Epoch: 1, Training Loss: 2.1346492767333984\n",
            "Epoch: 1, Training Loss: 2.292412042617798\n",
            "Epoch: 1, Training Loss: 2.2932686805725098\n",
            "Epoch: 1, Training Loss: 1.8463090658187866\n",
            "Epoch: 1, Training Loss: 1.8480995893478394\n",
            "Epoch: 1, Training Loss: 2.4265968799591064\n",
            "Epoch: 1, Training Loss: 2.3466477394104004\n",
            "Epoch: 1, Training Loss: 1.8869425058364868\n",
            "Epoch: 1, Training Loss: 2.140989303588867\n",
            "Epoch: 1, Training Loss: 1.9535694122314453\n",
            "Epoch: 1, Training Loss: 2.0779550075531006\n",
            "Epoch: 1, Training Loss: 1.7604761123657227\n",
            "Epoch: 1, Training Loss: 2.2961761951446533\n",
            "Epoch: 1, Training Loss: 2.2734463214874268\n",
            "Epoch: 1, Training Loss: 1.994807243347168\n",
            "Epoch: 1, Training Loss: 2.3667500019073486\n",
            "Epoch: 1, Training Loss: 2.536652088165283\n",
            "Epoch: 1, Training Loss: 2.1937577724456787\n",
            "Epoch: 1, Training Loss: 2.1239712238311768\n",
            "Epoch: 1, Training Loss: 1.9398415088653564\n",
            "Epoch: 1, Training Loss: 2.1451804637908936\n",
            "Epoch: 1, Training Loss: 1.9959579706192017\n",
            "Epoch: 1, Training Loss: 2.065142869949341\n",
            "Epoch: 1, Training Loss: 1.9381663799285889\n",
            "Epoch: 1, Training Loss: 2.0829720497131348\n",
            "Epoch: 1, Training Loss: 2.129169464111328\n",
            "Epoch: 1, Training Loss: 2.3791940212249756\n",
            "Epoch: 1, Training Loss: 2.095282554626465\n",
            "Epoch: 1, Training Loss: 2.079685688018799\n",
            "Epoch: 1, Training Loss: 1.9335638284683228\n",
            "Epoch: 1, Training Loss: 1.957899570465088\n",
            "Epoch: 1, Training Loss: 2.4443719387054443\n",
            "Epoch: 1, Training Loss: 2.0835249423980713\n",
            "Epoch: 1, Training Loss: 2.4263782501220703\n",
            "Epoch: 1, Training Loss: 1.934543490409851\n",
            "Epoch: 1, Training Loss: 1.8596248626708984\n",
            "Epoch: 1, Training Loss: 2.1898860931396484\n",
            "Epoch: 1, Training Loss: 2.226212739944458\n",
            "Epoch: 1, Training Loss: 1.9019114971160889\n",
            "Epoch: 1, Training Loss: 2.042224645614624\n",
            "Epoch: 1, Training Loss: 2.361025333404541\n",
            "Epoch: 1, Training Loss: 2.201220750808716\n",
            "Epoch: 1, Training Loss: 2.073739528656006\n",
            "Epoch: 1, Training Loss: 2.313145399093628\n",
            "Epoch: 1, Training Loss: 2.124462842941284\n",
            "Epoch: 1, Training Loss: 2.104623556137085\n",
            "Epoch: 1, Training Loss: 2.091590642929077\n",
            "Epoch: 1, Training Loss: 2.0365123748779297\n",
            "Epoch: 1, Training Loss: 2.2563536167144775\n",
            "Epoch: 1, Training Loss: 2.2164974212646484\n",
            "Epoch: 1, Training Loss: 2.189959764480591\n",
            "Epoch: 1, Training Loss: 2.0901408195495605\n",
            "Epoch: 1, Training Loss: 1.9203410148620605\n",
            "Epoch: 1, Training Loss: 1.966782808303833\n",
            "Epoch: 1, Training Loss: 1.9227023124694824\n",
            "Epoch: 1, Training Loss: 2.2426393032073975\n",
            "Epoch: 1, Training Loss: 2.193042516708374\n",
            "Epoch: 1, Training Loss: 2.1887102127075195\n",
            "Epoch: 1, Training Loss: 2.08656907081604\n",
            "Epoch: 1, Training Loss: 1.8716615438461304\n",
            "Epoch: 1, Training Loss: 2.342449426651001\n",
            "Epoch: 1, Training Loss: 2.4211645126342773\n",
            "Epoch: 1, Training Loss: 1.8129010200500488\n",
            "Epoch: 1, Training Loss: 2.0467283725738525\n",
            "Epoch: 1, Training Loss: 2.1053173542022705\n",
            "Epoch: 1, Training Loss: 1.9077394008636475\n",
            "Epoch: 1, Training Loss: 2.082824945449829\n",
            "Epoch: 1, Training Loss: 2.0695960521698\n",
            "Epoch: 1, Training Loss: 1.9130291938781738\n",
            "Epoch: 1, Training Loss: 2.0700340270996094\n",
            "Epoch: 1, Training Loss: 1.9345494508743286\n",
            "Epoch: 1, Training Loss: 2.016881227493286\n",
            "Epoch: 1, Training Loss: 2.4718515872955322\n",
            "Epoch: 1, Training Loss: 2.2072741985321045\n",
            "Epoch: 1, Training Loss: 2.1551215648651123\n",
            "Epoch: 1, Training Loss: 2.034518241882324\n",
            "Epoch: 1, Training Loss: 2.069791555404663\n",
            "Epoch: 1, Training Loss: 2.0989489555358887\n",
            "Epoch: 1, Training Loss: 2.078542947769165\n",
            "Epoch: 1, Training Loss: 2.1658976078033447\n",
            "Epoch: 1, Training Loss: 1.9345163106918335\n",
            "Epoch: 1, Training Loss: 1.9696624279022217\n",
            "Epoch: 1, Training Loss: 2.136686325073242\n",
            "Epoch: 1, Training Loss: 2.1224265098571777\n",
            "Epoch: 1, Training Loss: 2.1106252670288086\n",
            "Epoch: 1, Training Loss: 1.9095250368118286\n",
            "Epoch: 1, Training Loss: 2.1252267360687256\n",
            "Epoch: 1, Training Loss: 1.9120185375213623\n",
            "Epoch: 1, Training Loss: 2.0853195190429688\n",
            "Epoch: 1, Training Loss: 2.1173367500305176\n",
            "Epoch: 1, Training Loss: 1.8811498880386353\n",
            "Epoch: 1, Training Loss: 1.9301860332489014\n",
            "Epoch: 1, Training Loss: 2.1477317810058594\n",
            "Epoch: 1, Training Loss: 2.0779869556427\n",
            "Epoch: 1, Training Loss: 1.941339373588562\n",
            "Epoch: 1, Training Loss: 2.125251054763794\n",
            "Epoch: 1, Training Loss: 1.8457716703414917\n",
            "Epoch: 1, Training Loss: 2.303391695022583\n",
            "Epoch: 1, Training Loss: 2.03999400138855\n",
            "Epoch: 1, Training Loss: 1.9986488819122314\n",
            "Epoch: 1, Training Loss: 2.1150963306427\n",
            "Epoch: 1, Training Loss: 1.8048851490020752\n",
            "Epoch: 1, Training Loss: 1.7261457443237305\n",
            "Epoch: 1, Training Loss: 2.331441879272461\n",
            "Epoch: 1, Training Loss: 2.129338264465332\n",
            "Epoch: 1, Training Loss: 2.0174787044525146\n",
            "Epoch: 1, Training Loss: 1.8427443504333496\n",
            "Epoch: 1, Training Loss: 1.8796511888504028\n",
            "Epoch: 1, Training Loss: 1.9695593118667603\n",
            "Epoch: 1, Training Loss: 1.971623182296753\n",
            "Epoch: 1, Training Loss: 2.007234811782837\n",
            "Epoch: 1, Training Loss: 2.1067054271698\n",
            "Epoch: 1, Training Loss: 1.9637081623077393\n",
            "Epoch: 1, Training Loss: 1.9819847345352173\n",
            "Epoch: 1, Training Loss: 2.0858728885650635\n",
            "Epoch: 1, Training Loss: 2.1021578311920166\n",
            "Epoch: 1, Training Loss: 2.1593146324157715\n",
            "Epoch: 1, Training Loss: 2.2145261764526367\n",
            "Epoch: 1, Training Loss: 1.8491827249526978\n",
            "Epoch: 1, Training Loss: 2.524088144302368\n",
            "Epoch: 1, Training Loss: 1.9802021980285645\n",
            "Epoch: 1, Training Loss: 1.9948207139968872\n",
            "Epoch: 1, Training Loss: 2.0261096954345703\n",
            "Epoch: 1, Training Loss: 2.2749147415161133\n",
            "Epoch: 1, Training Loss: 2.2585763931274414\n",
            "Epoch: 1, Training Loss: 2.378291130065918\n",
            "Epoch: 1, Training Loss: 1.8074034452438354\n",
            "Epoch: 1, Training Loss: 2.356165885925293\n",
            "Epoch: 1, Training Loss: 1.8836472034454346\n",
            "Epoch: 1, Training Loss: 2.119912624359131\n",
            "Epoch: 1, Training Loss: 1.7694423198699951\n",
            "Epoch: 1, Training Loss: 2.065101146697998\n",
            "Epoch: 1, Training Loss: 2.3546972274780273\n",
            "Epoch: 1, Training Loss: 2.0892019271850586\n",
            "Epoch: 1, Training Loss: 1.9313850402832031\n",
            "Epoch: 1, Training Loss: 2.296069622039795\n",
            "Epoch: 1, Training Loss: 2.13692569732666\n",
            "Epoch: 1, Training Loss: 2.210085868835449\n",
            "Epoch: 1, Training Loss: 2.0272486209869385\n",
            "Epoch: 1, Training Loss: 2.102045774459839\n",
            "Epoch: 1, Training Loss: 1.8806993961334229\n",
            "Epoch: 1, Training Loss: 2.1336023807525635\n",
            "Epoch: 1, Training Loss: 2.0211901664733887\n",
            "Epoch: 1, Training Loss: 2.105159044265747\n",
            "Epoch: 1, Training Loss: 1.7321637868881226\n",
            "Epoch: 1, Training Loss: 2.1135876178741455\n",
            "Epoch: 1, Training Loss: 2.0687978267669678\n",
            "Epoch: 1, Training Loss: 2.128570079803467\n",
            "Epoch: 1, Training Loss: 2.388505697250366\n",
            "Epoch: 1, Training Loss: 2.1264727115631104\n",
            "Epoch: 1, Training Loss: 1.962889313697815\n",
            "Epoch: 1, Training Loss: 1.7910658121109009\n",
            "Epoch: 1, Training Loss: 2.2024803161621094\n",
            "Epoch: 1, Training Loss: 2.0467987060546875\n",
            "Epoch: 1, Training Loss: 1.901978850364685\n",
            "Epoch: 1, Training Loss: 1.8528835773468018\n",
            "Epoch: 1, Training Loss: 1.9784071445465088\n",
            "Epoch: 1, Training Loss: 1.9349395036697388\n",
            "Epoch: 1, Training Loss: 2.1053969860076904\n",
            "Epoch: 1, Training Loss: 1.9887242317199707\n",
            "Epoch: 1, Training Loss: 1.9664793014526367\n",
            "Epoch: 1, Training Loss: 2.157895565032959\n",
            "Epoch: 1, Training Loss: 1.8426140546798706\n",
            "Epoch: 1, Training Loss: 2.0653727054595947\n",
            "Epoch: 1, Training Loss: 2.040921449661255\n",
            "Epoch: 1, Training Loss: 2.0893330574035645\n",
            "Epoch: 1, Training Loss: 2.116149425506592\n",
            "Epoch: 1, Training Loss: 2.343792676925659\n",
            "Epoch: 1, Training Loss: 1.8462615013122559\n",
            "Epoch: 1, Training Loss: 2.2132070064544678\n",
            "Epoch: 1, Training Loss: 2.266786813735962\n",
            "Epoch: 1, Training Loss: 1.9052834510803223\n",
            "Epoch: 1, Training Loss: 2.0764575004577637\n",
            "Epoch: 1, Training Loss: 1.9679542779922485\n",
            "Epoch: 1, Training Loss: 1.745427131652832\n",
            "Epoch: 1, Training Loss: 2.0619256496429443\n",
            "Epoch: 1, Training Loss: 2.5679311752319336\n",
            "Epoch: 1, Training Loss: 2.313002586364746\n",
            "Epoch: 1, Training Loss: 1.9394327402114868\n",
            "Epoch: 1, Training Loss: 2.0614473819732666\n",
            "Epoch: 1, Training Loss: 1.7718524932861328\n",
            "Epoch: 1, Training Loss: 2.0331757068634033\n",
            "Epoch: 1, Training Loss: 2.1521801948547363\n",
            "Epoch: 1, Training Loss: 1.8391978740692139\n",
            "Epoch: 1, Training Loss: 1.991947054862976\n",
            "Epoch: 1, Training Loss: 2.0426642894744873\n",
            "Epoch: 1, Training Loss: 2.177269220352173\n",
            "Epoch: 1, Training Loss: 2.1762356758117676\n",
            "Epoch: 1, Training Loss: 2.0143814086914062\n",
            "Epoch: 1, Training Loss: 2.0303568840026855\n",
            "Epoch: 1, Training Loss: 2.0005078315734863\n",
            "Epoch: 1, Training Loss: 2.0761795043945312\n",
            "Epoch: 1, Training Loss: 2.0275497436523438\n",
            "Epoch: 1, Training Loss: 2.262342691421509\n",
            "Epoch: 1, Training Loss: 2.360081195831299\n",
            "Epoch: 1, Training Loss: 2.1165499687194824\n",
            "Epoch: 1, Training Loss: 1.810049057006836\n",
            "Epoch: 1, Training Loss: 1.664072036743164\n",
            "Epoch: 1, Training Loss: 2.301525115966797\n",
            "Epoch: 1, Training Loss: 2.2522594928741455\n",
            "Epoch: 1, Training Loss: 1.9878846406936646\n",
            "Epoch: 1, Training Loss: 1.900557041168213\n",
            "Epoch: 1, Training Loss: 2.081160306930542\n",
            "Epoch: 1, Training Loss: 1.8162646293640137\n",
            "Epoch: 1, Training Loss: 2.098602056503296\n",
            "Epoch: 1, Training Loss: 1.9334152936935425\n",
            "Epoch: 1, Training Loss: 2.1926989555358887\n",
            "Epoch: 1, Training Loss: 1.8899881839752197\n",
            "Epoch: 1, Training Loss: 1.8617645502090454\n",
            "Epoch: 1, Training Loss: 1.871982216835022\n",
            "Epoch: 1, Training Loss: 2.3749310970306396\n",
            "Epoch: 1, Training Loss: 2.3552298545837402\n",
            "Epoch: 1, Training Loss: 2.009413242340088\n",
            "Epoch: 1, Training Loss: 1.8577126264572144\n",
            "Epoch: 1, Training Loss: 2.1717681884765625\n",
            "Epoch: 1, Training Loss: 2.1037533283233643\n",
            "Epoch: 1, Training Loss: 1.804556965827942\n",
            "Epoch: 1, Training Loss: 1.7682164907455444\n",
            "Epoch: 1, Training Loss: 2.098391056060791\n",
            "Epoch: 1, Training Loss: 1.6901459693908691\n",
            "Epoch: 1, Training Loss: 2.2218358516693115\n",
            "Epoch: 1, Training Loss: 1.9773975610733032\n",
            "Epoch: 1, Training Loss: 1.939330816268921\n",
            "Epoch: 1, Training Loss: 1.992674469947815\n",
            "Epoch: 1, Training Loss: 2.3621838092803955\n",
            "Epoch: 1, Training Loss: 2.298999309539795\n",
            "Epoch: 1, Training Loss: 2.0593552589416504\n",
            "Epoch: 1, Training Loss: 2.243946075439453\n",
            "Epoch: 1, Training Loss: 1.9280742406845093\n",
            "Epoch: 1, Training Loss: 2.0219547748565674\n",
            "Epoch: 1, Training Loss: 2.3389089107513428\n",
            "Epoch: 1, Training Loss: 2.102771043777466\n",
            "Epoch: 1, Training Loss: 2.2223868370056152\n",
            "Epoch: 1, Training Loss: 2.278719425201416\n",
            "Epoch: 1, Training Loss: 2.274353504180908\n",
            "Epoch: 1, Training Loss: 2.0426645278930664\n",
            "Epoch: 1, Training Loss: 2.1847126483917236\n",
            "Epoch: 1, Training Loss: 2.1168036460876465\n",
            "Epoch: 1, Training Loss: 1.8993030786514282\n",
            "Epoch: 1, Training Loss: 2.0923705101013184\n",
            "Epoch: 1, Training Loss: 2.1202473640441895\n",
            "Epoch: 1, Training Loss: 2.054912805557251\n",
            "Epoch: 1, Training Loss: 1.780657410621643\n",
            "Epoch: 1, Training Loss: 1.894534945487976\n",
            "Epoch: 1, Training Loss: 1.746111512184143\n",
            "Epoch: 1, Training Loss: 2.133044958114624\n",
            "Epoch: 1, Training Loss: 2.060994863510132\n",
            "Epoch: 1, Training Loss: 1.9124701023101807\n",
            "Epoch: 1, Training Loss: 2.0816457271575928\n",
            "Epoch: 1, Training Loss: 2.0146942138671875\n",
            "Epoch: 1, Training Loss: 1.6482030153274536\n",
            "Epoch: 1, Training Loss: 2.0886003971099854\n",
            "Epoch: 1, Training Loss: 1.9712705612182617\n",
            "Epoch: 1, Training Loss: 2.019105911254883\n",
            "Epoch: 1, Training Loss: 1.9692885875701904\n",
            "Epoch: 1, Training Loss: 1.9600090980529785\n",
            "Epoch: 1, Training Loss: 1.9534488916397095\n",
            "Epoch: 1, Training Loss: 2.2029900550842285\n",
            "Epoch: 1, Training Loss: 2.1106536388397217\n",
            "Epoch: 1, Training Loss: 2.0108864307403564\n",
            "Epoch: 1, Training Loss: 1.9142777919769287\n",
            "Epoch: 1, Training Loss: 2.2235524654388428\n",
            "Epoch: 1, Training Loss: 1.923486590385437\n",
            "Epoch: 1, Training Loss: 1.8830512762069702\n",
            "Epoch: 1, Training Loss: 2.0210509300231934\n",
            "Epoch: 1, Training Loss: 2.0597739219665527\n",
            "Epoch: 1, Training Loss: 2.051525354385376\n",
            "Epoch: 1, Training Loss: 2.2552926540374756\n",
            "Epoch: 1, Training Loss: 2.122667074203491\n",
            "Epoch: 1, Training Loss: 2.385810375213623\n",
            "Epoch: 1, Training Loss: 2.077117919921875\n",
            "Epoch: 1, Training Loss: 2.0818727016448975\n",
            "Epoch: 1, Training Loss: 1.953230857849121\n",
            "Epoch: 1, Training Loss: 1.865407109260559\n",
            "Epoch: 1, Training Loss: 1.7211296558380127\n",
            "Epoch: 1, Training Loss: 2.1149394512176514\n",
            "Epoch: 1, Training Loss: 1.8744922876358032\n",
            "Epoch: 1, Training Loss: 2.079663038253784\n",
            "Epoch: 1, Training Loss: 1.9949229955673218\n",
            "Epoch: 1, Training Loss: 1.7340093851089478\n",
            "Epoch: 1, Training Loss: 1.8368972539901733\n",
            "Epoch: 1, Training Loss: 2.1850664615631104\n",
            "Epoch: 1, Training Loss: 1.9090412855148315\n",
            "Epoch: 1, Training Loss: 2.043980360031128\n",
            "Epoch: 1, Training Loss: 2.113752841949463\n",
            "Epoch: 1, Training Loss: 1.7647926807403564\n",
            "Epoch: 1, Training Loss: 2.3127048015594482\n",
            "Epoch: 1, Training Loss: 2.323978900909424\n",
            "Epoch: 1, Training Loss: 1.897511601448059\n",
            "Epoch: 1, Training Loss: 2.152263879776001\n",
            "Epoch: 1, Training Loss: 1.955926537513733\n",
            "Epoch: 1, Training Loss: 2.022963047027588\n",
            "Epoch: 1, Training Loss: 2.1412134170532227\n",
            "Epoch: 1, Training Loss: 2.12585186958313\n",
            "Epoch: 1, Training Loss: 1.97709059715271\n",
            "Epoch: 1, Training Loss: 2.0368621349334717\n",
            "Epoch: 1, Training Loss: 1.9079495668411255\n",
            "Epoch: 1, Training Loss: 2.4232537746429443\n",
            "Epoch: 1, Training Loss: 1.8108608722686768\n",
            "Epoch: 1, Training Loss: 1.8002307415008545\n",
            "Epoch: 1, Training Loss: 2.2584705352783203\n",
            "Epoch: 1, Training Loss: 1.8869335651397705\n",
            "Epoch: 1, Training Loss: 1.886967420578003\n",
            "Epoch: 1, Training Loss: 1.9866622686386108\n",
            "Epoch: 1, Training Loss: 1.7050585746765137\n",
            "Epoch: 1, Training Loss: 2.047182559967041\n",
            "Epoch: 1, Training Loss: 1.861861228942871\n",
            "Epoch: 1, Training Loss: 2.0223934650421143\n",
            "Epoch: 1, Training Loss: 2.0527517795562744\n",
            "Epoch: 1, Training Loss: 2.2486555576324463\n",
            "Epoch: 1, Training Loss: 2.044560194015503\n",
            "Epoch: 1, Training Loss: 2.043668031692505\n",
            "Epoch: 1, Training Loss: 1.856345534324646\n",
            "Epoch: 1, Training Loss: 2.1197404861450195\n",
            "Epoch: 1, Training Loss: 2.330049514770508\n",
            "Epoch: 1, Training Loss: 1.9692933559417725\n",
            "Epoch: 1, Training Loss: 1.8532373905181885\n",
            "Epoch: 1, Training Loss: 2.2347874641418457\n",
            "Epoch: 1, Training Loss: 1.8397129774093628\n",
            "Epoch: 1, Training Loss: 1.7088176012039185\n",
            "Epoch: 1, Training Loss: 2.137042284011841\n",
            "Epoch: 1, Training Loss: 2.2369346618652344\n",
            "Epoch: 1, Training Loss: 2.132016897201538\n",
            "Epoch: 1, Training Loss: 2.2022204399108887\n",
            "Epoch: 1, Training Loss: 2.1070353984832764\n",
            "Epoch: 1, Training Loss: 2.1639649868011475\n",
            "Epoch: 1, Training Loss: 2.1043286323547363\n",
            "Epoch: 1, Training Loss: 1.9113917350769043\n",
            "Epoch: 1, Training Loss: 1.9544461965560913\n",
            "Epoch: 1, Training Loss: 2.0818724632263184\n",
            "Epoch: 1, Training Loss: 1.9493181705474854\n",
            "Epoch: 1, Training Loss: 2.125580072402954\n",
            "Epoch: 1, Training Loss: 1.8028936386108398\n",
            "Epoch: 1, Training Loss: 1.8262887001037598\n",
            "Epoch: 1, Training Loss: 2.0939011573791504\n",
            "Epoch: 1, Training Loss: 1.7450603246688843\n",
            "Epoch: 1, Training Loss: 2.220165729522705\n",
            "Epoch: 1, Training Loss: 2.1499640941619873\n",
            "Epoch: 1, Training Loss: 1.7854589223861694\n",
            "Epoch: 1, Training Loss: 2.0036466121673584\n",
            "Epoch: 1, Training Loss: 1.9854024648666382\n",
            "Epoch: 1, Training Loss: 2.139241933822632\n",
            "Epoch: 1, Training Loss: 2.317521572113037\n",
            "Epoch: 1, Training Loss: 1.796905517578125\n",
            "Epoch: 1, Training Loss: 1.849876046180725\n",
            "Epoch: 1, Training Loss: 1.7172821760177612\n",
            "Epoch: 1, Training Loss: 1.914883017539978\n",
            "Epoch: 1, Training Loss: 1.910365104675293\n",
            "Epoch: 1, Training Loss: 2.3685877323150635\n",
            "Epoch: 1, Training Loss: 1.862667441368103\n",
            "Epoch: 1, Training Loss: 1.755194902420044\n",
            "Epoch: 1, Training Loss: 1.9556010961532593\n",
            "Epoch: 1, Training Loss: 1.850077509880066\n",
            "Epoch: 1, Training Loss: 2.002769708633423\n",
            "Epoch: 1, Training Loss: 2.125338554382324\n",
            "Epoch: 1, Training Loss: 1.8956005573272705\n",
            "Epoch: 1, Training Loss: 1.9889487028121948\n",
            "Epoch: 1, Training Loss: 1.8781670331954956\n",
            "Epoch: 1, Training Loss: 1.879140853881836\n",
            "Epoch: 1, Training Loss: 2.1616086959838867\n",
            "Epoch: 1, Training Loss: 2.0143167972564697\n",
            "Epoch: 1, Training Loss: 2.017360210418701\n",
            "Epoch: 1, Training Loss: 2.249699592590332\n",
            "Epoch: 1, Training Loss: 1.9173948764801025\n",
            "Epoch: 1, Training Loss: 1.845824122428894\n",
            "Epoch: 1, Training Loss: 1.9596467018127441\n",
            "Epoch: 1, Training Loss: 1.9143712520599365\n",
            "Epoch: 1, Training Loss: 2.167788028717041\n",
            "Epoch: 1, Training Loss: 1.865307331085205\n",
            "Epoch: 1, Training Loss: 1.9887875318527222\n",
            "Epoch: 1, Training Loss: 2.013498544692993\n",
            "Epoch: 1, Training Loss: 2.0216336250305176\n",
            "Epoch: 1, Training Loss: 2.252068042755127\n",
            "Epoch: 1, Training Loss: 1.971978783607483\n",
            "Epoch: 1, Training Loss: 2.14233660697937\n",
            "Epoch: 1, Training Loss: 2.085594654083252\n",
            "Epoch: 1, Training Loss: 2.0030837059020996\n",
            "Epoch: 1, Training Loss: 2.104485273361206\n",
            "Epoch: 1, Training Loss: 1.9751452207565308\n",
            "Epoch: 1, Training Loss: 1.8057690858840942\n",
            "Epoch: 1, Training Loss: 2.0180511474609375\n",
            "Epoch: 1, Training Loss: 2.069308042526245\n",
            "Epoch: 1, Training Loss: 1.829087257385254\n",
            "Epoch: 1, Training Loss: 2.049870729446411\n",
            "Epoch: 1, Training Loss: 1.7474653720855713\n",
            "Epoch: 1, Training Loss: 2.0637683868408203\n",
            "Epoch: 1, Training Loss: 2.1954166889190674\n",
            "Epoch: 1, Training Loss: 1.8304190635681152\n",
            "Epoch: 1, Training Loss: 1.8370027542114258\n",
            "Epoch: 1, Training Loss: 2.3098413944244385\n",
            "Epoch: 1, Training Loss: 1.8257962465286255\n",
            "Epoch: 1, Training Loss: 1.8144314289093018\n",
            "Epoch: 1, Training Loss: 2.1500675678253174\n",
            "Epoch: 1, Training Loss: 1.9009085893630981\n",
            "Epoch: 1, Training Loss: 1.839516520500183\n",
            "Epoch: 1, Training Loss: 1.8522253036499023\n",
            "Epoch: 1, Training Loss: 1.9513533115386963\n",
            "Epoch: 1, Training Loss: 1.995790719985962\n",
            "Epoch: 1, Training Loss: 1.9638748168945312\n",
            "Epoch: 1, Training Loss: 1.6141437292099\n",
            "Epoch: 1, Training Loss: 2.140070676803589\n",
            "Epoch: 1, Training Loss: 1.9102977514266968\n",
            "Epoch: 1, Training Loss: 2.0461273193359375\n",
            "Epoch: 1, Training Loss: 1.9958354234695435\n",
            "Epoch: 1, Training Loss: 1.9786525964736938\n",
            "Epoch: 1, Training Loss: 2.122123956680298\n",
            "Epoch: 1, Training Loss: 2.0305023193359375\n",
            "Epoch: 1, Training Loss: 2.0165367126464844\n",
            "Epoch: 1, Training Loss: 2.1155154705047607\n",
            "Epoch: 1, Training Loss: 2.019831418991089\n",
            "Epoch: 1, Training Loss: 1.7999111413955688\n",
            "Epoch: 1, Training Loss: 2.0882527828216553\n",
            "Epoch: 1, Training Loss: 1.7704336643218994\n",
            "Epoch: 1, Training Loss: 2.020508289337158\n",
            "Epoch: 1, Training Loss: 1.9146703481674194\n",
            "Epoch: 1, Training Loss: 2.0103979110717773\n",
            "Epoch: 1, Training Loss: 2.063776731491089\n",
            "Epoch: 1, Training Loss: 1.9908101558685303\n",
            "Epoch: 1, Training Loss: 2.2839558124542236\n",
            "Epoch: 1, Training Loss: 1.9237284660339355\n",
            "Epoch: 1, Training Loss: 2.071399688720703\n",
            "Epoch: 1, Training Loss: 2.2560548782348633\n",
            "Epoch: 1, Training Loss: 2.029876470565796\n",
            "Epoch: 1, Training Loss: 1.9773738384246826\n",
            "Epoch: 1, Training Loss: 2.2710626125335693\n",
            "Epoch: 1, Training Loss: 1.8578730821609497\n",
            "Epoch: 1, Training Loss: 1.7925159931182861\n",
            "Epoch: 1, Training Loss: 1.7902796268463135\n",
            "Epoch: 1, Training Loss: 1.8025434017181396\n",
            "Epoch: 1, Training Loss: 2.093811273574829\n",
            "Epoch: 1, Training Loss: 2.0677831172943115\n",
            "Epoch: 1, Training Loss: 2.0127453804016113\n",
            "Epoch: 1, Training Loss: 1.747692584991455\n",
            "Epoch: 1, Training Loss: 2.0871615409851074\n",
            "Epoch: 1, Training Loss: 1.700197696685791\n",
            "Epoch: 1, Training Loss: 1.9087690114974976\n",
            "Epoch: 1, Training Loss: 1.9851069450378418\n",
            "Epoch: 1, Training Loss: 1.9331625699996948\n",
            "Epoch: 1, Training Loss: 1.7355021238327026\n",
            "Epoch: 1, Training Loss: 2.1313726902008057\n",
            "Epoch: 1, Training Loss: 1.9831538200378418\n",
            "Epoch: 1, Training Loss: 1.8024256229400635\n",
            "Epoch: 1, Training Loss: 2.070443630218506\n",
            "Epoch: 1, Training Loss: 1.781840443611145\n",
            "Epoch: 1, Training Loss: 1.8638290166854858\n",
            "Epoch: 1, Training Loss: 1.6900748014450073\n",
            "Epoch: 1, Training Loss: 2.0493240356445312\n",
            "Epoch: 1, Training Loss: 2.334237813949585\n",
            "Epoch: 1, Training Loss: 1.9042911529541016\n",
            "Epoch: 1, Training Loss: 1.9317468404769897\n",
            "Epoch: 1, Training Loss: 2.4317245483398438\n",
            "Epoch: 1, Training Loss: 2.175153970718384\n",
            "Epoch: 1, Training Loss: 2.252709150314331\n",
            "Epoch: 1, Training Loss: 1.9867616891860962\n",
            "Epoch: 1, Training Loss: 1.8124407529830933\n",
            "Epoch: 1, Training Loss: 2.144641876220703\n",
            "Epoch: 1, Training Loss: 2.0843660831451416\n",
            "Epoch: 1, Training Loss: 1.816545009613037\n",
            "Epoch: 1, Training Loss: 2.2571024894714355\n",
            "Epoch: 1, Training Loss: 1.813593864440918\n",
            "Epoch: 1, Training Loss: 1.9618157148361206\n",
            "Epoch: 1, Training Loss: 1.9525079727172852\n",
            "Epoch: 1, Training Loss: 2.2161688804626465\n",
            "Epoch: 1, Training Loss: 1.996839165687561\n",
            "Epoch: 1, Training Loss: 2.2805936336517334\n",
            "Epoch: 1, Training Loss: 2.0905027389526367\n",
            "Epoch: 1, Training Loss: 2.069753885269165\n",
            "Epoch: 1, Training Loss: 2.0943830013275146\n",
            "Epoch: 1, Training Loss: 1.7809628248214722\n",
            "Epoch: 1, Training Loss: 2.079786777496338\n",
            "Epoch: 1, Training Loss: 2.2322604656219482\n",
            "Epoch: 1, Training Loss: 1.9576592445373535\n",
            "Epoch: 1, Training Loss: 1.9496415853500366\n",
            "Epoch: 1, Training Loss: 1.917558193206787\n",
            "Epoch: 1, Training Loss: 1.8207303285598755\n",
            "Epoch: 1, Training Loss: 1.9620904922485352\n",
            "Epoch: 1, Training Loss: 1.921773910522461\n",
            "Epoch: 1, Training Loss: 1.8773581981658936\n",
            "Epoch: 1, Training Loss: 1.8355554342269897\n",
            "Epoch: 1, Training Loss: 1.9910510778427124\n",
            "Epoch: 1, Training Loss: 1.7842636108398438\n",
            "Epoch: 1, Training Loss: 1.9789386987686157\n",
            "Epoch: 1, Training Loss: 1.9704809188842773\n",
            "Epoch: 1, Training Loss: 1.8542568683624268\n",
            "Epoch: 1, Training Loss: 1.7052961587905884\n",
            "Epoch: 1, Training Loss: 2.1544718742370605\n",
            "Epoch: 1, Training Loss: 2.0740368366241455\n",
            "Epoch: 1, Training Loss: 1.7878634929656982\n",
            "Epoch: 1, Training Loss: 2.2533411979675293\n",
            "Epoch: 1, Training Loss: 2.03285551071167\n",
            "Epoch: 1, Training Loss: 2.0532073974609375\n",
            "Epoch: 1, Training Loss: 2.0571794509887695\n",
            "Epoch: 1, Training Loss: 2.0035929679870605\n",
            "Epoch: 1, Training Loss: 2.2526683807373047\n",
            "Epoch: 1, Training Loss: 2.351353406906128\n",
            "Epoch: 1, Training Loss: 2.039846897125244\n",
            "Epoch: 1, Training Loss: 2.0418143272399902\n",
            "Epoch: 1, Training Loss: 1.9016653299331665\n",
            "Epoch: 1, Training Loss: 2.2096610069274902\n",
            "Epoch: 1, Training Loss: 1.7312887907028198\n",
            "Epoch: 1, Training Loss: 1.842907428741455\n",
            "Epoch: 1, Training Loss: 2.351780891418457\n",
            "Epoch: 1, Training Loss: 1.9608129262924194\n",
            "Epoch: 1, Training Loss: 1.9479142427444458\n",
            "Epoch: 1, Training Loss: 2.0170509815216064\n",
            "Epoch: 1, Training Loss: 2.0098581314086914\n",
            "Epoch: 1, Training Loss: 1.8562599420547485\n",
            "Epoch: 1, Training Loss: 1.7683937549591064\n",
            "Epoch: 1, Training Loss: 1.767381191253662\n",
            "Epoch: 1, Training Loss: 2.019395351409912\n",
            "Epoch: 1, Training Loss: 1.9302550554275513\n",
            "Epoch: 1, Training Loss: 1.8499276638031006\n",
            "Epoch: 1, Training Loss: 1.6987262964248657\n",
            "Epoch: 1, Training Loss: 1.9047695398330688\n",
            "Epoch: 1, Training Loss: 1.9592450857162476\n",
            "Epoch: 1, Training Loss: 2.2012696266174316\n",
            "Epoch: 1, Training Loss: 2.0008890628814697\n",
            "Epoch: 1, Training Loss: 1.9519144296646118\n",
            "Epoch: 1, Training Loss: 1.9259943962097168\n",
            "Epoch: 1, Training Loss: 2.045142889022827\n",
            "Epoch: 1, Training Loss: 2.042840003967285\n",
            "Epoch: 1, Training Loss: 1.9907642602920532\n",
            "Epoch: 1, Training Loss: 1.9363247156143188\n",
            "Epoch: 1, Training Loss: 1.8753786087036133\n",
            "Epoch: 1, Training Loss: 1.7404100894927979\n",
            "Epoch: 1, Training Loss: 2.0060181617736816\n",
            "Epoch: 1, Training Loss: 1.8368656635284424\n",
            "Epoch: 1, Training Loss: 2.0468246936798096\n",
            "Epoch: 1, Training Loss: 2.004261016845703\n",
            "Epoch: 1, Training Loss: 1.9690914154052734\n",
            "Epoch: 1, Training Loss: 1.8892929553985596\n",
            "Epoch: 1, Training Loss: 2.067066192626953\n",
            "Epoch: 1, Training Loss: 1.7386014461517334\n",
            "Epoch: 1, Training Loss: 2.1813344955444336\n",
            "Epoch: 1, Training Loss: 2.182737350463867\n",
            "Epoch: 1, Training Loss: 1.8347514867782593\n",
            "Epoch: 1, Training Loss: 1.8587342500686646\n",
            "Epoch: 1, Training Loss: 1.7619303464889526\n",
            "Epoch: 1, Training Loss: 1.9023089408874512\n",
            "Epoch: 1, Training Loss: 2.19529128074646\n",
            "Epoch: 1, Training Loss: 2.012932538986206\n",
            "Epoch: 1, Training Loss: 1.8906149864196777\n",
            "Epoch: 1, Training Loss: 1.7865513563156128\n",
            "Epoch: 1, Training Loss: 1.745862603187561\n",
            "Epoch: 1, Training Loss: 1.8125505447387695\n",
            "Epoch: 1, Training Loss: 1.7366434335708618\n",
            "Epoch: 1, Training Loss: 1.9382075071334839\n",
            "Epoch: 1, Training Loss: 1.6553313732147217\n",
            "Epoch: 1, Training Loss: 1.956548810005188\n",
            "Epoch: 1, Training Loss: 2.1906158924102783\n",
            "Epoch: 1, Training Loss: 2.0837159156799316\n",
            "Epoch: 1, Training Loss: 1.7400041818618774\n",
            "Epoch: 1, Training Loss: 1.7557404041290283\n",
            "Epoch: 1, Training Loss: 1.756374478340149\n",
            "Epoch: 1, Training Loss: 2.0006325244903564\n",
            "Epoch: 1, Training Loss: 1.9620897769927979\n",
            "Epoch: 1, Training Loss: 2.072094202041626\n",
            "Epoch: 1, Training Loss: 2.150972366333008\n",
            "Epoch: 1, Training Loss: 1.7814409732818604\n",
            "Epoch: 1, Training Loss: 2.1170809268951416\n",
            "Epoch: 1, Training Loss: 1.8990061283111572\n",
            "Epoch: 1, Training Loss: 2.0315704345703125\n",
            "Epoch: 1, Training Loss: 2.3079206943511963\n",
            "Epoch: 1, Training Loss: 2.0189197063446045\n",
            "Epoch: 1, Training Loss: 2.149482488632202\n",
            "Epoch: 1, Training Loss: 1.8673374652862549\n",
            "Epoch: 1, Training Loss: 2.053238868713379\n",
            "Epoch: 1, Training Loss: 1.9481174945831299\n",
            "Epoch: 1, Training Loss: 2.0965700149536133\n",
            "Epoch: 1, Training Loss: 2.064741611480713\n",
            "Epoch: 1, Training Loss: 1.9336928129196167\n",
            "Epoch: 1, Training Loss: 1.9414044618606567\n",
            "Epoch: 1, Training Loss: 1.9630805253982544\n",
            "Epoch: 1, Training Loss: 1.7395168542861938\n",
            "Epoch: 1, Training Loss: 1.9960739612579346\n",
            "Epoch: 1, Training Loss: 1.9031590223312378\n",
            "Epoch: 1, Training Loss: 1.8969731330871582\n",
            "Epoch: 1, Training Loss: 1.9586694240570068\n",
            "Epoch: 1, Training Loss: 2.1030938625335693\n",
            "Epoch: 1, Training Loss: 1.9716876745224\n",
            "Epoch: 1, Training Loss: 2.195326566696167\n",
            "Epoch: 1, Training Loss: 1.9212833642959595\n",
            "Epoch: 1, Training Loss: 2.1874892711639404\n",
            "Epoch: 1, Training Loss: 1.8743571043014526\n",
            "Epoch: 1, Training Loss: 1.6904503107070923\n",
            "Epoch: 1, Training Loss: 2.139216899871826\n",
            "Epoch: 1, Training Loss: 1.917742133140564\n",
            "Epoch: 1, Training Loss: 1.8551616668701172\n",
            "Epoch: 1, Training Loss: 1.8389698266983032\n",
            "Epoch: 1, Training Loss: 2.021986484527588\n",
            "Epoch: 1, Training Loss: 1.8844826221466064\n",
            "Epoch: 1, Training Loss: 2.2435200214385986\n",
            "Epoch: 1, Training Loss: 2.10349440574646\n",
            "Epoch: 1, Training Loss: 1.6758999824523926\n",
            "Epoch: 1, Training Loss: 2.1267526149749756\n",
            "Epoch: 1, Training Loss: 2.0754799842834473\n",
            "Epoch: 1, Training Loss: 2.148375988006592\n",
            "Epoch: 1, Training Loss: 1.6754190921783447\n",
            "Epoch: 1, Training Loss: 2.1718506813049316\n",
            "Epoch: 1, Training Loss: 2.1632697582244873\n",
            "Epoch: 1, Training Loss: 2.0238616466522217\n",
            "Epoch: 1, Training Loss: 2.05351185798645\n",
            "Epoch: 1, Training Loss: 2.075904369354248\n",
            "Epoch: 1, Training Loss: 1.8761683702468872\n",
            "Epoch: 1, Training Loss: 1.547552227973938\n",
            "Epoch: 1, Training Loss: 2.2588634490966797\n",
            "Epoch: 1, Training Loss: 2.3266963958740234\n",
            "Epoch: 1, Training Loss: 2.2254953384399414\n",
            "Epoch: 1, Training Loss: 1.964533805847168\n",
            "Epoch: 1, Training Loss: 2.049938201904297\n",
            "Epoch: 1, Training Loss: 1.9030801057815552\n",
            "Epoch: 1, Training Loss: 1.6957355737686157\n",
            "Epoch: 1, Training Loss: 1.8835737705230713\n",
            "Epoch: 1, Training Loss: 2.0048484802246094\n",
            "Epoch: 1, Training Loss: 1.8833919763565063\n",
            "Epoch: 1, Training Loss: 2.0162506103515625\n",
            "Epoch: 1, Training Loss: 1.772077202796936\n",
            "Epoch: 1, Training Loss: 1.806959867477417\n",
            "Epoch: 1, Training Loss: 1.6388518810272217\n",
            "Epoch: 1, Training Loss: 2.0835211277008057\n",
            "Epoch: 1, Training Loss: 1.7906700372695923\n",
            "Epoch: 1, Training Loss: 2.272024631500244\n",
            "Epoch: 1, Training Loss: 1.8130009174346924\n",
            "Epoch: 1, Training Loss: 1.711083173751831\n",
            "Epoch: 1, Training Loss: 2.111362934112549\n",
            "Epoch: 1, Training Loss: 2.0614516735076904\n",
            "Epoch: 1, Training Loss: 2.0321531295776367\n",
            "Epoch: 1, Training Loss: 1.6716327667236328\n",
            "Epoch: 1, Training Loss: 1.8423207998275757\n",
            "Epoch: 1, Training Loss: 1.411423921585083\n",
            "Epoch: 1, Training Loss: 2.1819634437561035\n",
            "Epoch: 1, Training Loss: 2.041783094406128\n",
            "Epoch: 1, Training Loss: 2.1120150089263916\n",
            "Epoch: 1, Training Loss: 1.8164808750152588\n",
            "Epoch: 1, Training Loss: 1.8993170261383057\n",
            "Epoch: 1, Training Loss: 1.9700677394866943\n",
            "Epoch: 1, Training Loss: 1.7909525632858276\n",
            "Epoch: 1, Training Loss: 1.8516356945037842\n",
            "Epoch: 1, Training Loss: 1.9953747987747192\n",
            "Epoch: 1, Training Loss: 1.7909786701202393\n",
            "Epoch: 1, Training Loss: 2.133436918258667\n",
            "Epoch: 1, Training Loss: 1.8820676803588867\n",
            "Epoch: 1, Training Loss: 1.846313238143921\n",
            "Epoch: 1, Training Loss: 1.6781986951828003\n",
            "Epoch: 1, Training Loss: 2.068161964416504\n",
            "Epoch: 1, Training Loss: 1.9746733903884888\n",
            "Epoch: 1, Training Loss: 2.0895261764526367\n",
            "Epoch: 1, Training Loss: 1.7965035438537598\n",
            "Epoch: 1, Training Loss: 1.9836604595184326\n",
            "Epoch: 1, Training Loss: 1.6837997436523438\n",
            "Epoch: 1, Training Loss: 1.9774388074874878\n",
            "Epoch: 1, Training Loss: 2.0892646312713623\n",
            "Epoch: 1, Training Loss: 2.2868802547454834\n",
            "Epoch: 1, Training Loss: 1.8127390146255493\n",
            "Epoch: 1, Training Loss: 1.8705586194992065\n",
            "Epoch: 1, Training Loss: 1.9866926670074463\n",
            "Epoch: 1, Training Loss: 1.804403305053711\n",
            "Epoch: 1, Training Loss: 1.8837488889694214\n",
            "Epoch: 1, Training Loss: 2.098236083984375\n",
            "Epoch: 1, Training Loss: 1.7648379802703857\n",
            "Epoch: 1, Training Loss: 1.9924461841583252\n",
            "Epoch: 1, Training Loss: 1.7798656225204468\n",
            "Epoch: 1, Training Loss: 1.7113789319992065\n",
            "Epoch: 1, Training Loss: 1.7554291486740112\n",
            "Epoch: 1, Training Loss: 2.0467231273651123\n",
            "Epoch: 1, Training Loss: 2.0067782402038574\n",
            "Epoch: 1, Training Loss: 1.8297836780548096\n",
            "Epoch: 1, Training Loss: 2.0597028732299805\n",
            "Epoch: 1, Training Loss: 1.9608639478683472\n",
            "Epoch: 1, Training Loss: 1.9842674732208252\n",
            "Epoch: 1, Training Loss: 1.807320475578308\n",
            "Epoch: 1, Training Loss: 2.0812606811523438\n",
            "Epoch: 1, Training Loss: 1.919439435005188\n",
            "Epoch: 1, Training Loss: 2.0024168491363525\n",
            "Epoch: 1, Training Loss: 2.0917248725891113\n",
            "Epoch: 1, Training Loss: 1.9275681972503662\n",
            "Epoch: 1, Training Loss: 2.003164768218994\n",
            "Epoch: 1, Training Loss: 1.9400502443313599\n",
            "Epoch: 1, Training Loss: 1.917083501815796\n",
            "Epoch: 1, Training Loss: 2.1373586654663086\n",
            "Epoch: 1, Training Loss: 1.975714087486267\n",
            "Epoch: 1, Training Loss: 2.040876865386963\n",
            "Epoch: 1, Training Loss: 2.1365859508514404\n",
            "Epoch: 1, Training Loss: 1.789135217666626\n",
            "Epoch: 1, Training Loss: 1.9310641288757324\n",
            "Epoch: 1, Training Loss: 1.6550830602645874\n",
            "Epoch: 1, Training Loss: 2.0251305103302\n",
            "Epoch: 1, Training Loss: 2.192437171936035\n",
            "Epoch: 1, Training Loss: 1.8783951997756958\n",
            "Epoch: 1, Training Loss: 1.9450873136520386\n",
            "Epoch: 1, Training Loss: 2.2053756713867188\n",
            "Epoch: 1, Training Loss: 1.9620206356048584\n",
            "Epoch: 1, Training Loss: 2.0915958881378174\n",
            "Epoch: 1, Training Loss: 1.6987764835357666\n",
            "Epoch: 1, Training Loss: 1.8661019802093506\n",
            "Epoch: 1, Training Loss: 1.7147636413574219\n",
            "Epoch: 1, Training Loss: 1.8299206495285034\n",
            "Epoch: 1, Training Loss: 1.884918212890625\n",
            "Epoch: 1, Training Loss: 2.0204315185546875\n",
            "Epoch: 1, Training Loss: 1.6330748796463013\n",
            "Epoch: 1, Training Loss: 1.7511436939239502\n",
            "Epoch: 1, Training Loss: 1.9550378322601318\n",
            "Epoch: 1, Training Loss: 1.9562960863113403\n",
            "Epoch: 1, Training Loss: 2.007370710372925\n",
            "Epoch: 1, Training Loss: 1.8994317054748535\n",
            "Epoch: 1, Training Loss: 1.8671518564224243\n",
            "Epoch: 1, Training Loss: 1.8783732652664185\n",
            "Epoch: 1, Training Loss: 1.8978853225708008\n",
            "Epoch: 1, Training Loss: 1.98681640625\n",
            "Epoch: 1, Training Loss: 1.7442216873168945\n",
            "Epoch: 1, Training Loss: 2.0460751056671143\n",
            "Epoch: 1, Training Loss: 1.7280046939849854\n",
            "Epoch: 1, Training Loss: 1.8853024244308472\n",
            "Epoch: 1, Training Loss: 1.9679691791534424\n",
            "Epoch: 1, Training Loss: 1.938196063041687\n",
            "Epoch: 1, Training Loss: 1.7103527784347534\n",
            "Epoch: 1, Training Loss: 2.027503728866577\n",
            "Epoch: 1, Training Loss: 2.02542781829834\n",
            "Epoch: 1, Training Loss: 2.296168088912964\n",
            "Epoch: 1, Training Loss: 2.0338428020477295\n",
            "Epoch: 1, Training Loss: 2.0131287574768066\n",
            "Epoch: 1, Training Loss: 2.0274765491485596\n",
            "Epoch: 1, Training Loss: 1.8247157335281372\n",
            "Epoch: 1, Training Loss: 1.738565444946289\n",
            "Epoch: 1, Training Loss: 2.2571957111358643\n",
            "Epoch: 1, Training Loss: 1.9958300590515137\n",
            "Epoch: 1, Training Loss: 1.9945859909057617\n",
            "Epoch: 1, Training Loss: 2.2615041732788086\n",
            "Epoch: 1, Training Loss: 2.015944004058838\n",
            "Epoch: 1, Training Loss: 1.8778194189071655\n",
            "Epoch: 1, Training Loss: 2.1009535789489746\n",
            "Epoch: 1, Training Loss: 2.1583662033081055\n",
            "Epoch: 1, Training Loss: 1.8393398523330688\n",
            "Epoch: 1, Training Loss: 1.8166224956512451\n",
            "Epoch: 1, Training Loss: 1.8904279470443726\n",
            "Epoch: 1, Training Loss: 1.924608588218689\n",
            "Epoch: 1, Training Loss: 1.9476550817489624\n",
            "Epoch: 1, Training Loss: 1.9225212335586548\n",
            "Epoch: 1, Training Loss: 1.987095832824707\n",
            "Epoch: 1, Training Loss: 1.6334155797958374\n",
            "Epoch: 1, Training Loss: 2.091447591781616\n",
            "Epoch: 1, Training Loss: 2.0388588905334473\n",
            "Epoch: 1, Training Loss: 1.7805641889572144\n",
            "Epoch: 1, Training Loss: 1.9957069158554077\n",
            "Epoch: 1, Training Loss: 1.788014531135559\n",
            "Epoch: 1, Training Loss: 2.360806703567505\n",
            "Epoch: 1, Training Loss: 2.1370468139648438\n",
            "Epoch: 1, Training Loss: 1.8549045324325562\n",
            "Epoch: 1, Training Loss: 1.828019380569458\n",
            "Epoch: 1, Training Loss: 2.2293941974639893\n",
            "Epoch: 1, Training Loss: 1.964970350265503\n",
            "Epoch: 1, Training Loss: 2.161276340484619\n",
            "Epoch: 1, Training Loss: 1.9263694286346436\n",
            "Epoch: 1, Training Loss: 1.7625998258590698\n",
            "Epoch: 1, Training Loss: 1.8536604642868042\n",
            "Epoch: 1, Training Loss: 2.0067801475524902\n",
            "Epoch: 1, Training Loss: 1.9279003143310547\n",
            "Epoch: 1, Training Loss: 1.8856464624404907\n",
            "Epoch: 1, Training Loss: 2.1832118034362793\n",
            "Epoch: 1, Training Loss: 2.178185224533081\n",
            "Epoch: 1, Training Loss: 1.8230688571929932\n",
            "Epoch: 1, Training Loss: 2.114452362060547\n",
            "Epoch: 1, Training Loss: 1.506887674331665\n",
            "Epoch: 1, Training Loss: 2.098727226257324\n",
            "Epoch: 1, Training Loss: 1.7656491994857788\n",
            "Epoch: 1, Training Loss: 1.8671175241470337\n",
            "Epoch: 1, Training Loss: 2.0318143367767334\n",
            "Epoch: 1, Training Loss: 1.986220121383667\n",
            "Epoch: 1, Training Loss: 1.9819797277450562\n",
            "Epoch: 1, Training Loss: 1.8721585273742676\n",
            "Epoch: 1, Training Loss: 1.8444433212280273\n",
            "Epoch: 1, Training Loss: 1.8821020126342773\n",
            "Epoch: 1, Training Loss: 1.5735446214675903\n",
            "Epoch: 1, Training Loss: 1.8543870449066162\n",
            "Epoch: 1, Training Loss: 1.8058847188949585\n",
            "Epoch: 1, Training Loss: 2.148841142654419\n",
            "Epoch: 1, Training Loss: 1.8836548328399658\n",
            "Epoch: 1, Training Loss: 1.7459546327590942\n",
            "Epoch: 1, Training Loss: 1.791136622428894\n",
            "Epoch: 1, Training Loss: 1.9012197256088257\n",
            "Epoch: 1, Training Loss: 1.8758553266525269\n",
            "Epoch: 1, Training Loss: 1.9029343128204346\n",
            "Epoch: 1, Training Loss: 1.9077308177947998\n",
            "Epoch: 1, Training Loss: 1.9949063062667847\n",
            "Epoch: 1, Training Loss: 2.011383295059204\n",
            "Epoch: 1, Training Loss: 1.7843066453933716\n",
            "Epoch: 1, Training Loss: 1.8472391366958618\n",
            "Epoch: 1, Training Loss: 1.8989648818969727\n",
            "Epoch: 1, Training Loss: 2.0908937454223633\n",
            "Epoch: 1, Training Loss: 1.9720878601074219\n",
            "Epoch: 1, Training Loss: 1.8813172578811646\n",
            "Epoch: 1, Training Loss: 1.824863076210022\n",
            "Epoch: 1, Training Loss: 1.9268369674682617\n",
            "Epoch: 1, Training Loss: 2.074176073074341\n",
            "Epoch: 1, Training Loss: 2.0391125679016113\n",
            "Epoch: 1, Training Loss: 1.9180266857147217\n",
            "Epoch: 1, Training Loss: 2.0229227542877197\n",
            "Epoch: 1, Training Loss: 1.7382678985595703\n",
            "Epoch: 1, Training Loss: 2.128654718399048\n",
            "Epoch: 1, Training Loss: 2.0814263820648193\n",
            "Epoch: 1, Training Loss: 1.9118438959121704\n",
            "Epoch: 1, Training Loss: 1.9662238359451294\n",
            "Epoch: 1, Training Loss: 1.9321925640106201\n",
            "Epoch: 1, Training Loss: 1.5918070077896118\n",
            "Epoch: 1, Training Loss: 1.943472146987915\n",
            "Epoch: 1, Training Loss: 2.0154404640197754\n",
            "Epoch: 1, Training Loss: 2.117668867111206\n",
            "Epoch: 1, Training Loss: 2.268134355545044\n",
            "Epoch: 1, Training Loss: 1.7557927370071411\n",
            "Epoch: 1, Training Loss: 2.064846992492676\n",
            "Epoch: 1, Training Loss: 1.8862875699996948\n",
            "Epoch: 1, Training Loss: 1.9606595039367676\n",
            "Epoch: 1, Training Loss: 1.9755877256393433\n",
            "Epoch: 1, Training Loss: 1.6622166633605957\n",
            "Epoch: 1, Training Loss: 1.9846099615097046\n",
            "Epoch: 1, Training Loss: 1.7114088535308838\n",
            "Epoch: 1, Training Loss: 1.8994580507278442\n",
            "Epoch: 1, Training Loss: 1.8506555557250977\n",
            "Epoch: 1, Training Loss: 2.1822431087493896\n",
            "Epoch: 1, Training Loss: 2.0158638954162598\n",
            "Epoch: 1, Training Loss: 2.1779685020446777\n",
            "Epoch: 1, Training Loss: 2.1497952938079834\n",
            "Epoch: 1, Training Loss: 1.860541582107544\n",
            "Epoch: 1, Training Loss: 1.9244887828826904\n",
            "Epoch: 1, Training Loss: 1.9029698371887207\n",
            "Epoch: 1, Training Loss: 1.8571268320083618\n",
            "Epoch: 1, Training Loss: 1.7792212963104248\n",
            "Epoch: 1, Training Loss: 2.0198116302490234\n",
            "Epoch: 1, Training Loss: 2.271469831466675\n",
            "Epoch: 1, Training Loss: 2.042724847793579\n",
            "Epoch: 1, Training Loss: 1.80729079246521\n",
            "Epoch: 1, Training Loss: 1.813779592514038\n",
            "Epoch: 1, Training Loss: 1.9031164646148682\n",
            "Epoch: 1, Training Loss: 1.7051458358764648\n",
            "Epoch: 1, Training Loss: 2.168914556503296\n",
            "Epoch: 1, Training Loss: 2.0357885360717773\n",
            "Epoch: 1, Training Loss: 1.696407437324524\n",
            "Epoch: 1, Training Loss: 1.8892749547958374\n",
            "Epoch: 1, Training Loss: 1.7756564617156982\n",
            "Epoch: 1, Training Loss: 1.812986969947815\n",
            "Epoch: 1, Training Loss: 1.8582099676132202\n",
            "Epoch: 1, Training Loss: 2.1446938514709473\n",
            "Epoch: 1, Training Loss: 1.758871078491211\n",
            "Epoch: 1, Training Loss: 1.6245689392089844\n",
            "Epoch: 1, Training Loss: 2.1110763549804688\n",
            "Epoch: 1, Training Loss: 2.103816032409668\n",
            "Epoch: 1, Training Loss: 1.8388534784317017\n",
            "Epoch: 1, Training Loss: 1.7806142568588257\n",
            "Epoch: 1, Training Loss: 1.5313640832901\n",
            "Epoch: 1, Training Loss: 2.05324125289917\n",
            "Epoch: 1, Training Loss: 2.1172995567321777\n",
            "Epoch: 1, Training Loss: 2.012800455093384\n",
            "Epoch: 1, Training Loss: 2.05926251411438\n",
            "Epoch: 1, Training Loss: 2.1228504180908203\n",
            "Epoch: 1, Training Loss: 1.8205710649490356\n",
            "Epoch: 1, Training Loss: 1.5559676885604858\n",
            "Epoch: 1, Training Loss: 1.9623790979385376\n",
            "Epoch: 1, Training Loss: 1.9527055025100708\n",
            "Epoch: 1, Training Loss: 1.742289662361145\n",
            "Epoch: 1, Training Loss: 1.920371174812317\n",
            "Epoch: 1, Training Loss: 1.8799968957901\n",
            "Epoch: 1, Training Loss: 1.7885056734085083\n",
            "Epoch: 1, Training Loss: 1.619435429573059\n",
            "Epoch: 1, Training Loss: 1.985527515411377\n",
            "Epoch: 1, Training Loss: 1.9921091794967651\n",
            "Epoch: 1, Training Loss: 1.7549749612808228\n",
            "Epoch: 1, Training Loss: 1.8032121658325195\n",
            "Epoch: 1, Training Loss: 1.729007363319397\n",
            "Epoch: 1, Training Loss: 2.270097255706787\n",
            "Epoch: 1, Training Loss: 1.8818310499191284\n",
            "Epoch: 1, Training Loss: 1.9775516986846924\n",
            "Epoch: 1, Training Loss: 1.7688713073730469\n",
            "Epoch: 1, Training Loss: 2.0665528774261475\n",
            "Epoch: 1, Training Loss: 1.901977300643921\n",
            "Epoch: 1, Training Loss: 2.2023279666900635\n",
            "Epoch: 1, Training Loss: 1.7642133235931396\n",
            "Epoch: 1, Training Loss: 1.999703288078308\n",
            "Epoch: 1, Training Loss: 1.8663524389266968\n",
            "Epoch: 1, Training Loss: 2.0565807819366455\n",
            "Epoch: 1, Training Loss: 2.157602310180664\n",
            "Epoch: 1, Training Loss: 2.1084089279174805\n",
            "Epoch: 1, Training Loss: 2.0201337337493896\n",
            "Epoch: 1, Training Loss: 1.8921440839767456\n",
            "Epoch: 1, Training Loss: 1.9961031675338745\n",
            "Epoch: 1, Training Loss: 1.7722227573394775\n",
            "Epoch: 1, Training Loss: 2.0455868244171143\n",
            "Epoch: 1, Training Loss: 1.7199825048446655\n",
            "Epoch: 1, Training Loss: 2.0342986583709717\n",
            "Epoch: 1, Training Loss: 2.068944215774536\n",
            "Epoch: 1, Training Loss: 2.0284557342529297\n",
            "Epoch: 1, Training Loss: 1.738953709602356\n",
            "Epoch: 1, Training Loss: 1.9918967485427856\n",
            "Epoch: 1, Training Loss: 1.9531261920928955\n",
            "Epoch: 1, Training Loss: 1.907457947731018\n",
            "Epoch: 1, Training Loss: 1.8933672904968262\n",
            "Epoch: 1, Validation Loss: 1.618631362915039\n",
            "Epoch: 1, Validation Loss: 1.4761109352111816\n",
            "Epoch: 1, Validation Loss: 1.7691364288330078\n",
            "Epoch: 1, Validation Loss: 2.1025521755218506\n",
            "Epoch: 1, Validation Loss: 1.9286863803863525\n",
            "Epoch: 1, Validation Loss: 2.0373547077178955\n",
            "Epoch: 1, Validation Loss: 1.742379903793335\n",
            "Epoch: 1, Validation Loss: 1.9135489463806152\n",
            "Epoch: 1, Validation Loss: 1.8333759307861328\n",
            "Epoch: 1, Validation Loss: 1.7521452903747559\n",
            "Epoch: 1, Validation Loss: 2.040350914001465\n",
            "Epoch: 1, Validation Loss: 1.925663709640503\n",
            "Epoch: 1, Validation Loss: 1.8194199800491333\n",
            "Epoch: 1, Validation Loss: 1.73065185546875\n",
            "Epoch: 1, Validation Loss: 1.6273030042648315\n",
            "Epoch: 1, Validation Loss: 1.6248661279678345\n",
            "Epoch: 1, Validation Loss: 1.878865361213684\n",
            "Epoch: 1, Validation Loss: 1.9451026916503906\n",
            "Epoch: 1, Validation Loss: 1.7386821508407593\n",
            "Epoch: 1, Validation Loss: 1.7392463684082031\n",
            "Epoch: 1, Validation Loss: 1.6149781942367554\n",
            "Epoch: 1, Validation Loss: 1.8763631582260132\n",
            "Epoch: 1, Validation Loss: 2.252793788909912\n",
            "Epoch: 1, Validation Loss: 1.8104901313781738\n",
            "Epoch: 1, Validation Loss: 1.8357709646224976\n",
            "Epoch: 1, Validation Loss: 1.8322640657424927\n",
            "Epoch: 1, Validation Loss: 1.683977723121643\n",
            "Epoch: 1, Validation Loss: 1.8347818851470947\n",
            "Epoch: 1, Validation Loss: 1.8273541927337646\n",
            "Epoch: 1, Validation Loss: 1.548420786857605\n",
            "Epoch: 1, Validation Loss: 1.8771541118621826\n",
            "Epoch: 1, Validation Loss: 2.0359020233154297\n",
            "Epoch: 1, Validation Loss: 1.9614123106002808\n",
            "Epoch: 1, Validation Loss: 1.8743035793304443\n",
            "Epoch: 1, Validation Loss: 1.6980338096618652\n",
            "Epoch: 1, Validation Loss: 1.8854742050170898\n",
            "Epoch: 1, Validation Loss: 1.7081266641616821\n",
            "Epoch: 1, Validation Loss: 2.0732333660125732\n",
            "Epoch: 1, Validation Loss: 1.5017168521881104\n",
            "Epoch: 1, Validation Loss: 1.8152730464935303\n",
            "Epoch: 1, Validation Loss: 1.8673913478851318\n",
            "Epoch: 1, Validation Loss: 1.5517163276672363\n",
            "Epoch: 1, Validation Loss: 1.6305441856384277\n",
            "Epoch: 1, Validation Loss: 1.9352517127990723\n",
            "Epoch: 1, Validation Loss: 1.563440203666687\n",
            "Epoch: 1, Validation Loss: 1.73184072971344\n",
            "Epoch: 1, Validation Loss: 1.757348895072937\n",
            "Epoch: 1, Validation Loss: 1.674782156944275\n",
            "Epoch: 1, Validation Loss: 1.741895318031311\n",
            "Epoch: 1, Validation Loss: 1.8917917013168335\n",
            "Epoch: 1, Validation Loss: 1.7911639213562012\n",
            "Epoch: 1, Validation Loss: 1.819966435432434\n",
            "Epoch: 1, Validation Loss: 1.6907895803451538\n",
            "Epoch: 1, Validation Loss: 1.7199786901474\n",
            "Epoch: 1, Validation Loss: 1.9527207612991333\n",
            "Epoch: 1, Validation Loss: 1.794870376586914\n",
            "Epoch: 1, Validation Loss: 1.704091191291809\n",
            "Epoch: 1, Validation Loss: 1.7945901155471802\n",
            "Epoch: 1, Validation Loss: 1.8096364736557007\n",
            "Epoch: 1, Validation Loss: 1.6777629852294922\n",
            "Epoch: 1, Validation Loss: 1.6931482553482056\n",
            "Epoch: 1, Validation Loss: 1.7822744846343994\n",
            "Epoch: 1, Validation Loss: 1.7479982376098633\n",
            "Epoch: 1, Validation Loss: 2.045323371887207\n",
            "Epoch: 1, Validation Loss: 1.7433425188064575\n",
            "Epoch: 1, Validation Loss: 2.000063896179199\n",
            "Epoch: 1, Validation Loss: 1.9415944814682007\n",
            "Epoch: 1, Validation Loss: 2.0747194290161133\n",
            "Epoch: 1, Validation Loss: 1.596323013305664\n",
            "Epoch: 1, Validation Loss: 1.5459476709365845\n",
            "Epoch: 1, Validation Loss: 1.7629516124725342\n",
            "Epoch: 1, Validation Loss: 1.8662197589874268\n",
            "Epoch: 1, Validation Loss: 1.9330121278762817\n",
            "Epoch: 1, Validation Loss: 1.6831531524658203\n",
            "Epoch: 1, Validation Loss: 1.5071455240249634\n",
            "Epoch: 1, Validation Loss: 1.6761600971221924\n",
            "Epoch: 1, Validation Loss: 1.6540602445602417\n",
            "Epoch: 1, Validation Loss: 1.654807686805725\n",
            "Epoch: 1, Validation Loss: 1.7470201253890991\n",
            "Epoch: 1, Validation Loss: 1.812218189239502\n",
            "Epoch: 1, Validation Loss: 2.0009775161743164\n",
            "Epoch: 1, Validation Loss: 1.9629263877868652\n",
            "Epoch: 1, Validation Loss: 2.1766231060028076\n",
            "Epoch: 1, Validation Loss: 2.055119276046753\n",
            "Epoch: 1, Validation Loss: 1.8407381772994995\n",
            "Epoch: 1, Validation Loss: 1.8955254554748535\n",
            "Epoch: 1, Validation Loss: 1.57048499584198\n",
            "Epoch: 1, Validation Loss: 1.6065394878387451\n",
            "Epoch: 1, Validation Loss: 1.8239902257919312\n",
            "Epoch: 1, Validation Loss: 1.8517037630081177\n",
            "Epoch: 1, Validation Loss: 1.9185254573822021\n",
            "Epoch: 1, Validation Loss: 1.7834126949310303\n",
            "Epoch: 1, Validation Loss: 1.8825420141220093\n",
            "Epoch: 1, Validation Loss: 1.72797691822052\n",
            "Epoch: 1, Validation Loss: 1.8246097564697266\n",
            "Epoch: 1, Validation Loss: 1.4864827394485474\n",
            "Epoch: 1, Validation Loss: 1.9608922004699707\n",
            "Epoch: 1, Validation Loss: 1.816405177116394\n",
            "Epoch: 1, Validation Loss: 1.791316032409668\n",
            "Epoch: 1, Validation Loss: 1.8823398351669312\n",
            "Epoch: 1, Validation Loss: 1.9865423440933228\n",
            "Epoch: 1, Validation Loss: 2.0155997276306152\n",
            "Epoch: 1, Validation Loss: 1.9847378730773926\n",
            "Epoch: 1, Validation Loss: 1.7205065488815308\n",
            "Epoch: 1, Validation Loss: 1.743439793586731\n",
            "Epoch: 1, Validation Loss: 1.572076439857483\n",
            "Epoch: 1, Validation Loss: 1.9800591468811035\n",
            "Epoch: 1, Validation Loss: 1.5604861974716187\n",
            "Epoch: 1, Validation Loss: 1.8751693964004517\n",
            "Epoch: 1, Validation Loss: 1.7247172594070435\n",
            "Epoch: 1, Validation Loss: 1.952620267868042\n",
            "Epoch: 1, Validation Loss: 1.602996587753296\n",
            "Epoch: 1, Validation Loss: 1.7231101989746094\n",
            "Epoch: 1, Validation Loss: 2.0529520511627197\n",
            "Epoch: 1, Validation Loss: 1.6517603397369385\n",
            "Epoch: 1, Validation Loss: 1.883070945739746\n",
            "Epoch: 1, Validation Loss: 1.7998400926589966\n",
            "Epoch: 1, Validation Loss: 1.3200252056121826\n",
            "Epoch: 1, Validation Loss: 1.9562914371490479\n",
            "Epoch: 1, Validation Loss: 1.658852219581604\n",
            "Epoch: 1, Validation Loss: 1.8377374410629272\n",
            "Epoch: 1, Validation Loss: 1.701147437095642\n",
            "Epoch: 1, Validation Loss: 1.703944206237793\n",
            "Epoch: 1, Validation Loss: 1.8271510601043701\n",
            "Epoch: 1, Validation Loss: 1.751423716545105\n",
            "Epoch: 1, Validation Loss: 1.7796953916549683\n",
            "Epoch: 1, Validation Loss: 1.8310736417770386\n",
            "Epoch: 1, Validation Loss: 1.631856918334961\n",
            "Epoch: 1, Validation Loss: 1.5779144763946533\n",
            "Epoch: 1, Validation Loss: 1.7152099609375\n",
            "Epoch: 1, Validation Loss: 1.849954605102539\n",
            "Epoch: 1, Validation Loss: 1.7119615077972412\n",
            "Epoch: 1, Validation Loss: 1.926653504371643\n",
            "Epoch: 1, Validation Loss: 1.9499455690383911\n",
            "Epoch: 1, Validation Loss: 1.7481684684753418\n",
            "Epoch: 1, Validation Loss: 1.648727536201477\n",
            "Epoch: 1, Validation Loss: 1.8857760429382324\n",
            "Epoch: 1, Validation Loss: 1.8160330057144165\n",
            "Epoch: 1, Validation Loss: 1.613459825515747\n",
            "Epoch: 1, Validation Loss: 2.0284841060638428\n",
            "Epoch: 1, Validation Loss: 1.9078903198242188\n",
            "Epoch: 1, Validation Loss: 1.8529949188232422\n",
            "Epoch: 1, Validation Loss: 2.054258346557617\n",
            "Epoch: 1, Validation Loss: 1.8647353649139404\n",
            "Epoch: 1, Validation Loss: 2.131485939025879\n",
            "Epoch: 1, Validation Loss: 1.8910472393035889\n",
            "Epoch: 1, Validation Loss: 1.9215283393859863\n",
            "Epoch: 1, Validation Loss: 1.641710638999939\n",
            "Epoch: 1, Validation Loss: 1.9619687795639038\n",
            "Epoch: 1, Validation Loss: 1.7767086029052734\n",
            "Epoch: 1, Validation Loss: 1.9106947183609009\n",
            "Epoch: 1, Validation Loss: 1.6464260816574097\n",
            "Epoch: 1, Validation Loss: 1.670327067375183\n",
            "Epoch: 1, Validation Loss: 1.840111255645752\n",
            "Epoch: 1, Validation Loss: 1.663769006729126\n",
            "Epoch: 1, Validation Loss: 1.5215297937393188\n",
            "Epoch: 1, Validation Loss: 1.7083191871643066\n",
            "Epoch: 1, Validation Loss: 1.4779086112976074\n",
            "Epoch: 1, Validation Loss: 2.001493453979492\n",
            "Epoch: 1, Validation Loss: 1.856468677520752\n",
            "Epoch: 1, Validation Loss: 2.0623674392700195\n",
            "Epoch: 1, Validation Loss: 1.6437771320343018\n",
            "Epoch: 1, Validation Loss: 1.6540127992630005\n",
            "Epoch: 1, Validation Loss: 1.8834102153778076\n",
            "Epoch: 1, Validation Loss: 1.5245310068130493\n",
            "Epoch: 1, Validation Loss: 2.0635809898376465\n",
            "Epoch: 1, Validation Loss: 1.980454921722412\n",
            "Epoch: 1, Validation Loss: 1.6234925985336304\n",
            "Epoch: 1, Validation Loss: 1.9801586866378784\n",
            "Epoch: 1, Validation Loss: 1.6868354082107544\n",
            "Epoch: 1, Validation Loss: 1.9061849117279053\n",
            "Epoch: 1, Validation Loss: 1.757304072380066\n",
            "Epoch: 1, Validation Loss: 1.9346133470535278\n",
            "Epoch: 1, Validation Loss: 1.9911484718322754\n",
            "Epoch: 1, Validation Loss: 1.908298373222351\n",
            "Epoch: 1, Validation Loss: 1.7653632164001465\n",
            "Epoch: 1, Validation Loss: 1.9537287950515747\n",
            "Epoch: 1, Validation Loss: 1.8031346797943115\n",
            "Epoch: 1, Validation Loss: 1.7813791036605835\n",
            "Epoch: 1, Validation Loss: 1.9994269609451294\n",
            "Epoch: 1, Validation Loss: 1.903998613357544\n",
            "Epoch: 1, Validation Loss: 1.9301154613494873\n",
            "Epoch: 1, Validation Loss: 1.6683549880981445\n",
            "Epoch: 1, Validation Loss: 1.6325675249099731\n",
            "Epoch: 1, Validation Loss: 1.829925775527954\n",
            "Epoch: 1, Validation Loss: 1.787113070487976\n",
            "Epoch: 1, Validation Loss: 1.774735927581787\n",
            "Epoch: 1, Validation Loss: 1.9506232738494873\n",
            "Epoch: 1, Validation Loss: 1.9435399770736694\n",
            "Epoch: 1, Validation Loss: 1.7198458909988403\n",
            "Epoch: 1, Validation Loss: 1.6534045934677124\n",
            "Epoch: 1, Validation Loss: 1.7742022275924683\n",
            "Epoch: 1, Validation Loss: 1.6819792985916138\n",
            "Epoch: 1, Validation Loss: 1.8656007051467896\n",
            "Epoch: 1, Validation Loss: 1.7319059371948242\n",
            "Epoch: 1, Validation Loss: 1.5620704889297485\n",
            "Epoch: 1, Validation Loss: 1.8647661209106445\n",
            "Epoch: 1, Validation Loss: 1.7325870990753174\n",
            "Epoch: 1, Validation Loss: 1.8912259340286255\n",
            "Epoch: 1, Validation Loss: 1.8164441585540771\n",
            "Epoch: 1, Validation Loss: 1.569873332977295\n",
            "Epoch: 1, Validation Loss: 2.01489520072937\n",
            "Epoch: 1, Validation Loss: 1.8297014236450195\n",
            "Epoch: 1, Validation Loss: 1.8017656803131104\n",
            "Epoch: 1, Validation Loss: 1.774686336517334\n",
            "Epoch: 1, Validation Loss: 1.6589595079421997\n",
            "Epoch: 1, Validation Loss: 1.7940524816513062\n",
            "Epoch: 1, Validation Loss: 1.7042001485824585\n",
            "Epoch: 1, Validation Loss: 1.836161732673645\n",
            "Epoch: 1, Validation Loss: 1.6702836751937866\n",
            "Epoch: 1, Validation Loss: 1.7437938451766968\n",
            "Epoch: 1, Validation Loss: 1.6544935703277588\n",
            "Epoch: 1, Validation Loss: 1.9539464712142944\n",
            "Epoch: 1, Validation Loss: 1.7642650604248047\n",
            "Epoch: 1, Validation Loss: 1.858450174331665\n",
            "Epoch: 1, Validation Loss: 2.0795350074768066\n",
            "Epoch: 1, Validation Loss: 1.6845864057540894\n",
            "Epoch: 1, Validation Loss: 1.7258845567703247\n",
            "Epoch: 1, Validation Loss: 1.4357930421829224\n",
            "Epoch: 1, Validation Loss: 1.9174985885620117\n",
            "Epoch: 1, Validation Loss: 1.918331265449524\n",
            "Epoch: 1, Validation Loss: 1.7194453477859497\n",
            "Epoch: 1, Validation Loss: 1.9407289028167725\n",
            "Epoch: 1, Validation Loss: 1.839235782623291\n",
            "Epoch: 1, Validation Loss: 1.676924705505371\n",
            "Epoch: 1, Validation Loss: 1.6605005264282227\n",
            "Epoch: 1, Validation Loss: 1.6708664894104004\n",
            "Epoch: 1, Validation Loss: 1.9043110609054565\n",
            "Epoch: 1, Validation Loss: 1.7652227878570557\n",
            "Epoch: 1, Validation Loss: 1.580521583557129\n",
            "Epoch: 1, Validation Loss: 2.0533885955810547\n",
            "Epoch: 1, Validation Loss: 1.6819490194320679\n",
            "Epoch: 1, Validation Loss: 1.4628894329071045\n",
            "Epoch: 1, Validation Loss: 1.855947732925415\n",
            "Epoch: 1, Validation Loss: 1.8453084230422974\n",
            "Epoch: 1, Validation Loss: 1.8566761016845703\n",
            "Epoch: 1, Validation Loss: 1.7725903987884521\n",
            "Epoch: 1, Validation Loss: 1.9889447689056396\n",
            "Epoch: 1, Validation Loss: 1.7897255420684814\n",
            "Epoch: 1, Validation Loss: 1.6705970764160156\n",
            "Epoch: 1, Validation Loss: 1.8453328609466553\n",
            "Epoch: 1, Validation Loss: 1.9292776584625244\n",
            "Epoch: 1, Validation Loss: 1.859206199645996\n",
            "Epoch: 1, Validation Loss: 1.9967468976974487\n",
            "Epoch: 1, Validation Loss: 1.8802945613861084\n",
            "Epoch: 1, Validation Loss: 1.7532325983047485\n",
            "Epoch: 1, Validation Loss: 1.8209611177444458\n",
            "Epoch: 1, Validation Loss: 1.8687505722045898\n",
            "Epoch: 1, Validation Loss: 1.90656578540802\n",
            "Epoch: 1, Validation Loss: 1.7857966423034668\n",
            "Epoch: 1, Validation Loss: 1.8381613492965698\n",
            "Epoch: 1, Validation Loss: 1.825226068496704\n",
            "Epoch: 1, Validation Loss: 1.6166099309921265\n",
            "Epoch: 1, Validation Loss: 1.8646528720855713\n",
            "Epoch: 1, Validation Loss: 1.6149967908859253\n",
            "Epoch: 1, Validation Loss: 1.628511667251587\n",
            "Epoch: 1, Validation Loss: 1.7283159494400024\n",
            "Epoch: 1, Validation Loss: 1.7351820468902588\n",
            "Epoch: 1, Validation Loss: 2.1504709720611572\n",
            "Epoch: 1, Validation Loss: 1.7970906496047974\n",
            "Epoch: 1, Validation Loss: 1.6798657178878784\n",
            "Epoch: 1, Validation Loss: 1.8191245794296265\n",
            "Epoch: 1, Validation Loss: 1.7228739261627197\n",
            "Epoch: 1, Validation Loss: 1.5766034126281738\n",
            "Epoch: 1, Validation Loss: 1.6617494821548462\n",
            "Epoch: 1, Validation Loss: 1.8175437450408936\n",
            "Epoch: 1, Validation Loss: 1.8134287595748901\n",
            "Epoch: 1, Validation Loss: 1.914659857749939\n",
            "Epoch: 1, Validation Loss: 1.385040044784546\n",
            "Epoch: 1, Validation Loss: 2.082263231277466\n",
            "Epoch: 1, Validation Loss: 1.7089999914169312\n",
            "Epoch: 1, Validation Loss: 1.7739362716674805\n",
            "Epoch: 1, Validation Loss: 1.6852718591690063\n",
            "Epoch: 1, Validation Loss: 1.5450937747955322\n",
            "Epoch: 1, Validation Loss: 1.8620823621749878\n",
            "Epoch: 1, Validation Loss: 2.003419876098633\n",
            "Epoch: 1, Validation Loss: 2.2182745933532715\n",
            "Epoch: 1, Validation Loss: 1.7841684818267822\n",
            "Epoch: 1, Validation Loss: 1.6437262296676636\n",
            "Epoch: 1, Validation Loss: 1.9350045919418335\n",
            "Epoch: 1, Validation Loss: 2.016995668411255\n",
            "Epoch: 1, Validation Loss: 2.1254942417144775\n",
            "Epoch: 1, Validation Loss: 1.665400505065918\n",
            "Epoch: 1, Validation Loss: 1.958080768585205\n",
            "Epoch: 1, Validation Loss: 1.7870867252349854\n",
            "Epoch: 1, Validation Loss: 1.8433561325073242\n",
            "Epoch: 1, Validation Loss: 1.5617233514785767\n",
            "Epoch: 1, Validation Loss: 2.0670528411865234\n",
            "Epoch: 1, Validation Loss: 1.757441759109497\n",
            "Epoch: 1, Validation Loss: 1.953047513961792\n",
            "Epoch: 1, Validation Loss: 1.6709741353988647\n",
            "Epoch: 1, Validation Loss: 1.6264153718948364\n",
            "Epoch: 1, Validation Loss: 1.965312123298645\n",
            "Epoch: 1, Validation Loss: 1.6182492971420288\n",
            "Epoch: 1, Validation Loss: 1.7533001899719238\n",
            "Epoch: 1, Validation Loss: 1.7762647867202759\n",
            "Epoch: 1, Validation Loss: 2.1008050441741943\n",
            "Epoch: 1, Validation Loss: 1.8937546014785767\n",
            "Epoch: 1, Validation Loss: 1.6651917695999146\n",
            "Epoch: 1, Validation Loss: 1.880383849143982\n",
            "Epoch: 1, Validation Loss: 1.7018482685089111\n",
            "Epoch: 1, Validation Loss: 1.7728625535964966\n",
            "Epoch: 1, Validation Loss: 1.8498550653457642\n",
            "Epoch: 1, Validation Loss: 1.713106393814087\n",
            "Epoch: 1, Validation Loss: 1.8846741914749146\n",
            "Epoch: 1, Validation Loss: 1.8354852199554443\n",
            "Epoch: 1, Validation Loss: 1.946023941040039\n",
            "Epoch: 1, Validation Loss: 1.6000269651412964\n",
            "Epoch: 1, Validation Loss: 1.8447026014328003\n",
            "Epoch: 1, Validation Loss: 1.7096223831176758\n",
            "Epoch: 1, Validation Loss: 1.820923924446106\n",
            "Epoch: 1, Validation Loss: 1.8789677619934082\n",
            "Epoch: 1, Validation Loss: 1.9327324628829956\n",
            "Epoch: 1, Validation Loss: 1.6411103010177612\n",
            "Epoch: 1, Validation Loss: 1.8721970319747925\n",
            "Epoch: 1, Validation Loss: 1.862031102180481\n",
            "Epoch: 1, Validation Loss: 1.9215492010116577\n",
            "Epoch: 1, Validation Loss: 1.7148022651672363\n",
            "Epoch: 1, Validation Loss: 1.9658772945404053\n",
            "Epoch: 1, Validation Loss: 1.5567209720611572\n",
            "Epoch: 1, Validation Loss: 2.0150160789489746\n",
            "Epoch: 1, Validation Loss: 2.0960593223571777\n",
            "Epoch: 1, Validation Loss: 1.7418900728225708\n",
            "Epoch: 1, Validation Loss: 1.8948020935058594\n",
            "Epoch: 1, Validation Loss: 1.786332607269287\n",
            "Epoch: 1, Validation Loss: 1.84589421749115\n",
            "Epoch: 1, Validation Loss: 1.9521925449371338\n",
            "Epoch: 1, Validation Loss: 1.7852493524551392\n",
            "Epoch: 1, Validation Loss: 1.576102614402771\n",
            "Epoch: 1, Validation Loss: 1.9215834140777588\n",
            "Epoch: 1, Validation Loss: 1.7159602642059326\n",
            "Epoch: 1, Validation Loss: 1.8103444576263428\n",
            "Epoch: 1, Validation Loss: 1.7485665082931519\n",
            "Epoch: 1, Validation Loss: 2.086653232574463\n",
            "Epoch: 1, Validation Loss: 1.975254774093628\n",
            "Epoch: 1, Validation Loss: 1.707789421081543\n",
            "Epoch: 1, Validation Loss: 1.8799341917037964\n",
            "Epoch: 1, Validation Loss: 1.5458834171295166\n",
            "Epoch: 1, Validation Loss: 1.5269101858139038\n",
            "Epoch: 1, Validation Loss: 1.891253113746643\n",
            "Epoch: 1, Validation Loss: 1.8077495098114014\n",
            "Epoch: 1, Validation Loss: 1.952028512954712\n",
            "Epoch: 1, Validation Loss: 1.747904896736145\n",
            "Epoch: 1, Validation Loss: 1.7462953329086304\n",
            "Epoch: 1, Validation Loss: 1.820900559425354\n",
            "Epoch: 1, Validation Loss: 1.7506661415100098\n",
            "Epoch: 1, Validation Loss: 1.905150294303894\n",
            "Epoch: 1, Validation Loss: 1.8061916828155518\n",
            "Epoch: 1, Validation Loss: 1.4339334964752197\n",
            "Epoch: 1, Validation Loss: 1.9203433990478516\n",
            "Epoch: 1, Validation Loss: 1.7891520261764526\n",
            "Epoch: 1, Validation Loss: 1.5906850099563599\n",
            "Epoch: 1, Validation Loss: 1.7857755422592163\n",
            "Epoch: 1, Validation Loss: 1.6791307926177979\n",
            "Epoch: 1, Validation Loss: 1.960149884223938\n",
            "Epoch: 1, Validation Loss: 1.5734081268310547\n",
            "Epoch: 1, Validation Loss: 1.9472849369049072\n",
            "Epoch: 1, Validation Loss: 1.8997398614883423\n",
            "Epoch: 1, Validation Loss: 1.7742136716842651\n",
            "Epoch: 1, Validation Loss: 1.5181680917739868\n",
            "Epoch: 1, Validation Loss: 1.6303455829620361\n",
            "Epoch: 1, Validation Loss: 1.6932125091552734\n",
            "Epoch: 1, Validation Loss: 1.7886346578598022\n",
            "Epoch: 1, Validation Loss: 1.732048749923706\n",
            "Epoch: 1, Validation Loss: 1.7403205633163452\n",
            "Epoch: 1, Validation Loss: 1.7063976526260376\n",
            "Epoch: 1, Validation Loss: 1.5982205867767334\n",
            "Epoch: 1, Validation Loss: 1.5921818017959595\n",
            "Epoch: 1, Validation Loss: 1.8354872465133667\n",
            "Epoch: 1, Validation Loss: 1.970748782157898\n",
            "Epoch: 1, Validation Loss: 1.8060630559921265\n",
            "Epoch: 1, Validation Loss: 1.8191754817962646\n",
            "Epoch: 1, Validation Loss: 1.8260539770126343\n",
            "Epoch: 1, Validation Loss: 1.8425579071044922\n",
            "Epoch: 1, Validation Loss: 1.6456177234649658\n",
            "Epoch: 1, Validation Loss: 1.6285730600357056\n",
            "Epoch: 1, Validation Loss: 1.6069469451904297\n",
            "Epoch: 1, Validation Loss: 1.7898552417755127\n",
            "Epoch: 1, Validation Loss: 1.7047820091247559\n",
            "Epoch: 1, Validation Loss: 1.7469207048416138\n",
            "Epoch: 1, Validation Loss: 1.7643858194351196\n",
            "Epoch: 1, Validation Loss: 1.8260002136230469\n",
            "Epoch: 1, Validation Loss: 1.6272882223129272\n",
            "Epoch: 1, Validation Loss: 1.8492809534072876\n",
            "Epoch: 1, Validation Loss: 1.6312522888183594\n",
            "Epoch: 1, Validation Loss: 1.974936604499817\n",
            "Epoch: 1, Validation Loss: 1.8454365730285645\n",
            "Epoch: 1, Validation Loss: 1.732761263847351\n",
            "Epoch: 1, Validation Loss: 1.8805556297302246\n",
            "Epoch: 1, Validation Loss: 1.5304853916168213\n",
            "Epoch: 1, Validation Loss: 1.6464260816574097\n",
            "Epoch: 1, Validation Loss: 2.100768804550171\n",
            "Epoch: 1, Validation Loss: 1.720445990562439\n",
            "Epoch: 1, Validation Loss: 1.8557374477386475\n",
            "Epoch: 1, Validation Loss: 1.9350789785385132\n",
            "Epoch: 1, Validation Loss: 1.6112513542175293\n",
            "Epoch: 1, Validation Loss: 1.7124329805374146\n",
            "Epoch: 1, Validation Loss: 1.7384856939315796\n",
            "Epoch: 1, Validation Loss: 1.8784762620925903\n",
            "Epoch: 1, Validation Loss: 1.5689698457717896\n",
            "Epoch: 1, Validation Loss: 1.9447944164276123\n",
            "Epoch: 1, Validation Loss: 1.9349451065063477\n",
            "Epoch: 1, Validation Loss: 1.7667136192321777\n",
            "Epoch: 1, Validation Loss: 1.6547051668167114\n",
            "Epoch: 1, Validation Loss: 1.9832332134246826\n",
            "Epoch: 1, Validation Loss: 1.7836283445358276\n",
            "Epoch: 1, Validation Loss: 1.925845980644226\n",
            "Epoch: 1, Validation Loss: 1.8375394344329834\n",
            "Epoch: 1, Validation Loss: 1.471904993057251\n",
            "Epoch: 1, Validation Loss: 2.0393056869506836\n",
            "Epoch: 1, Validation Loss: 1.9224445819854736\n",
            "Epoch: 1, Validation Loss: 1.7743010520935059\n",
            "Epoch: 1, Validation Loss: 1.8507827520370483\n",
            "Epoch: 1, Validation Loss: 1.8967162370681763\n",
            "Epoch: 1, Validation Loss: 1.86253023147583\n",
            "Epoch: 1, Validation Loss: 1.7437889575958252\n",
            "Epoch: 1, Validation Loss: 1.80832839012146\n",
            "Epoch: 1, Validation Loss: 1.867008090019226\n",
            "Epoch: 1, Validation Loss: 1.818434238433838\n",
            "Epoch: 1, Validation Loss: 1.6943296194076538\n",
            "Epoch: 1, Validation Loss: 1.6920017004013062\n",
            "Epoch: 1, Validation Loss: 1.885377049446106\n",
            "Epoch: 1, Validation Loss: 1.9361650943756104\n",
            "Epoch: 1, Validation Loss: 1.6201856136322021\n",
            "Epoch: 1, Validation Loss: 1.77030611038208\n",
            "Epoch: 1, Validation Loss: 1.7488858699798584\n",
            "Epoch: 1, Validation Loss: 1.8038181066513062\n",
            "Epoch: 1, Validation Loss: 2.005305290222168\n",
            "Epoch: 1, Validation Loss: 1.5176844596862793\n",
            "Epoch: 1, Validation Loss: 1.8072271347045898\n",
            "Epoch: 1, Validation Loss: 1.8859940767288208\n",
            "Epoch: 1, Validation Loss: 1.7705307006835938\n",
            "Epoch: 1, Validation Loss: 1.6104406118392944\n",
            "Epoch: 1, Validation Loss: 1.565053105354309\n",
            "Epoch: 1, Validation Loss: 2.0462448596954346\n"
          ]
        }
      ],
      "source": [
        "src_vocab_size = 10_000\n",
        "tgt_vocab_size = 10_000\n",
        "d_model = 512\n",
        "num_heads = 4\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "num_epochs = 1\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------------------\")\n",
        "    transformer.train()\n",
        "    for data in train_dataloader:\n",
        "        src_data, tgt_data = data\n",
        "        optimizer.zero_grad()\n",
        "        output = transformer(src_data, tgt_data[:, :-1])\n",
        "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch: {epoch+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    transformer.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in val_dataloader:\n",
        "            src_data, tgt_data = data\n",
        "            output = transformer(src_data, tgt_data[:, :-1])\n",
        "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "            print(f\"Epoch: {epoch+1}, Validation Loss: {loss.item()}\")\n",
        "\n",
        "    torch.save(transformer.state_dict(), f'./transformer_state_dict_epoch_{epoch+1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLQfPbuV_MW4",
        "outputId": "aa375304-4e95-4845-e04b-02116278f127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.6052852869033813\n",
            "Test Loss: 1.7744276523590088\n",
            "Test Loss: 1.7148017883300781\n",
            "Test Loss: 1.682302713394165\n",
            "Test Loss: 1.7974210977554321\n",
            "Test Loss: 1.8694230318069458\n",
            "Test Loss: 1.9955693483352661\n",
            "Test Loss: 1.478837251663208\n",
            "Test Loss: 1.612669825553894\n",
            "Test Loss: 1.7183172702789307\n",
            "Test Loss: 1.962624192237854\n",
            "Test Loss: 1.717918872833252\n",
            "Test Loss: 1.9392225742340088\n",
            "Test Loss: 1.8702709674835205\n",
            "Test Loss: 1.7806113958358765\n",
            "Test Loss: 1.8002043962478638\n",
            "Test Loss: 1.7125005722045898\n",
            "Test Loss: 1.5445551872253418\n",
            "Test Loss: 1.5599184036254883\n",
            "Test Loss: 1.8407514095306396\n",
            "Test Loss: 1.8727189302444458\n",
            "Test Loss: 1.6432327032089233\n",
            "Test Loss: 1.8154791593551636\n",
            "Test Loss: 1.8881146907806396\n",
            "Test Loss: 1.492788314819336\n",
            "Test Loss: 1.6067529916763306\n",
            "Test Loss: 1.7294621467590332\n",
            "Test Loss: 2.1103644371032715\n",
            "Test Loss: 1.979024887084961\n",
            "Test Loss: 1.7265684604644775\n",
            "Test Loss: 1.655185580253601\n",
            "Test Loss: 1.3915770053863525\n",
            "Test Loss: 1.550269365310669\n",
            "Test Loss: 1.5718532800674438\n",
            "Test Loss: 1.67478609085083\n",
            "Test Loss: 1.6420693397521973\n",
            "Test Loss: 1.7406327724456787\n",
            "Test Loss: 1.8678321838378906\n",
            "Test Loss: 1.92299222946167\n",
            "Test Loss: 1.997302532196045\n",
            "Test Loss: 1.7893743515014648\n",
            "Test Loss: 1.7992123365402222\n",
            "Test Loss: 1.7557379007339478\n",
            "Test Loss: 1.6535600423812866\n",
            "Test Loss: 2.06956148147583\n",
            "Test Loss: 1.8636492490768433\n",
            "Test Loss: 1.947414755821228\n",
            "Test Loss: 1.691988229751587\n",
            "Test Loss: 1.7350986003875732\n",
            "Test Loss: 1.6496717929840088\n",
            "Test Loss: 1.8240339756011963\n",
            "Test Loss: 1.5754514932632446\n",
            "Test Loss: 1.7809412479400635\n",
            "Test Loss: 1.576478123664856\n",
            "Test Loss: 1.78146231174469\n",
            "Test Loss: 1.945945382118225\n",
            "Test Loss: 1.6091471910476685\n",
            "Test Loss: 1.806064248085022\n",
            "Test Loss: 1.9507263898849487\n",
            "Test Loss: 2.0705983638763428\n",
            "Test Loss: 1.9956550598144531\n",
            "Test Loss: 1.7671363353729248\n",
            "Test Loss: 2.0899112224578857\n",
            "Test Loss: 1.8069679737091064\n",
            "Test Loss: 1.823194980621338\n",
            "Test Loss: 1.7774966955184937\n",
            "Test Loss: 1.6156545877456665\n",
            "Test Loss: 1.8213903903961182\n",
            "Test Loss: 1.7964959144592285\n",
            "Test Loss: 1.688646912574768\n",
            "Test Loss: 1.984831690788269\n",
            "Test Loss: 2.1926074028015137\n",
            "Test Loss: 1.9306933879852295\n",
            "Test Loss: 1.8747971057891846\n",
            "Test Loss: 1.660266399383545\n",
            "Test Loss: 1.6948431730270386\n",
            "Test Loss: 1.7509607076644897\n",
            "Test Loss: 1.8913999795913696\n",
            "Test Loss: 1.3828812837600708\n",
            "Test Loss: 1.6798105239868164\n",
            "Test Loss: 1.832912802696228\n",
            "Test Loss: 1.7174017429351807\n",
            "Test Loss: 1.620060920715332\n",
            "Test Loss: 1.79954993724823\n",
            "Test Loss: 1.8528982400894165\n",
            "Test Loss: 1.825805902481079\n",
            "Test Loss: 1.7469377517700195\n",
            "Test Loss: 1.620453119277954\n",
            "Test Loss: 1.8714641332626343\n",
            "Test Loss: 1.7724312543869019\n",
            "Test Loss: 1.8295245170593262\n",
            "Test Loss: 2.020569086074829\n",
            "Test Loss: 1.5730310678482056\n",
            "Test Loss: 1.8605939149856567\n",
            "Test Loss: 1.9062671661376953\n",
            "Test Loss: 1.8832184076309204\n",
            "Test Loss: 1.6887445449829102\n",
            "Test Loss: 1.66480553150177\n",
            "Test Loss: 1.957837700843811\n",
            "Test Loss: 1.8309195041656494\n",
            "Test Loss: 1.6613543033599854\n",
            "Test Loss: 1.8424381017684937\n",
            "Test Loss: 1.646531105041504\n",
            "Test Loss: 1.7350066900253296\n",
            "Test Loss: 1.8319696187973022\n",
            "Test Loss: 1.937800645828247\n",
            "Test Loss: 2.1299500465393066\n",
            "Test Loss: 1.9532204866409302\n",
            "Test Loss: 1.6636027097702026\n",
            "Test Loss: 1.7755701541900635\n",
            "Test Loss: 1.9147570133209229\n",
            "Test Loss: 1.7401578426361084\n",
            "Test Loss: 2.118570566177368\n",
            "Test Loss: 1.6170281171798706\n",
            "Test Loss: 1.5512175559997559\n",
            "Test Loss: 1.9542778730392456\n",
            "Test Loss: 1.9011390209197998\n",
            "Test Loss: 1.8026875257492065\n",
            "Test Loss: 1.9359663724899292\n",
            "Test Loss: 2.076855421066284\n",
            "Test Loss: 1.8127996921539307\n",
            "Test Loss: 1.6920981407165527\n",
            "Test Loss: 2.0385308265686035\n",
            "Test Loss: 1.7765285968780518\n",
            "Test Loss: 1.751649022102356\n",
            "Test Loss: 1.8232616186141968\n",
            "Test Loss: 1.8893814086914062\n",
            "Test Loss: 1.8297579288482666\n",
            "Test Loss: 1.8376996517181396\n",
            "Test Loss: 1.5695619583129883\n",
            "Test Loss: 1.642572283744812\n",
            "Test Loss: 1.5331624746322632\n",
            "Test Loss: 1.7583717107772827\n",
            "Test Loss: 2.042419910430908\n",
            "Test Loss: 1.8892107009887695\n",
            "Test Loss: 1.8965108394622803\n",
            "Test Loss: 1.6979197263717651\n",
            "Test Loss: 1.7466695308685303\n",
            "Test Loss: 1.8257784843444824\n",
            "Test Loss: 1.7987340688705444\n",
            "Test Loss: 1.948724389076233\n",
            "Test Loss: 1.7263113260269165\n",
            "Test Loss: 1.5920064449310303\n",
            "Test Loss: 1.7042075395584106\n",
            "Test Loss: 1.465735673904419\n",
            "Test Loss: 1.878410816192627\n",
            "Test Loss: 1.4460406303405762\n",
            "Test Loss: 1.868607521057129\n",
            "Test Loss: 1.9542829990386963\n",
            "Test Loss: 1.9044835567474365\n",
            "Test Loss: 1.610163688659668\n",
            "Test Loss: 1.8406609296798706\n",
            "Test Loss: 1.5138866901397705\n",
            "Test Loss: 1.7860597372055054\n",
            "Test Loss: 1.934529423713684\n",
            "Test Loss: 1.6700321435928345\n",
            "Test Loss: 1.9524179697036743\n",
            "Test Loss: 1.9193662405014038\n",
            "Test Loss: 1.646688461303711\n",
            "Test Loss: 1.5703513622283936\n",
            "Test Loss: 1.8506475687026978\n",
            "Test Loss: 2.0657684803009033\n",
            "Test Loss: 1.3261773586273193\n",
            "Test Loss: 1.7850176095962524\n",
            "Test Loss: 1.596243143081665\n",
            "Test Loss: 1.899524211883545\n",
            "Test Loss: 1.8229048252105713\n",
            "Test Loss: 1.9228523969650269\n",
            "Test Loss: 1.4579874277114868\n",
            "Test Loss: 1.6393462419509888\n",
            "Test Loss: 1.9490115642547607\n",
            "Test Loss: 1.840502142906189\n",
            "Test Loss: 1.769878625869751\n",
            "Test Loss: 1.7680410146713257\n",
            "Test Loss: 1.6497712135314941\n",
            "Test Loss: 1.7074146270751953\n",
            "Test Loss: 2.198066234588623\n",
            "Test Loss: 1.7125645875930786\n",
            "Test Loss: 1.6608918905258179\n",
            "Test Loss: 1.7052834033966064\n",
            "Test Loss: 1.7641172409057617\n",
            "Test Loss: 1.7780711650848389\n",
            "Test Loss: 1.826660394668579\n",
            "Test Loss: 1.8765305280685425\n",
            "Test Loss: 1.8671842813491821\n",
            "Test Loss: 1.9745107889175415\n",
            "Test Loss: 1.9263372421264648\n",
            "Test Loss: 1.7195589542388916\n",
            "Test Loss: 1.838611125946045\n",
            "Test Loss: 1.6881088018417358\n",
            "Test Loss: 1.5757043361663818\n",
            "Test Loss: 1.8224714994430542\n",
            "Test Loss: 1.5836553573608398\n",
            "Test Loss: 1.7728239297866821\n",
            "Test Loss: 1.6690940856933594\n",
            "Test Loss: 1.8389304876327515\n",
            "Test Loss: 1.9584475755691528\n",
            "Test Loss: 1.7753024101257324\n",
            "Test Loss: 1.7746177911758423\n",
            "Test Loss: 1.6023951768875122\n",
            "Test Loss: 1.8077034950256348\n",
            "Test Loss: 1.7506617307662964\n",
            "Test Loss: 2.0556235313415527\n",
            "Test Loss: 2.088618040084839\n",
            "Test Loss: 1.687108039855957\n",
            "Test Loss: 1.8424063920974731\n",
            "Test Loss: 2.080859661102295\n",
            "Test Loss: 1.8435884714126587\n",
            "Test Loss: 1.7716848850250244\n",
            "Test Loss: 1.7532669305801392\n",
            "Test Loss: 2.0288949012756348\n",
            "Test Loss: 1.7478277683258057\n",
            "Test Loss: 1.8620449304580688\n",
            "Test Loss: 1.7572861909866333\n",
            "Test Loss: 1.8513160943984985\n",
            "Test Loss: 2.0701682567596436\n",
            "Test Loss: 1.7568913698196411\n",
            "Test Loss: 1.7455374002456665\n",
            "Test Loss: 1.936467170715332\n",
            "Test Loss: 1.915846347808838\n",
            "Test Loss: 1.7808020114898682\n",
            "Test Loss: 1.8699297904968262\n",
            "Test Loss: 1.7931725978851318\n",
            "Test Loss: 1.5270063877105713\n",
            "Test Loss: 1.7762755155563354\n",
            "Test Loss: 1.8346185684204102\n",
            "Test Loss: 1.8872252702713013\n",
            "Test Loss: 2.124279499053955\n",
            "Test Loss: 1.9220458269119263\n",
            "Test Loss: 1.6662346124649048\n",
            "Test Loss: 2.1510767936706543\n",
            "Test Loss: 1.700600028038025\n",
            "Test Loss: 1.9349480867385864\n",
            "Test Loss: 2.0266525745391846\n",
            "Test Loss: 1.8104110956192017\n",
            "Test Loss: 1.647252082824707\n",
            "Test Loss: 1.9635515213012695\n",
            "Test Loss: 1.7367891073226929\n",
            "Test Loss: 1.5719434022903442\n",
            "Test Loss: 1.9335196018218994\n",
            "Test Loss: 1.8671807050704956\n",
            "Test Loss: 1.6489440202713013\n",
            "Test Loss: 1.83150053024292\n",
            "Test Loss: 2.0002214908599854\n",
            "Test Loss: 1.7871769666671753\n",
            "Test Loss: 1.9539358615875244\n",
            "Test Loss: 1.818640112876892\n",
            "Test Loss: 1.9899710416793823\n",
            "Test Loss: 2.022338390350342\n",
            "Test Loss: 1.5627119541168213\n",
            "Test Loss: 1.8357523679733276\n",
            "Test Loss: 1.7109910249710083\n",
            "Test Loss: 1.7570428848266602\n",
            "Test Loss: 1.6583284139633179\n",
            "Test Loss: 1.7919987440109253\n",
            "Test Loss: 1.637170672416687\n",
            "Test Loss: 2.0029003620147705\n",
            "Test Loss: 1.8581197261810303\n",
            "Test Loss: 1.6262767314910889\n",
            "Test Loss: 1.6070666313171387\n",
            "Test Loss: 1.8548301458358765\n",
            "Test Loss: 1.8992847204208374\n",
            "Test Loss: 1.5535942316055298\n",
            "Test Loss: 1.7735540866851807\n",
            "Test Loss: 1.5670034885406494\n",
            "Test Loss: 1.6569772958755493\n",
            "Test Loss: 1.753278136253357\n",
            "Test Loss: 1.8743473291397095\n",
            "Test Loss: 1.3906329870224\n",
            "Test Loss: 1.860943078994751\n",
            "Test Loss: 2.169656753540039\n",
            "Test Loss: 1.7714567184448242\n",
            "Test Loss: 1.7227290868759155\n",
            "Test Loss: 1.5064502954483032\n",
            "Test Loss: 1.6455070972442627\n",
            "Test Loss: 2.039139986038208\n",
            "Test Loss: 1.836304783821106\n",
            "Test Loss: 1.6483858823776245\n",
            "Test Loss: 1.7502731084823608\n",
            "Test Loss: 1.9359771013259888\n",
            "Test Loss: 1.9461534023284912\n",
            "Test Loss: 1.6919745206832886\n",
            "Test Loss: 1.6261146068572998\n",
            "Test Loss: 1.8872686624526978\n",
            "Test Loss: 1.7320606708526611\n",
            "Test Loss: 1.9061187505722046\n",
            "Test Loss: 1.83645761013031\n",
            "Test Loss: 2.019421339035034\n",
            "Test Loss: 1.9517155885696411\n",
            "Test Loss: 1.6834526062011719\n",
            "Test Loss: 1.8392658233642578\n",
            "Test Loss: 1.7656711339950562\n",
            "Test Loss: 1.8929716348648071\n",
            "Test Loss: 1.8453545570373535\n",
            "Test Loss: 1.8041783571243286\n",
            "Test Loss: 1.8840878009796143\n",
            "Test Loss: 1.6897088289260864\n",
            "Test Loss: 2.0909488201141357\n",
            "Test Loss: 1.9082058668136597\n",
            "Test Loss: 1.7534271478652954\n",
            "Test Loss: 2.0339856147766113\n",
            "Test Loss: 1.485599398612976\n",
            "Test Loss: 1.7380603551864624\n",
            "Test Loss: 1.8966927528381348\n",
            "Test Loss: 1.715370774269104\n",
            "Test Loss: 1.592727780342102\n",
            "Test Loss: 1.6117323637008667\n",
            "Test Loss: 1.9110671281814575\n",
            "Test Loss: 1.7754690647125244\n",
            "Test Loss: 1.755380630493164\n",
            "Test Loss: 1.4362493753433228\n",
            "Test Loss: 1.789167046546936\n",
            "Test Loss: 1.7280352115631104\n",
            "Test Loss: 1.7585866451263428\n",
            "Test Loss: 1.7977020740509033\n",
            "Test Loss: 1.6607662439346313\n",
            "Test Loss: 1.8392668962478638\n",
            "Test Loss: 1.6277955770492554\n",
            "Test Loss: 1.810117483139038\n",
            "Test Loss: 1.9908385276794434\n",
            "Test Loss: 1.5756090879440308\n",
            "Test Loss: 1.9762852191925049\n",
            "Test Loss: 1.8721606731414795\n",
            "Test Loss: 1.7463555335998535\n",
            "Test Loss: 1.6793943643569946\n",
            "Test Loss: 1.8139203786849976\n",
            "Test Loss: 2.039196252822876\n",
            "Test Loss: 1.5725313425064087\n",
            "Test Loss: 1.7551649808883667\n",
            "Test Loss: 1.8054895401000977\n",
            "Test Loss: 1.7531516551971436\n",
            "Test Loss: 1.8092398643493652\n",
            "Test Loss: 1.671080231666565\n",
            "Test Loss: 1.7148057222366333\n",
            "Test Loss: 1.6634485721588135\n",
            "Test Loss: 1.67158043384552\n",
            "Test Loss: 1.9400310516357422\n",
            "Test Loss: 1.8466169834136963\n",
            "Test Loss: 1.6666967868804932\n",
            "Test Loss: 1.854141354560852\n",
            "Test Loss: 1.8062973022460938\n",
            "Test Loss: 1.9240442514419556\n",
            "Test Loss: 1.5848541259765625\n",
            "Test Loss: 1.7731404304504395\n",
            "Test Loss: 1.8257291316986084\n",
            "Test Loss: 1.6912531852722168\n",
            "Test Loss: 1.8404446840286255\n",
            "Test Loss: 1.8035333156585693\n",
            "Test Loss: 1.4984840154647827\n",
            "Test Loss: 1.806241512298584\n",
            "Test Loss: 1.9808257818222046\n",
            "Test Loss: 1.7232507467269897\n",
            "Test Loss: 1.9379515647888184\n",
            "Test Loss: 2.091155529022217\n",
            "Test Loss: 1.95603346824646\n",
            "Test Loss: 1.7495170831680298\n",
            "Test Loss: 1.6191970109939575\n",
            "Test Loss: 1.7477400302886963\n",
            "Test Loss: 1.8440122604370117\n",
            "Test Loss: 1.7512720823287964\n",
            "Test Loss: 2.033195972442627\n",
            "Test Loss: 1.9378761053085327\n",
            "Test Loss: 1.9156644344329834\n",
            "Test Loss: 1.8259947299957275\n",
            "Test Loss: 1.7044888734817505\n",
            "Test Loss: 1.6803535223007202\n",
            "Test Loss: 1.715276837348938\n",
            "Test Loss: 1.774159550666809\n",
            "Test Loss: 1.7347276210784912\n",
            "Test Loss: 1.7271894216537476\n",
            "Test Loss: 1.7144997119903564\n",
            "Test Loss: 1.7896920442581177\n",
            "Test Loss: 1.817941427230835\n",
            "Test Loss: 1.9334683418273926\n",
            "Test Loss: 1.7347729206085205\n",
            "Test Loss: 1.9285815954208374\n",
            "Test Loss: 1.698594570159912\n",
            "Test Loss: 1.7569793462753296\n",
            "Test Loss: 1.8250577449798584\n",
            "Test Loss: 2.0460329055786133\n",
            "Test Loss: 1.7167948484420776\n",
            "Test Loss: 1.6768972873687744\n",
            "Test Loss: 1.8656556606292725\n",
            "Test Loss: 1.7673956155776978\n",
            "Test Loss: 1.511340856552124\n",
            "Test Loss: 1.4882686138153076\n",
            "Test Loss: 1.7756950855255127\n",
            "Test Loss: 1.6964834928512573\n",
            "Test Loss: 2.120567798614502\n",
            "Test Loss: 1.985973834991455\n",
            "Test Loss: 1.6062053442001343\n",
            "Test Loss: 1.8256303071975708\n",
            "Test Loss: 1.7131595611572266\n",
            "Test Loss: 1.8748397827148438\n",
            "Test Loss: 1.5942320823669434\n",
            "Test Loss: 1.914509892463684\n",
            "Test Loss: 1.7804038524627686\n",
            "Test Loss: 1.7582910060882568\n",
            "Test Loss: 1.7318987846374512\n",
            "Test Loss: 1.7758934497833252\n",
            "Test Loss: 2.007307529449463\n",
            "Test Loss: 1.8050546646118164\n",
            "Test Loss: 1.7200796604156494\n",
            "Test Loss: 1.7481660842895508\n",
            "Test Loss: 1.819313645362854\n",
            "Test Loss: 1.9038951396942139\n",
            "Test Loss: 1.522120714187622\n",
            "Test Loss: 1.753178358078003\n",
            "Test Loss: 1.7665029764175415\n",
            "Test Loss: 1.5794956684112549\n",
            "Test Loss: 1.8698232173919678\n",
            "Test Loss: 1.6852308511734009\n",
            "Test Loss: 1.6146711111068726\n",
            "Test Loss: 1.7447222471237183\n",
            "Test Loss: 1.6317728757858276\n",
            "Test Loss: 1.8483309745788574\n",
            "Test Loss: 1.7733904123306274\n",
            "Test Loss: 1.8221242427825928\n",
            "Test Loss: 1.6785237789154053\n",
            "Test Loss: 1.568077802658081\n",
            "Test Loss: 1.9623396396636963\n",
            "Test Loss: 1.7056224346160889\n",
            "Test Loss: 1.8820499181747437\n",
            "Test Loss: 1.8787704706192017\n",
            "Test Loss: 1.908829689025879\n",
            "Test Loss: 2.0109128952026367\n",
            "Test Loss: 2.100672721862793\n",
            "Test Loss: 1.855241060256958\n",
            "Test Loss: 1.736802101135254\n",
            "Test Loss: 1.579969048500061\n",
            "Test Loss: 1.7941558361053467\n",
            "Test Loss: 1.643280029296875\n",
            "Test Loss: 1.5536072254180908\n",
            "Test Loss: 1.7957898378372192\n",
            "Test Loss: 2.367063522338867\n"
          ]
        }
      ],
      "source": [
        "transformer.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        src_data, tgt_data = data\n",
        "        output = transformer(src_data, tgt_data[:, :-1])\n",
        "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "        print(f\"Test Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofl363KBgiyq"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsbGPbE_hFPD",
        "outputId": "8cc6286a-420b-49c1-b101-304eb84df4e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path = \"./transformer_state_dict_epoch_1\"\n",
        "state_dict = torch.load(model_path)\n",
        "\n",
        "src_vocab_size = 10_000\n",
        "tgt_vocab_size = 10_000\n",
        "d_model = 512\n",
        "num_heads = 4\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "num_epochs = 3\n",
        "\n",
        "transformer_loaded = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
        "transformer_loaded.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "x0NYt9BPMufm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def translate(src):\n",
        "    # Tokenize the source sentence\n",
        "    src_tokens = tokenizer[SRC_LANGUAGE](src)\n",
        "    # Initialize the target tokens with <BOS> (beginning of sequence)\n",
        "    tgt_tokens = [\"<BOS>\"]\n",
        "\n",
        "    # Convert source tokens to tensor: add BOS, EOS, and pad to max_seq_len\n",
        "    src_vectors = torch.tensor(\n",
        "        ([BOS_IDX] + vocab[SRC_LANGUAGE](src_tokens) + [EOS_IDX] + [0] * (max_seq_len - len(src_tokens)))[:max_seq_len],\n",
        "        dtype=torch.long, device=device\n",
        "    ).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Generate target tokens step by step\n",
        "    for i in range(max_seq_len):\n",
        "        # Convert target tokens to tensor: pad to max_seq_len\n",
        "        tgt_vectors = torch.tensor(\n",
        "            (vocab[TGT_LANGUAGE](tgt_tokens) + [0] * (max_seq_len - len(tgt_tokens)))[:max_seq_len],\n",
        "            dtype=torch.long, device=device\n",
        "        ).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Pass source and target vectors through the transformer model\n",
        "        output = transformer(src_vectors, tgt_vectors)\n",
        "        # Get the predicted token index (argmax of softmax output)\n",
        "        idx = torch.argmax(nn.functional.softmax(output, dim=2)[0][i]).item()\n",
        "        # Append the predicted token to the target tokens\n",
        "        tgt_tokens.append(vocab[TGT_LANGUAGE].lookup_token(idx))\n",
        "\n",
        "        # Stop if <EOS> (end of sequence) is predicted\n",
        "        if idx == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    # Join the target tokens into a sentence, remove special tokens, and strip whitespace\n",
        "    return \" \".join(tgt_tokens).replace(\"<BOS>\", \"\").replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d9xThngkeK6W",
        "outputId": "d094fdff-752f-42b6-dd08-7f7b4898bf60"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ich bin Student .'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"Hello, I am a student.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lE7m-fQefb0V",
        "outputId": "28aa4619-dd93-4609-e501-801808b29a0d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Mein Name ist Johannes .'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"My name is John.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QyHIl2fhfp2O",
        "outputId": "783eeecf-5530-403a-b121-982a868651bd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ich lerne .'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I am learning German.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KahapCeCfwMr",
        "outputId": "d431b618-383c-48f2-f13a-ada07a3f89ec"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ich esse <UNK> .'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I eat bananas.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q_7aRX5Hf0A7",
        "outputId": "68a9d1df-79c0-426a-9063-1b8d661b2918"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ich habe drei Bücher und zwei .'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I have three books and two pens.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OQkoUnAmgMw1",
        "outputId": "17fec17a-f324-4314-e592-353616c7159b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<UNK> du in einem Büro ?'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"Do you work in an office?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eQ3u-gnngu_l",
        "outputId": "6ad6a650-190b-44d7-e3c4-f4e22a1101f9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Wie geht es dir ?'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"How are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_p_dI_QOiWRJ",
        "outputId": "4ed50fe3-c7ba-4876-c11d-029885f4b2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "People who have children are happier than people who don't have children.\n",
            "Leute, die Kinder haben, sind glücklicher als solche ohne.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Die Leute haben Kinder , die Leute haben , die Kinder nicht haben .'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng, ger = test_lines[0].split('\\t')\n",
        "print(eng)\n",
        "print(ger)\n",
        "translate(eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "T0VbizfWil7D",
        "outputId": "3db7fa6c-8efb-4e06-aee2-3b6ac1969ecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You don't need to know everything.\n",
            "Du musst nicht alles wissen.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Du brauchst nicht alles wissen .'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng, ger = test_lines[500].split('\\t')\n",
        "print(eng)\n",
        "print(ger)\n",
        "translate(eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vp-71fggiuM5",
        "outputId": "67aedcad-d969-4790-9054-2405162c5756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It could get complicated.\n",
            "Es könnte schwierig werden.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Es könnte kompliziert werden .'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng, ger = train_lines[1000].split('\\t')\n",
        "print(eng)\n",
        "print(ger)\n",
        "translate(eng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fGxzFl4Ui0lY",
        "outputId": "521c53c4-f010-4c63-b922-5c6e0cbdf292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you think it's a good idea to feed your dog table scraps?\n",
            "Meinst du, dass es eine gute Idee ist, deinen Hund mit Tischabfällen zu füttern?\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Denkst du , es ist ein guter Hund , dein Hund zu füttern ?'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng, ger = train_lines[10000].split('\\t')\n",
        "print(eng)\n",
        "print(ger)\n",
        "translate(eng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HIFB166pJMq"
      },
      "source": [
        "# Export model and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "NRQyHCffi7-s"
      },
      "outputs": [],
      "source": [
        "torch.save(vocab[SRC_LANGUAGE], \"./vocab-english\")\n",
        "torch.save(vocab[TGT_LANGUAGE], \"./vocab-german\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "DFaLmjFWqB3e"
      },
      "outputs": [],
      "source": [
        "torch.save(tokenizer[SRC_LANGUAGE], \"./tokenizer-english\")\n",
        "torch.save(tokenizer[TGT_LANGUAGE], \"./tokenizer-german\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "v5leNOqGqmT3"
      },
      "outputs": [],
      "source": [
        "torch.save(transformer, \"./transformer_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ndAILIdq58F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
